[
  null,
  {
    "sim": 0.687643242155347,
    "gen": {
      "title": "Stochastic Model Predictive Control of Air Conditioning System for Electric Vehicles: Sensitivity Study, Comparison, and Improvement",
      "url": "https://www.semanticscholar.org/paper/9d45b322d0002f2fdb1e1028e02a67d23684ff4b",
      "abstract": "A stochastic model predictive controller (SMPC) of air conditioning (AC) system is proposed to improve the energy efficiency of electric vehicles (EVs). A Markov-chain based velocity predictor is adopted to provide a sense of the future disturbances over the SMPC control horizon. The sensitivity of electrified AC plant to solar radiation, ambient temperature, and relative air flow speed is quantificationally analyzed from an energy efficiency perspective. Three control approaches are compared in terms of the electricity consumption, cabin temperature, and comfort fluctuation, which include the proposed SMPC method, a generally used bang-bang controller, and dynamic programming as the benchmark. Real solar radiation and ambient temperature data are measured to validate the effectiveness of the SMPC. Comparison results illustrate that SMPC is able to improve the AC energy economy by 12% compared to the rule-based controller. The cabin temperature variation is reduced by more than 50.4%, resulting with a much better cabin comfort.",
      "fieldsOfStudy": [
        "Computer Science",
        "Mathematics"
      ],
      "topics": [
        "SMPC",
        "ambient temperature",
        "comfort fluctuation",
        "relative air flow speed",
        "dynamic programming",
        "EVs",
        "electric vehicles",
        "electrified AC plant",
        "AC",
        "proposed SMPC method",
        "SMPC control horizon",
        "Real solar radiation",
        "solar radiation",
        "AC energy economy",
        "energy efficiency perspective",
        "Real solar radiation and ambient temperature data"
      ]
    },
    "org": {
      "title": "Optimal Participation of Price-Maker Battery Energy Storage Systems in Energy, Reserve and Pay as Performance Regulation Markets",
      "url": "https://www.semanticscholar.org/paper/e22afef6da5dc8063f3fb4fa77da3e30eef5462d",
      "abstract": "Motivated by the need of assessing the optimal allocation of battery energy storage services across various markets and the corresponding impact on market operations, an optimization framework is proposed in this work to coordinate the operation of an independent utility-scale price-maker battery energy storage system (BESS) in the energy, spinning reserve and performance-based regulation markets. The entire problem is formulated as a bi-level optimization process, where the structure of all markets is modeled considering the joint operation limits. The strategic bidding behavior of a price-maker BESS in a pay as performance regulation market is investigated. Additionally, a specific approach is introduced for modeling automatic generation control (AGC) signals in the optimization. Although the formulated problem is non-linear, it is converted to mixed-integer linear programming (MILP) to find the optimum solution. The proposed framework is evaluated using test case scenarios created from real-world market data. Case study results show the impact of BESS's price-making behavior on the joint operation of energy, reserve, and regulation markets.",
      "fieldsOfStudy": [
        "Engineering",
        "Computer Science"
      ],
      "topics": [
        "market operations",
        "regulation markets",
        "markets",
        "battery energy storage services",
        "performance-based regulation markets",
        "real-world market data",
        "energy",
        "test case scenarios",
        "BESS",
        "joint operation limits",
        "automatic generation control",
        "linear programming",
        "markets",
        "spinning reserve",
        "pay as performance regulation market",
        "joint operation",
        "independent utility-scale price-maker battery energy storage system",
        "energy, reserve, and regulation markets",
        "energy, spinning reserve and performance-based regulation markets",
        "bi-level optimization process",
        "Case study results"
      ]
    }
  },
  {
    "sim": 0.37085767385730906,
    "gen": {
      "title": "Two-Stream Network Based on Visual Saliency Sharing for 3D Model Recognition",
      "url": "https://www.semanticscholar.org/paper/29563d62b1fa441c5fa024d0903e30c8d77f6bfe",
      "abstract": "Shape representation for 3D models is an important topic in computer vision, multimedia analysis, and computer graphics. Recent multiview-based methods demonstrate promising performance for 3D model recognition and retrieval. However, most of the multiview-based methods focus on the visual information from the taken views and ignore correlation information among these views, which means the similarity and differentiation of multiple views have lost in their methods. In order to address this issue, we propose a novel two-stream network architecture for 3D model recognition and retrieval. The proposed network includes two sub-networks: a multi-view convolutional neural network (MVCNN) that extracts the view information from the taken views, and an Visual Saliency model that defines the weight of views based on the similarity and differentiation information of multiple views. Special, the weight of views defined by the Visual Saliency model can effectively be used to guide the visual information fusion in MVCNN model. This design can make the MVCNN model save visual information and the correlation information of these views in the learning step. Finally, we employ early-fusion method to fuse the feature vectors from MVCNN model and Visual Saliency model respectively, to generate the shape descriptor for 3D model recognition and retrieval. The experimental result on two public datasets, ModelNet40 and ShapeNetCore55, demonstrates the correlation information of multiple views is crucial for view-based 3D model recognition methods and the proposed method can achieve the state-of-the-art performance on both 3D object classification and retrieval.",
      "fieldsOfStudy": [
        "Computer Science"
      ],
      "topics": [
        "3D model recognition",
        "MVCNN model",
        "multiple views",
        "visual information",
        "views",
        "correlation information",
        "3D models",
        "view-based 3D model recognition methods",
        "Visual Saliency",
        "retrieval",
        "computer graphics",
        "3D",
        "Visual Saliency model",
        "computer vision",
        "view information"
      ]
    },
    "org": {
      "title": "InferBeam: A Fast Beam Alignment Protocol for Millimeter-wave Networking",
      "url": "https://www.semanticscholar.org/paper/b5d5a98cb4aef27f0d60847250219e62f2ade665",
      "abstract": "We introduce fast millimeter-wave base station (BS) and its antenna sector selection for user equipment based on its location. Using a conditional random field inference model with specially designed parameters, which are robust to change of environment, InferBeam allows the use of measurement samples on best beam selection at a small number of locations to infer the rest dynamically. Compared to beam-sweeping based approaches in the literature, InferBeam can drastically reduce the setup cost for beam alignment for a new environment, and also the latency in acquiring a new beam under intermittent blockage. We have evaluated InferBeam using a discrete event simulation. Our results indicate that the system can make best beam selection for 98% of locations in test environments comprising smallsized apartment or office spaces, while sampling fewer than 1% of locations. InferBeam is a complete protocol for best beam inference that can be integrated into millimeter-wave standards for accelerating the much-needed fast and economic beam alignment capability.",
      "fieldsOfStudy": [
        "Computer Science"
      ],
      "topics": [
        "best beam selection",
        "best beam inference",
        "beam alignment",
        "test environments",
        "locations",
        "environment",
        "intermittent blockage",
        "InferBeam",
        "beam-sweeping based approaches",
        "measurement samples",
        "user equipment",
        "smallsized apartment or office spaces",
        "new beam",
        "new environment",
        "fast millimeter-wave base station"
      ]
    }
  },
  {
    "sim": 0.6505032563664611,
    "gen": {
      "title": "Hybrid Transmission Distribution Co-simulation: Frequency Regulation using Battery Energy Storage",
      "url": "https://www.semanticscholar.org/paper/e769ac5d6810f7d01d0ccc85dd35ea0e4ce0cfb8",
      "abstract": "Battery energy storage systems (BESS) are proving to be an effective solution in providing frequency regulation services to the bulk grid. However, there are several concerns for the transmission/distribution system operators (TSO/DSO) with the frequent dispatching of the distribution-connected fast-responding storage systems. Unfortunately, the existing decoupled models for transmission and distribution (T&D) simulations are unable to capture the complex interactions between the two systems especially concerning frequency regulation problems due to rapidly varying distribution-connected distributed energy resources (DERs). In this paper, an iteratively coupled T&D hybrid co-simulation framework is developed to facilitate the planning studies for the system operators to help integrate distribution-connected BESS in providing frequency regulation services in response to highly variable DERs such as photovoltaic generation (PVs). Specifically, the proposed framework helps evaluate: (1) the effects of distribution-connected DERs/PVs on the response of the system's automatic generation control (AGC) response, and (2) highlights the use of BESS in providing frequency regulation services using integrated T&D model. The proposed framework is demonstrated using the IEEE 9-bus transmission system model (operating in dynamics mode) coupled with multiple EPRI Ckt-24 distribution system models (operating in quasi-static mode). It is shown that the proposed co-simulation framework helps better visualize the system AGC response and frequency regulation especially in the presence of high-levels of DER generation variability requiring frequent dispatch of BESS.",
      "fieldsOfStudy": [
        "Engineering",
        "Computer Science"
      ],
      "topics": [
        "multiple EPRI Ckt-24 distribution system models",
        "Battery energy storage systems",
        "frequency regulation problems",
        "integrated T&D model",
        "BESS",
        "DER generation variability",
        "response",
        "photovoltaic generation",
        "quasi-static mode",
        "distribution-connected BESS",
        "dynamics mode",
        "frequent dispatch",
        "frequency regulation services",
        "EPRI Ckt-24 distribution",
        "system AGC response and frequency regulation"
      ]
    },
    "org": {
      "title": "Electric Load and Power Forecasting Using Ensemble Gaussian Process Regression",
      "url": "https://www.semanticscholar.org/paper/55b1259d0d694e26f39761035c51c9cee017c161",
      "abstract": "We propose a new forecasting method for predicting load demand and generation scheduling. Accurate week-long forecasting of load demand and optimal power generation is critical for efficient operation of power grid systems. In this work, we use a synthetic data set describing a power grid with 700 buses and 134 generators over a 365-days period with data synthetically generated at an hourly rate. The proposed approach for week-long forecasting is based on the Gaussian process regression (GPR) method, with prior covariance matrices of the quantities of interest (QoI) computed from ensembles formed by up to twenty preceding weeks of QoI observations. Then, we use these covariances within the GPR framework to forecast the QoIs for the following week. We demonstrate that the the proposed ensemble GPR (EGPR) method is capable of accurately forecasting weekly total load demand and power generation profiles. The EGPR method is shown to outperform traditional forecasting methods including the standard GPR and autoregressive integrated moving average (ARIMA) methods.",
      "fieldsOfStudy": [
        "Computer Science",
        "Mathematics"
      ],
      "topics": [
        "traditional forecasting methods",
        "power grid systems",
        "optimal power generation",
        "load demand",
        "QoI observations",
        "generation scheduling",
        "weekly total load demand and power generation profiles",
        "efficient operation",
        "QoI",
        "GPR",
        "prior covariance matrices",
        "new forecasting method",
        "data",
        "week-long forecasting",
        "hourly"
      ]
    }
  },
  {
    "sim": 0.3231731895915234,
    "gen": {
      "title": "Approximate graph spectral decomposition with the Variational Quantum Eigensolver",
      "url": "https://www.semanticscholar.org/paper/671fd6fde6f04f6a7c239e78cbed72747d3226bb",
      "abstract": "Spectral graph theory is a branch of mathematics that studies the relationships between the eigenvectors and eigenvalues of Laplacian and adjacency matrices and their associated graphs. The Variational Quantum Eigen- solver (VQE) algorithm was proposed as a hybrid quantum/classical algorithm that is used to quickly determine the ground state of a Hamiltonian, and more generally, the lowest eigenvalue of a matrix M \u2208 Rnxn. There are many interesting problems associated with the spectral decompositions of associated matrices, such as partitioning, embedding, and the determination of other properties. In this paper, we will expand upon the VQE algorithm to analyze the spectra of directed and undirected graphs. We evaluate runtime and accuracy comparisons (empirically and theoretically) between different choices of ansatz parameters, graph sizes, graph densities, and matrix types, and demonstrate the effectiveness of our approach on Rigetti's QCS platform on graphs of up to 64 vertices, finding eigenvalues of adjacency and Laplacian matrices. We finally make direct comparisons to classical performance with the Quantum Virtual Machine (QVM) in the appendix, observing a superpolynomial runtime improvement of our algorithm when run using a quantum computer.*",
      "fieldsOfStudy": [
        "Engineering",
        "Mathematics",
        "Computer Science",
        "Physics"
      ],
      "topics": [
        "adjacency matrices",
        "matrix types",
        "Laplacian matrices",
        "graph densities",
        "associated matrices",
        "graph sizes",
        "graphs",
        "Spectral graph theory",
        "M \u2208 Rnxn",
        "properties",
        "eigenvalues",
        "adjacency",
        "directed and undirected graphs",
        "associated graphs",
        "Rigetti",
        "Laplacian and adjacency matrices",
        "Rnxn",
        "matrix M"
      ]
    },
    "org": {
      "title": "Improving PIE's performance over high-delay paths",
      "url": "https://www.semanticscholar.org/paper/590376f677fac42996db5415ed4db5dc8d9c669f",
      "abstract": "Bufferbloat is excessive latency due to over- provisioned network buffers. PIE and CoDel are two recently proposed Active Queue Management (AQM) algorithms, designed to tackle bufferbloat by lowering the queuing delay without degrading the bottleneck utilization. PIE uses a proportional integral controller to maintain the average queuing delay at a desired level; however, large Round Trip Times (RTT) result in large spikes in queuing delays, which induce high dropping probability and low utilization. To deal with this problem, we propose Maximum and Average queuing Delay with PIE (MADPIE). Loosely based on the drop policy used by CoDel to keep queuing delay bounded, MADPIE is a simple extension to PIE that adds deterministic packet drops at controlled intervals. By means of simulations, we observe that our proposed change does not affect PIE's performance when RTT < 100 ms. The deterministic drops are more dominant when the RTT increases, which results in lower maximum queuing delays and better performance for VoIP traffic and small file downloads, with no major impact on bulk transfers.",
      "fieldsOfStudy": [
        "Computer Science"
      ],
      "topics": [
        "lower maximum queuing delays",
        "low utilization",
        "delays",
        "bulk transfers",
        "small file downloads",
        "large spikes",
        "Round Trip Times",
        "PIE",
        "deterministic packet drops",
        "better performance",
        "high dropping probability",
        "RTT",
        "over- provisioned network buffers",
        "large Round Trip Times",
        "Maximum and Average queuing Delay",
        "queuing delay"
      ]
    }
  },
  {
    "sim": 0.4360520167467782,
    "gen": {
      "title": "Characterizing and Detecting Hateful Users on Twitter",
      "url": "https://www.semanticscholar.org/paper/6c8cf4c52ac677e41882aa2f26f3938c6aa063db",
      "abstract": "\n \n Current approaches to characterize and detect hate speech focus on content posted in Online Social Networks (OSNs). They face shortcomings to get the full picture of hate speech due to its subjectivity and the noisiness of OSN text. This work partially addresses these issues by shifting the focus towards users. We obtain a sample of Twitter's retweet graph with 100,386 users and annotate 4,972 as hateful or normal, and also find 668 users suspended after 4 months. Our analysis shows that hateful/suspended users differ from normal/active ones in terms of their activity patterns, word usage and network structure. Exploiting Twitter's network of connections, we find that a node embedding algorithm outperforms content-based approaches for detecting both hateful and suspended users. Overall, we present a user-centric view of hate speech, paving the way for better detection and understanding of this relevant and challenging issue.\n \n",
      "fieldsOfStudy": [
        "Computer Science"
      ],
      "topics": [
        "hate speech focus",
        "users",
        "hate speech",
        "Online Social Networks",
        "network structure",
        "OSN text",
        "word usage",
        "OSNs",
        "hateful/suspended users",
        "content",
        "better detection",
        "algorithm outperforms content-based approaches",
        "terms",
        "normal/active ones",
        "understanding",
        "hateful and suspended users",
        "Current approaches",
        "100,386 users"
      ]
    },
    "org": {
      "title": "A novel ensemble deep learning model for stock prediction based on stock prices and news",
      "url": "https://www.semanticscholar.org/paper/d5aaa87a737c4ff98e0955b951b9892d03d221af",
      "abstract": null,
      "fieldsOfStudy": [
        "Computer Science",
        "Medicine",
        "Economics"
      ]
    }
  },
  {
    "sim": 0.24173747956802916,
    "gen": {
      "title": "Robust Adaptive Sliding Mode Control of Markovian Jump Systems with Uncertain Mode-dependent Time-varying Delays and Partly Unknown Transition Probabilities",
      "url": "https://www.semanticscholar.org/paper/a7843cabd3346983f18a701c50643ac4b9d12600",
      "abstract": null,
      "fieldsOfStudy": [
        "Computer Science"
      ]
    },
    "org": {
      "title": "A Distributed Satisfactory Content Delivery Scheme for QoS Provisioning in Delay Tolerant Networks",
      "url": "https://www.semanticscholar.org/paper/dc9cd201e4dc2c2e682a2db78fb973cfe9a7cfe8",
      "abstract": "We deal in this paper with the content forwarding problem in Delay Tolerant Networks (DTNs). We first formulate the content delivery interaction as a non-cooperative satisfaction game. On one hand, the source node seeks to ensure a delivery probability above some given threshold. On the other hand, the relay nodes seek to maximize their own payoffs. The source node offers a reward (virtual coins) to the relay which caches and forwards the file to the final destination. Each relay faces the dilemma of accepting/rejecting to cache the source's file. Cooperation incurs energy cost due to caching, carrying and forwarding the source's file. Yet, when a relay accepts to cooperate, it may receive some reward if it succeeds to be the first relay to forward the content to the destination. Otherwise, the relay may receive some penalty in the form of a constant regret; the latter parameter is introduced to make incentive for cooperation. Next, we introduce the concept of Satisfaction Equilibrium (SE) as a solution concept to the induced game. Now, the source node is solely interested in reaching a file delivery probability greater than some given threshold, while the relays behave rationally to maximize their respective payoffs. Full characterizations of the SEs for both pure and mixed strategies are derived. Furthermore, we propose two learning algorithms allowing the players (source/relays) to reach the SE strategies. Finally, extensive numerical investigations and some learning simulations are carried out to illustrate the behaviour of the interacting nodes.",
      "fieldsOfStudy": [
        "Mathematics",
        "Computer Science"
      ],
      "topics": [
        "relays",
        "node",
        "source",
        "energy cost",
        "Delay Tolerant Networks",
        "cooperation",
        "non-cooperative satisfaction game",
        "file delivery probability",
        "virtual coins",
        "SE",
        "Satisfaction Equilibrium",
        "respective payoffs",
        "The source node",
        "incentive",
        "DTNs",
        "relay nodes",
        "source/relays",
        "source node"
      ]
    }
  },
  {
    "sim": 0.3742529826600154,
    "gen": {
      "title": "A Variational Bayesian Approach for Image Restoration\u2014Application to Image Deblurring With Poisson\u2013Gaussian Noise",
      "url": "https://www.semanticscholar.org/paper/bcd5841bfcb2448b4fcafe030ba2dd791db1d17f",
      "abstract": "In this paper, a methodology is investigated for signal recovery in the presence of non-Gaussian noise. In contrast with regularized minimization approaches often adopted in the literature, in our algorithm the regularization parameter is reliably estimated from the observations. As the posterior density of the unknown parameters is analytically intractable, the estimation problem is derived in a variational Bayesian framework where the goal is to provide a good approximation to the posterior distribution in order to compute posterior mean estimates. Moreover, a majorization technique is employed to circumvent the difficulties raised by the intricate forms of the non-Gaussian likelihood and of the prior density. We demonstrate the potential of the proposed approach through comparisons with state-of-the-art techniques that are specifically tailored to signal recovery in the presence of mixed Poisson\u2013Gaussian noise. Results show that the proposed approach is efficient and achieves performance comparable with other methods where the regularization parameter is manually tuned from the ground truth.",
      "fieldsOfStudy": [
        "Computer Science",
        "Mathematics"
      ],
      "topics": [
        "posterior mean estimates",
        "Gaussian",
        "non-Gaussian likelihood",
        "methods",
        "Bayesian",
        "signal recovery",
        "mixed Poisson",
        "order",
        "posterior density",
        "regularized minimization approaches",
        "variational Bayesian framework",
        "recovery",
        "non-Gaussian",
        "mixed Poisson\u2013Gaussian noise",
        "regularization parameter",
        "posterior distribution",
        "unknown parameters"
      ]
    },
    "org": {
      "title": "Nonlinear spread of rumor and inoculation strategies in the nodes with degree dependent tie strength in complex networks",
      "url": "https://www.semanticscholar.org/paper/30f316f58b33f7d04328622c3df8840d0d164704",
      "abstract": "In earlier rumor spreading models, at each time step nodes contact all of their neighbors. In more realistic scenario it is possible that a node may contact only some of its neighbors to spread the rumor. Therefore it is must in real world complex networks, the classic rumor spreading model need to be modified to consider the dependence of rumor spread rate on the degree of the spreader and the informed nodes. We have given a modified rumor spreading model to accommodate these facts. This new model, has been studied for rumor spreading in complex networks in this work. Nonlinear rumor spread exponent $\\alpha$ and degree dependent tie strength exponent $\\beta$ in any complex network gives rumor threshold as some finite value. In the present work, the modified rumor spreading model has been studied in scale free networks. It is also found that if $ \\alpha $ and $ \\beta $ parameters are tuned to appropriate value, the rumor threshold becomes independent of network size. In any social network, rumors can spread may have undesirable effect. One of the possible solutions to control rumor spread, is to inoculate a certain fraction of nodes against rumors. The inoculation can be done randomly or in a targeted fashion. We have used modified rumor spreading model over scale free networks to investigate the efficacy of inoculation. Random and targeted inoculation schemes have been applied. It has been observed that rumor threshold in random inoculation scheme is greater than the rumor threshold in the model without any inoculation scheme. But random inoculation is not that much effective. The rumor threshold in targeted inoculation is very high than the rumor threshold in the random inoculation in suppressing the rumor.",
      "fieldsOfStudy": [
        "Computer Science",
        "Physics"
      ],
      "topics": [
        "rumor threshold",
        "earlier rumor spreading models",
        "rumors",
        "modified rumor",
        "rumor spread rate",
        "Nonlinear rumor spread exponent",
        "spreading model",
        "complex networks",
        "scale free networks",
        "random inoculation",
        "targeted inoculation",
        "network size",
        "modified rumor spreading model"
      ]
    }
  },
  {
    "sim": 0.49186617405701494,
    "gen": {
      "title": "Transfer Learning via $\\ell_1$ Regularization",
      "url": "https://www.semanticscholar.org/paper/6f25687b0d69eee885025b944c9b3f0fd03746d3",
      "abstract": "Machine learning algorithms typically require abundant data under a stationary environment. However, environments are nonstationary in many real-world applications. Critical issues lie in how to effectively adapt models under an ever-changing environment. We propose a method for transferring knowledge from a source domain to a target domain via $\\ell_1$ regularization. We incorporate $\\ell_1$ regularization of differences between source parameters and target parameters, in addition to an ordinary $\\ell_1$ regularization. Hence, our method yields sparsity for both the estimates themselves and changes of the estimates. The proposed method has a tight estimation error bound under a stationary environment, and the estimate remains unchanged from the source estimate under small residuals. Moreover, the estimate is consistent with the underlying function, even when the source estimate is mistaken due to nonstationarity. Empirical results demonstrate that the proposed method effectively balances stability and plasticity.",
      "fieldsOfStudy": [
        "Computer Science",
        "Mathematics"
      ],
      "topics": [
        "source parameters",
        "target parameters",
        "environments",
        "small residuals",
        "nonstationarity",
        "source estimate",
        "differences",
        "source domain",
        "addition",
        "real-world applications",
        "abundant data",
        "stationary environment",
        "plasticity",
        "target domain",
        "estimate"
      ]
    },
    "org": {
      "title": "Similarity Measures for Vocal-Based Drum Sample Retrieval Using Deep Convolutional Auto-Encoders",
      "url": "https://www.semanticscholar.org/paper/eab422b69605e047597a743ebb657b2728ad7f8e",
      "abstract": "The expressive nature of the voice provides a powerful medium for communicating sonic ideas, motivating recent research on methods for query by vocalisation. Meanwhile, deep learning methods have demonstrated state-of-the-art results for matching vocal imitations to imitated sounds, yet little is known about how well learned features represent the perceptual similarity between vocalisations and queried sounds. In this paper, we address this question using similarity ratings between vocal imitations and imitated drum sounds. We use a linear mixed effect regression model to show how features learned by convolutional auto-encoders (CAEs) perform as predictors for perceptual similarity between sounds. Our experiments show that CAEs outperform three baseline feature sets (spectrogram-based representations, MFCCs, and temporal features) at predicting the subjective similarity ratings. We also investigate how the size and shape of the encoded layer effects the predictive power of the learned features. The results show that preservation of temporal information is more important than spectral resolution for this application.",
      "fieldsOfStudy": [
        "Computer Science",
        "Engineering"
      ],
      "topics": [
        "queried sounds",
        "imitated drum sounds",
        "sounds",
        "temporal features",
        "similarity ratings",
        "features",
        "vocal imitations",
        "vocalisations",
        "learned features",
        "subjective similarity ratings",
        "baseline feature sets",
        "perceptual similarity"
      ]
    }
  },
  {
    "sim": 0.5900259037914357,
    "gen": {
      "title": "Smart Home Energy Management System for Power System Resiliency",
      "url": "https://www.semanticscholar.org/paper/0fc9ecc0703137de32e054f81d53eae773832110",
      "abstract": "The need for resiliency of electricity supply is increasing due to increasing frequency of natural disasters-such as hurricanes-that disrupt supply from the power grid. Rooftop solar photovoltaic (PV) panels together with batteries can provide resiliency in many scenarios. Without intelligent and automated decision making that can trade off conflicting requirements, a large PV system and a large battery is needed to provide meaningful resiliency. By using forecast of solar generation and household demand, an intelligent decision maker can operate the equipment (battery and critical loads) to ensure that the critical loads are serviced to the maximum duration possible. With the aid of such an intelligent control system, a smaller (and thus lower cost) system can service the primary loads for the same duration that a much larger system will be needed to service otherwise. In this paper we propose such an intelligent control system. A model predictive control (MPC) architecture is used that uses available measurements and forecasts to make optimal decisions for batteries and critical loads in real time. The optimization problem is formulated as a MILP (mixed integer linear program) due to the on/off decisions for the loads. Performance is compared with a non-intelligent baseline controller, for a PV-battery system chosen carefully for a single family house in Florida. Simulations are conducted for a one week period during hurricane Irma in 2017. Simulations show that the cost of the PV +battery system to provide a certain resiliency performance, duration the primary load can be serviced successfully, can be halved by the proposed control system.",
      "fieldsOfStudy": [
        "Computer Science",
        "Engineering",
        "Mathematics"
      ],
      "topics": [
        "critical loads",
        "meaningful resiliency",
        "batteries",
        "resiliency",
        "PV +battery system",
        "optimal decisions",
        "battery and critical loads",
        "increasing frequency",
        "large PV system",
        "scenarios",
        "electricity supply",
        "real time",
        "proposed control system",
        "natural disasters",
        "Florida",
        "battery",
        "duration",
        "hurricane Irma"
      ]
    },
    "org": {
      "title": "Simulation and Optimisation of Air Conditioning Systems using Machine Learning",
      "url": "https://www.semanticscholar.org/paper/f8940b313d76d5236d7fd8db5cc6aed93d02a481",
      "abstract": "In building management, usually static thermal setpoints are used to maintain the inside temperature of a building at a comfortable level irrespective of its occupancy. This strategy can cause a massive amount of energy wastage and therewith increase energy related expenses. This paper explores how to optimise the setpoints used in a particular room during its unoccupied periods using machine learning approaches. We introduce a deep-learning model based on Recurrent Neural Networks (RNN) that can predict the temperatures of a future period directly where a particular room is unoccupied and by using these predicted temperatures, we define the optimal thermal setpoints to be used inside the room during the unoccupied period. We show that RNNs are particularly suitable for this learning task as they enable us to learn across many relatively short series, which is necessary to focus on particular operation modes of the air conditioning (AC) system. We evaluate the prediction accuracy of our RNN model against a set of state-of-the-art models and are able to outperform those by a large margin. We furthermore analyse the usage of our RNN model in optimising the energy consumption of an AC system in a real-world scenario using the temperature data from a university lecture theatre. Based on the simulations, we show that our RNN model can lead to savings around 20% compared with the traditional temperature controlling model that does not use optimisation techniques.",
      "fieldsOfStudy": [
        "Computer Science",
        "Mathematics"
      ],
      "topics": [
        "machine learning approaches",
        "particular operation modes",
        "energy related expenses",
        "static thermal setpoints",
        "traditional temperature controlling model",
        "energy wastage",
        "optimisation techniques",
        "AC",
        "RNN",
        "Recurrent Neural Networks",
        "predicted temperatures",
        "inside temperature",
        "building management",
        "unoccupied periods",
        "unoccupied period"
      ]
    }
  },
  {
    "sim": 0.47786345678589903,
    "gen": {
      "title": "Hybrid feature selection methods for online biomedical publication classification",
      "url": "https://www.semanticscholar.org/paper/839071659776c143b635f2b8dde5ffa8c18c9327",
      "abstract": "We review several feature selection methods: Recursive Feature Elimination, Select K Best, and Random Forests, as elements of a processing chain for feature selection in a text mining task. The text mining task is a multi-label classification problem of label assignment; metadata that is usually applied to published scientific papers by expert curators. In the formulation of this classification task, a feature space that is dramatically larger than the available training data occurs naturally and inevitably. We explore ways to reduce the dimension of the feature space, and show that sequential feature selection does substantially improve performance for this complex type of data.",
      "fieldsOfStudy": [
        "Computer Science"
      ],
      "topics": [
        "feature selection",
        "sequential feature selection",
        "feature selection methods",
        "expert curators",
        "data",
        "published scientific papers",
        "Select K Best",
        "Random Forests",
        "Recursive Feature Elimination",
        "label assignment",
        "elements",
        "multi-label classification problem",
        "available training data",
        "metadata",
        "feature space"
      ]
    },
    "org": {
      "title": "Gaussian Processes on Graphs via Spectral Kernel Learning",
      "url": "https://www.semanticscholar.org/paper/57005786e79889e2515549319b88c1ebea3444b1",
      "abstract": "We propose a graph spectrum-based Gaussian process for prediction of signals defined on nodes of the graph. The model is designed to capture various graph signal structures through a highly adaptive kernel that incorporates a flexible polynomial function in the graph spectral domain. Unlike most existing approaches, we propose to learn such a spectral kernel, where the polynomial setup enables learning without the need for eigen-decomposition of the graph Laplacian. In addition, this kernel has the interpretability of graph filtering achieved by a bespoke maximum likelihood learning algorithm that enforces the positivity of the spectrum. We demonstrate the interpretability of the model in synthetic experiments from which we show the various ground truth spectral filters can be accurately recovered, and the adaptability translates to superior performances in the prediction of real-world graph data of various characteristics.",
      "fieldsOfStudy": [
        "Computer Science",
        "Engineering",
        "Mathematics"
      ],
      "topics": [
        "graph signal structures",
        "graph filtering",
        "characteristics",
        "spectral filters",
        "real-world graph data",
        "prediction",
        "graph spectral domain",
        "signals",
        "superior performances",
        "graph spectrum-based Gaussian process",
        "kernel",
        "Gaussian",
        "nodes",
        "graph",
        "Laplacian",
        "graph Laplacian",
        "learning",
        "ground truth spectral filters",
        "bespoke maximum likelihood learning algorithm"
      ]
    }
  },
  {
    "sim": 0.2840000848929687,
    "gen": {
      "title": "Learning-Based Event-Triggered Control for Synchronization of Passive Multiagent Systems Under Attack",
      "url": "https://www.semanticscholar.org/paper/3511fe6e65e91d41be0072ae436cc8f5e90388ad",
      "abstract": "In this article, we study the synchronization of a group of output passive agents that communicate with each other according to an underlying communication graph. A distributed event-triggered control framework that guarantees synchronization and reduces the required communication rate is introduced. A general Byzantine attack on a multiagent system is defined and its negative effects on synchronization are characterized. The Byzantine agents are able to intelligently falsify their data and manipulate the underlying communication graph by altering their control feedback weights. Next, a decentralized decision-making and detection framework is introduced and its steady-state and transient performances are analyzed. Furthermore, a method of identifying Byzantine neighbors and a learning-based procedure for estimating the attack parameters are introduced. Finally, learning-based control frameworks to mitigate the effects of the attack are proposed.",
      "fieldsOfStudy": [
        "Computer Science"
      ],
      "topics": [
        "output passive agents",
        "underlying communication graph",
        "control feedback weights",
        "required communication rate",
        "Byzantine neighbors",
        "steady-state and transient performances",
        "A distributed event-triggered control framework",
        "negative effects",
        "attack parameters",
        "A general Byzantine attack",
        "Byzantine",
        "synchronization",
        "The Byzantine agents",
        "learning-based control frameworks"
      ]
    },
    "org": {
      "title": "Mem-fractive properties of mushrooms",
      "url": "https://www.semanticscholar.org/paper/3459ac7cdc16014df4bc0a54b753e48046c15de0",
      "abstract": "Memristors close the loop for I\u2013V characteristics of the traditional, passive, semi-conductor devices. A memristor is a physical realisation of the material implication and thus is a universal logical element. Memristors are getting particular interest in the field of bioelectronics. Electrical properties of living substrates are not binary and there is nearly a continuous transitions from being non-memristive to mem-fractive (exhibiting a combination of passive memory) to ideally memristive. In laboratory experiments we show that living oyster mushrooms Pleurotus ostreatus exhibit mem-fractive properties. We offer a piece-wise polynomial approximation of the I\u2013V behaviour of the oyster mushrooms. We also report spiking activity, oscillations in conduced current of the oyster mushrooms.",
      "fieldsOfStudy": [
        "Computer Science",
        "Physics",
        "Medicine"
      ],
      "topics": [
        "mem-fractive properties",
        "passive memory",
        "living oyster mushrooms",
        "Pleurotus ostreatus",
        "Electrical properties",
        "living substrates",
        "traditional, passive, semi-conductor devices",
        "Pleurotus",
        "conduced current",
        "bioelectronics",
        "oyster mushrooms",
        "universal logical element",
        "particular interest",
        "V characteristics",
        "spiking activity"
      ]
    }
  },
  {
    "sim": 0.4416008635735402,
    "gen": {
      "title": "An Energy-aware Online Learning Framework for Resource Management in Heterogeneous Platforms",
      "url": "https://www.semanticscholar.org/paper/4f6e463fd64140150fbf01a28fd40f95a9d2d15d",
      "abstract": "Mobile platforms must satisfy the contradictory requirements of fast response time and minimum energy consumption as a function of dynamically changing applications. To address this need, systems-on-chip (SoC) that are at the heart of these devices provide a variety of control knobs, such as the number of active cores and their voltage/frequency levels. Controlling these knobs optimally at runtime is challenging for two reasons. First, the large configuration space prohibits exhaustive solutions. Second, control policies designed offline are at best sub-optimal, since many potential new applications are unknown at design-time. We address these challenges by proposing an online imitation learning approach. Our key idea is to construct an offline policy and adapt it online to new applications to optimize a given metric (e.g., energy). The proposed methodology leverages the supervision enabled by power-performance models learned at runtime. We demonstrate its effectiveness on a commercial mobile platform with 16 diverse benchmarks. Our approach successfully adapts the control policy to an unknown application after executing less than 25% of its instructions.",
      "fieldsOfStudy": [
        "Computer Science",
        "Engineering"
      ],
      "topics": [
        "new applications",
        "potential new applications",
        "minimum energy consumption",
        "fast response time",
        "dynamically changing applications",
        "control policies",
        "unknown application",
        "control knobs",
        "runtime",
        "active cores",
        "exhaustive solutions",
        "best sub",
        "design-time",
        "Mobile platforms",
        "given metric",
        "online imitation learning approach"
      ]
    },
    "org": {
      "title": "Estimating the Causal Impact of Recommendation Systems from Observational Data",
      "url": "https://www.semanticscholar.org/paper/13181e11021ac8663cca3b090e10ecb4ffe0b534",
      "abstract": "Recommendation systems are an increasingly prominent part of the web, accounting for up to a third of all traffic on several of the world's most popular sites. Nevertheless, little is known about how much activity such systems actually cause over and above activity that would have occurred via other means (e.g., search) if recommendations were absent. Although the ideal way to estimate the causal impact of recommendations is via randomized experiments, such experiments are costly and may inconvenience users. In this paper, therefore, we present a method for estimating causal effects from purely observational data. Specifically, we show that causal identification through an instrumental variable is possible when a product experiences an instantaneous shock in direct traffic and the products recommended next to it do not. We then apply our method to browsing logs containing anonymized activity for 2.1 million users on Amazon.com over a 9 month period and analyze over 4,000 unique products that experience such shocks. We find that although recommendation click-throughs do account for a large fraction of traffic among these products, at least 75% of this activity would likely occur in the absence of recommendations. We conclude with a discussion about the assumptions under which the method is appropriate and caveats around extrapolating results to other products, sites, or settings.",
      "fieldsOfStudy": [
        "Computer Science",
        "Mathematics"
      ],
      "topics": [
        "products",
        "shocks",
        "direct traffic",
        "experiments",
        "settings",
        "systems",
        "traffic",
        "anonymized activity",
        "recommendations",
        "sites",
        "users",
        "means",
        "randomized experiments",
        "Amazon.com",
        "activity"
      ]
    }
  },
  null,
  {
    "sim": 0.5758406436297203,
    "gen": {
      "title": "Energy-Efficient Resource Allocation for Relay-Assisted Backscatter Communication Systems",
      "url": "https://www.semanticscholar.org/paper/33c2b7efc89200b3d213903dc38ecfc4806d1a7b",
      "abstract": "In this paper, we investigate energy-efficient resource allocation in relay-assisted backscatter communication systems. We employed a backscatter user device, and a relay device that supported to operate in different modes, i.e., the backscatter communication (BackCom) mode and the harvest-then transmit (HTT) mode, respectively. We formulate the system energy efficiency (EE) maximization problem by jointly optimizing the power and time allocations. A dual-layer iterative resource allocation algorithm that separates power allocation from time allocation is presented. The inner-layer is responsible for optimizing power allocation, while the outer-layer determines the time allocation strategy. Finally, an algorithm is proposed for jointly determining the optimal solution. Simulation results show that the proposed resource allocation algorithm can increase the system EE.",
      "fieldsOfStudy": [
        "Computer Science"
      ],
      "topics": [
        "time allocation",
        "power allocation",
        "different modes",
        "HTT",
        "time allocation strategy",
        "proposed resource allocation algorithm",
        "energy-efficient resource allocation",
        "relay-assisted backscatter communication systems",
        "BackCom",
        "power and time allocations",
        "A dual-layer iterative resource allocation algorithm",
        "system EE",
        "system energy efficiency",
        "(BackCom) mode",
        "backscatter user device",
        "harvest-then transmit (HTT) mode",
        "i.e., the backscatter communication (BackCom) mode",
        "system energy efficiency (EE) maximization problem",
        "relay device"
      ]
    },
    "org": {
      "title": "Low-Power Wide-Area Networks for Sustainable IoT",
      "url": "https://www.semanticscholar.org/paper/d5902a2d1a774e610c2cd5133776b41f7db5095c",
      "abstract": "LPWA networks are attracting extensive attention because of their ability to offer low-cost and massive connectivity to IoT devices distributed over wide geographical areas. This article provides a brief overview of the existing LPWA technologies and useful insights to aid the large-scale deployment of LPWA networks. In particular, we first review the currently competing candidates of LPWA networks, such as NB-IoT and LoRa, in terms of technical fundamentals and large-scale deployment potential. Then we present two implementation examples of LPWA networks. By analyzing the field-test results, we identify several challenges that prevent LPWA technologies from moving from theory to wide-spread practice.",
      "fieldsOfStudy": [
        "Computer Science",
        "Mathematics"
      ],
      "topics": [
        "LPWA networks",
        "wide geographical areas",
        "LPWA technologies",
        "LPWA",
        "IoT devices",
        "technical fundamentals",
        "large-scale deployment potential",
        "extensive attention",
        "useful insights",
        "existing LPWA technologies",
        "IoT",
        "challenges",
        "terms",
        "LoRa",
        "theory",
        "large-scale deployment",
        "NB-IoT"
      ]
    }
  },
  {
    "sim": 0.38001201388330974,
    "gen": {
      "title": "On the Complexity of Best-Arm Identification in Multi-Armed Bandit Models",
      "url": "https://www.semanticscholar.org/paper/4dc0e6599d543a5ad3c32b9127c9dac04e34021b",
      "abstract": "The stochastic multi-armed bandit model is a simple abstraction that has proven useful in many different contexts in statistics and machine learning. Whereas the achievable limit in terms of regret minimization is now well known, our aim is to contribute to a better understanding of the performance in terms of identifying the m best arms. We introduce generic notions of complexity for the two dominant frameworks considered in the literature: fixed-budget and fixed-confidence settings. In the fixed-confidence setting, we provide the first known distribution-dependent lower bound on the complexity that involves information-theoretic quantities and holds when m \u2265 1 under general assumptions. In the specific case of two armed-bandits, we derive refined lower bounds in both the fixedcon fidence and fixed-budget settings, along with matching algorithms for Gaussian and Bernoulli bandit models. These results show in particular that the complexity of the fixed-budget setting may be smaller than the complexity of the fixed-confidence setting, contradicting the familiar behavior observed when testing fully specified alternatives. In addition, we also provide improved sequential stopping rules that have guaranteed error probabilities and shorter average running times. The proofs rely on two technical results that are of independent interest: a deviation lemma for self-normalized sums (Lemma 7) and a novel change of measure inequality for bandit models (Lemma 1).",
      "fieldsOfStudy": [
        "Mathematics",
        "Computer Science"
      ],
      "topics": [
        "bandit models",
        "machine learning",
        "different contexts",
        "shorter average running times",
        "general assumptions",
        "measure inequality",
        "Lemma",
        "complexity",
        "fixed-budget settings",
        "refined lower bounds",
        "Gaussian and Bernoulli bandit models",
        "alternatives",
        "The stochastic multi-armed bandit model",
        "statistics",
        "error probabilities",
        "guaranteed error probabilities",
        "improved sequential stopping rules",
        "matching algorithms"
      ]
    },
    "org": {
      "title": "Adaptive path planning for depth\u2010constrained bathymetric mapping with an autonomous surface vessel",
      "url": "https://www.semanticscholar.org/paper/9ff30a397a9a4706c886260f8dd19eb8765a53da",
      "abstract": "This paper describes the design, implementation, and testing of a suite of algorithms to enable depth\u2010constrained autonomous bathymetric (underwater topography) mapping by an autonomous surface vessel (ASV). Given a target depth and a bounding polygon, the ASV will find and follow the intersection of the bounding polygon and the depth contour as modeled online with a Gaussian process (GP). This intersection, once mapped, will then be used as a boundary within which a path will be planned for coverage to build a map of the bathymetry. Efficient methods are implemented enabling online fitting, prediction and hyperparameter optimization within the GP framework on a small embedded PC. New algorithms are introduced for the partitioning of convex polygons to allow efficient path planning for coverage. These algorithms are tested both in simulation and in the field with a small twin hull differential thrust vessel built for the task.",
      "fieldsOfStudy": [
        "Computer Science"
      ],
      "topics": [
        "GP",
        "convex polygons",
        "efficient path planning",
        "ASV",
        "polygon",
        "coverage",
        "small embedded PC",
        "hyperparameter optimization",
        "small twin hull differential",
        "autonomous surface vessel",
        "Efficient methods",
        "GP framework",
        "New algorithms",
        "bounding polygon",
        "small twin hull differential thrust vessel",
        "underwater topography",
        "online fitting"
      ]
    }
  },
  {
    "sim": 0.48874726027061777,
    "gen": {
      "title": "Less is More: A Call to Focus on Simpler Models in Genetic Programming for Interpretable Machine Learning",
      "url": "https://www.semanticscholar.org/paper/0cf476ddad794a879968f233fdf791697a1c280b",
      "abstract": ". Interpretability can be critical for the safe and responsible use of ma- chine learning models in high-stakes applications. So far, evolutionary computation (EC), in particular in the form of genetic programming (GP), represents a key enabler for the discovery of interpretable machine learning (IML) models. In this short paper, we argue that research in GP for IML needs to focus on searching in the space of low-complexity models, by investigating new kinds of search strate-gies and recombination methods. Moreover, based on our experience of bringing research into clinical practice, we believe that research should strive to design better ways of modelling and pursuing interpretability, for the obtained solutions to ultimately be most useful.",
      "fieldsOfStudy": [
        "Computer Science"
      ],
      "topics": [
        "new kinds",
        "IML",
        "better ways",
        "ma- chine learning models",
        "low-complexity models",
        "research",
        "high-stakes applications",
        "search strate-gies and recombination methods",
        "genetic programming",
        "clinical practice",
        "interpretability",
        "modelling",
        "GP",
        "obtained solutions",
        "recombination methods",
        "search strate-gies"
      ]
    },
    "org": {
      "title": "On the Sampling Problem for Kernel Quadrature",
      "url": "https://www.semanticscholar.org/paper/7b4d3112d4e08c99d78739b2db75408c8bcd883e",
      "abstract": "The standard Kernel Quadrature method for numerical integration with random point sets (also called Bayesian Monte Carlo) is known to converge in root mean square error at a rate determined by the ratio $s/d$, where $s$ and $d$ encode the smoothness and dimension of the integrand. However, an empirical investigation reveals that the rate constant $C$ is highly sensitive to the distribution of the random points. In contrast to standard Monte Carlo integration, for which optimal importance sampling is well-understood, the sampling distribution that minimises $C$ for Kernel Quadrature does not admit a closed form. This paper argues that the practical choice of sampling distribution is an important open problem. One solution is considered; a novel automatic approach based on adaptive tempering and sequential Monte Carlo. Empirical results demonstrate a dramatic reduction in integration error of up to 4 orders of magnitude can be achieved with the proposed method.",
      "fieldsOfStudy": [
        "Computer Science",
        "Mathematics"
      ],
      "topics": [
        "Bayesian Monte Carlo",
        "standard Monte Carlo integration",
        "Monte Carlo",
        "sequential Monte Carlo",
        "integration error",
        "root mean square error",
        "numerical integration",
        "distribution",
        "Kernel Quadrature",
        "dimension",
        "adaptive tempering",
        "The standard Kernel Quadrature method",
        "Kernel",
        "random points",
        "optimal importance sampling",
        "sampling distribution"
      ]
    }
  },
  null,
  {
    "sim": 0.6908711413832687,
    "gen": {
      "title": "Separating value functions across time-scales",
      "url": "https://www.semanticscholar.org/paper/5dd064334057eaf1ecf19bb31ade3cd032dc205c",
      "abstract": "In many finite horizon episodic reinforcement learning (RL) settings, it is desirable to optimize for the undiscounted return - in settings like Atari, for instance, the goal is to collect the most points while staying alive in the long run. Yet, it may be difficult (or even intractable) mathematically to learn with this target. As such, temporal discounting is often applied to optimize over a shorter effective planning horizon. This comes at the risk of potentially biasing the optimization target away from the undiscounted goal. In settings where this bias is unacceptable - where the system must optimize for longer horizons at higher discounts - the target of the value function approximator may increase in variance leading to difficulties in learning. We present an extension of temporal difference (TD) learning, which we call TD($\\Delta$), that breaks down a value function into a series of components based on the differences between value functions with smaller discount factors. The separation of a longer horizon value function into these components has useful properties in scalability and performance. We discuss these properties and show theoretic and empirical improvements over standard TD learning in certain settings.",
      "fieldsOfStudy": [
        "Computer Science",
        "Mathematics"
      ],
      "topics": [
        "longer horizons",
        "longer horizon value function",
        "certain settings",
        "smaller discount factors",
        "standard TD learning",
        "settings",
        "higher discounts",
        "finite horizon",
        "value function approximator",
        "useful properties",
        "value function",
        "components",
        "performance",
        "temporal difference",
        "learning",
        "shorter effective planning horizon"
      ]
    },
    "org": {
      "title": "A Human Mixed Strategy Approach to Deep Reinforcement Learning",
      "url": "https://www.semanticscholar.org/paper/d611817963a682821043e1796343d020f815841a",
      "abstract": "In 2015, Google's Deepmind announced an advancement in creating an autonomous agent based on deep reinforcement learning (DRL) that could beat a professional player in a series of 49 Atari games. However, the current manifestation of DRL is still immature, and has significant drawbacks. One of DRL's imperfections is its lack of \"exploration\" during the training process, especially when working with high-dimensional problems. In this paper, we propose a mixed strategy approach that mimics behaviors of human when interacting with environment, and create a \"thinking\" agent that allows for more efficient exploration in the DRL training process. The simulation results based on the Breakout game show that our scheme achieves a higher probability of obtaining a maximum score than does the baseline DRL algorithm, i.e., the asynchronous advantage actor-critic method. The proposed scheme therefore can be applied effectively to solving a complicated task in a real-world application.",
      "fieldsOfStudy": [
        "Computer Science",
        "Mathematics"
      ],
      "topics": [
        "DRL",
        "significant drawbacks",
        "deep reinforcement learning",
        "high-dimensional problems",
        "baseline DRL algorithm",
        "Atari",
        "higher probability",
        "environment",
        "49 Atari games",
        "training process",
        "human",
        "maximum score",
        "DRLs imperfections",
        "efficient exploration",
        "Breakout game",
        "behaviors"
      ]
    }
  },
  null,
  {
    "sim": 0.5968897218643138,
    "gen": {
      "title": "SHNE: Representation Learning for Semantic-Associated Heterogeneous Networks",
      "url": "https://www.semanticscholar.org/paper/2f38368505f1cc36e2edb96b42acde70130d484b",
      "abstract": "Representation learning in heterogeneous networks faces challenges due to heterogeneous structural information of multiple types of nodes and relations, and also due to the unstructured attribute or content (e.g., text) associated with some types of nodes. While many recent works have studied homogeneous, heterogeneous, and attributed networks embedding, there are few works that have collectively solved these challenges in heterogeneous networks. In this paper, we address them by developing a Semantic-aware Heterogeneous Network Embedding model (SHNE). SHNE performs joint optimization of heterogeneous SkipGram and deep semantic encoding for capturing both heterogeneous structural closeness and unstructured semantic relations among all nodes, as function of node content, that exist in the network. Extensive experiments demonstrate that SHNE outperforms state-of-the-art baselines in various heterogeneous network mining tasks, such as link prediction, document retrieval, node recommendation, relevance search, and class visualization.",
      "fieldsOfStudy": [
        "Computer Science"
      ],
      "topics": [
        "heterogeneous networks",
        "heterogeneous network mining tasks",
        "heterogeneous structural information",
        "node recommendation",
        "heterogeneous SkipGram",
        "unstructured semantic relations",
        "nodes",
        "networks",
        "class visualization",
        "node content",
        "multiple types",
        "deep semantic encoding",
        "relevance search",
        "content",
        "Heterogeneous Network Embedding",
        "document retrieval",
        "works"
      ]
    },
    "org": {
      "title": "A Distributed Synchronous SGD Algorithm with Global Top-k Sparsification for Low Bandwidth Networks",
      "url": "https://www.semanticscholar.org/paper/22c844d939f755f07b4e78ab830586f10cb0fa6d",
      "abstract": "Distributed synchronous stochastic gradient descent (S-SGD) with data parallelism has been widely used in training large-scale deep neural networks (DNNs), but it typically requires very high communication bandwidth between computational workers (e.g., GPUs) to exchange gradients iteratively. Recently, Top-k sparsification techniques have been proposed to reduce the volume of data to be exchanged among workers and thus alleviate the network pressure. Top-k sparsification can zero-out a significant portion of gradients without impacting the model convergence. However, the sparse gradients should be transferred with their indices, and the irregular indices make the sparse gradients aggregation difficult. Current methods that use AllGather to accumulate the sparse gradients have a communication complexity of O(kP), where P is the number of workers, which is inefficient on low bandwidth networks with a large number of workers. We observe that not all top-k gradients from P workers are needed for the model update, and therefore we propose a novel global Top-k (gTop-k) sparsification mechanism to address the difficulty of aggregating sparse gradients. Specifically, we choose global top-k largest absolute values of gradients from P workers, instead of accumulating all local top-k gradients to update the model in each iteration. The gradient aggregation method based on gTop-k sparsification, namely gTopKAllReduce, reduces the communication complexity from O(kP) to O(k log P). Through extensive experiments on different DNNs, we verify that gTop-k S-SGD has nearly consistent convergence performance with S-SGD, and it has only slight degradations on generalization performance. In terms of scaling efficiency, we evaluate gTop-k on a cluster with 32 GPU machines which are interconnected with 1 Gbps Ethernet. The experimental results show that our method achieves 2.7-12\u00d7 higher scaling efficiency than S-SGD with dense gradients and 1.1-1.7\u00d7 improvement than the existing Top-k S-SGD.",
      "fieldsOfStudy": [
        "Computer Science"
      ],
      "topics": [
        "dense gradients",
        "gradients",
        "computational workers",
        "workers",
        "low bandwidth networks",
        "generalization performance",
        "data parallelism",
        "large-scale deep neural networks",
        "log P",
        "sparse gradients",
        "S-SGD",
        "gTop-k S-SGD",
        "Distributed synchronous stochastic gradient descent",
        "O(k log P",
        "sparse gradients aggregation",
        "global top-k largest absolute values"
      ]
    }
  },
  {
    "sim": 0.6428273750863638,
    "gen": {
      "title": "Review on the Application of Metalearning in Artificial Intelligence",
      "url": "https://www.semanticscholar.org/paper/6e30ea095728d00deceae9cd036e152f61ae290b",
      "abstract": "In recent years, artificial intelligence supported by big data has gradually become more dependent on deep reinforcement learning. However, the application of deep reinforcement learning in artificial intelligence is limited by prior knowledge and model selection, which further affects the efficiency and accuracy of prediction, and also fails to realize the learning ability of autonomous learning and prediction. Metalearning came into being because of this. Through learning the information metaknowledge, the ability to autonomously judge and select the appropriate model can be formed, and the parameters can be adjusted independently to achieve further optimization. It is a novel method to solve big data problems in the current neural network model, and it adapts to the development trend of artificial intelligence. This article first briefly introduces the research process and basic theory of metalearning and discusses the differences between metalearning and machine learning and the research direction of metalearning in big data. Then, four typical applications of metalearning in the field of artificial intelligence are summarized: few-shot learning, robot learning, unsupervised learning, and intelligent medicine. Then, the challenges and solutions of metalearning are analyzed. Finally, a systematic summary of the full text is made, and the future development prospect of this field is assessed.",
      "fieldsOfStudy": [
        "Computer Science",
        "Medicine"
      ],
      "topics": [
        "autonomous learning",
        "unsupervised learning",
        "robot learning",
        "machine learning",
        "deep reinforcement learning",
        "artificial intelligence",
        "big data problems",
        "big data",
        "intelligent medicine",
        "optimization",
        "metalearning",
        "learning ability",
        "prediction",
        "prior knowledge and model selection",
        "current neural network model",
        "model selection",
        "few-shot learning"
      ]
    },
    "org": {
      "title": "Transforming task representations to allow deep learning models to perform novel tasks",
      "url": "https://www.semanticscholar.org/paper/5ea7d562df6aac215630df8abf4a1321a9e47e6e",
      "abstract": "An important aspect of intelligence is the ability to adapt to a novel task without any direct experience (zero-shot), based on its relationship to previous tasks. Humans can exhibit this cognitive flexibility. By contrast, deep-learning models that achieve superhuman performance in specific tasks generally fail to adapt to even slight task alterations. To address this, we propose a general computational framework for adapting to novel tasks based on their relationship to prior tasks. We begin by learning vector representations of tasks. To adapt to new tasks, we propose meta-mappings, higher-order tasks that transform basic task representations. We demonstrate this framework across a wide variety of tasks and computational paradigms, ranging from regression to image classification and reinforcement learning. We compare to both human adaptability, and language-based approaches to zero-shot learning. Across these domains, meta-mapping is successful, often achieving 80-90% performance, without any data, on a novel task that directly contradicts its prior experience. We further show that using meta-mapping as a starting point can dramatically accelerate later learning on a new task, and reduce learning time and cumulative error substantially. Our results provide insight into a possible computational basis of intelligent adaptability, and offer a possible framework for modeling cognitive flexibility and building more flexible artificial intelligence.",
      "fieldsOfStudy": [
        "Computer Science",
        "Mathematics"
      ],
      "topics": [
        "previous tasks",
        "tasks",
        "specific tasks",
        "basic task representations",
        "learning time",
        "slight task alterations",
        "new task",
        "computational paradigms",
        "intelligence",
        "image classification and reinforcement learning",
        "vector representations",
        "reinforcement learning",
        "cumulative error",
        "higher-order tasks",
        "superhuman performance"
      ]
    }
  },
  null,
  null,
  null,
  null,
  {
    "sim": 0.7627011728420954,
    "gen": {
      "title": "Optimal DG allocation and sizing in power system networks using swarm-based algorithms",
      "url": "https://www.semanticscholar.org/paper/3f60016cdcd2f484c290f9f6694cbd4e795471fd",
      "abstract": "Distributed generation (DG) units are power generating plants that are very important to the architecture of present power system networks. The benefit of the addition of these DG units is to increase the power supply to a network. However, the installation of these DG units can cause an adverse effect if not properly allocated and/or sized. Therefore, there is a need to optimally allocate and size them to avoid cases such as voltage instability and expensive investment costs. In this paper, two swarm-based meta-heuristic algorithms, particle swarm optimization (PSO) and whale optimization algorithm (WOA) were developed to solve optimal placement and sizing of DG units in the quest for transmission network planning. A supportive technique, loss sensitivity factors (LSF) was used to identify potential buses for optimal location of DG units. The feasibility of the algorithms was confirmed on two IEEE bus test systems (14- and 30-bus). Comparison results showed that both algorithms produce good solutions and they outperform each other in different metrics. The WOA real power loss reduction considering techno-economic factors in the IEEE 14-bus and 30-bus test system are 6.14 MW and 10.77 MW, compared to the PSOs' 6.47 MW and 11.73 MW respectively. The PSO has a more reduced total DG unit size in both bus systems with 133.45 MW and 82.44 MW compared to WOAs' 152.21 MW and 82.44 MW respectively. The paper unveils the strengths and weaknesses of the PSO and the WOA in the application of optimal sizing of DG units in transmission networks.",
      "fieldsOfStudy": [
        "Computer Science",
        "Engineering",
        "Mathematics"
      ],
      "topics": [
        "MW",
        "present power system networks",
        "DG units",
        "potential buses",
        "transmission networks",
        "expensive investment costs",
        "10.77 MW",
        "11.73 MW",
        "133.45 MW",
        "6.14 MW",
        "82.44 MW",
        "PSOs",
        "DG",
        "optimal sizing",
        "152.21 MW",
        "6.47 MW",
        "WOAs",
        "power generating plants",
        "WOAs 152.21 MW"
      ]
    },
    "org": {
      "title": "Multi-Objective Optimal Reactive Power Dispatch of Power Systems by Combining Classification-Based Multi-Objective Evolutionary Algorithm and Integrated Decision Making",
      "url": "https://www.semanticscholar.org/paper/06df99366340293fcd5176b29d733a34db66f824",
      "abstract": "For the purpose of addressing the multi-objective optimal reactive power dispatch (MORPD) problem, a two-step approach is proposed in this paper. First of all, to ensure the economy and security of the power system, the MORPD model aiming to minimize active power loss and voltage deviation is formulated. And then the two-step approach integrating decision-making into optimization is proposed to solve the model. Specifically speaking, the first step aims to seek the Pareto optimal solutions (POSs) with good distribution by using a multi-objective optimization (MOO) algorithm named classification and Pareto domination based multi-objective evolutionary algorithm (CPSMOEA). Furthermore, the reference Pareto-optimal front is generated to validate the Pareto front obtained using CPSMOEA; in the second step, integrated decision-making by combining fuzzy c-means algorithm (FCM) with grey relation projection method (GRP) aims to extract the best compromise solutions which reflect the preferences of decision-makers from the POSs. Based on the test results on the IEEE 30-bus and IEEE 118-bus test systems, it is demonstrated that the proposed approach not only manages to address the MORPD issue but also outperforms other commonly-used MOO algorithms including multi-objective particle swarm optimization (MOPSO), preference-inspired coevolutionary algorithm (PICEAg) and the third evolution step of generalized differential evolution (GDE3).",
      "fieldsOfStudy": [
        "Computer Science",
        "Mathematics",
        "Engineering"
      ],
      "topics": [
        "multi-objective particle swarm optimization",
        "classification and Pareto domination based multi-objective evolutionary algorithm",
        "generalized differential evolution",
        "active power loss",
        "good distribution",
        "grey relation projection method",
        "multi-objective optimal reactive power dispatch",
        "Pareto",
        "POSs",
        "grey relation",
        "voltage deviation",
        "optimization",
        "MORPD",
        "CPSMOEA",
        "evolution step",
        "Pareto domination based multi-objective evolutionary algorithm",
        "preference-inspired coevolutionary algorithm",
        "fuzzy c-means algorithm",
        "multi-objective optimal reactive power dispatch (MORPD) problem",
        "multi-objective optimization (MOO) algorithm"
      ]
    }
  },
  {
    "sim": 0.23482792673114605,
    "gen": {
      "title": "The Leiden Manifesto for research metrics",
      "url": "https://www.semanticscholar.org/paper/1e683c22969a158083680a245edae9ea9f0b6a3d",
      "abstract": null,
      "fieldsOfStudy": [
        "Art"
      ]
    },
    "org": {
      "title": "RMITB at TREC COVID 2020",
      "url": "https://www.semanticscholar.org/paper/6aae04c3aacda91fe6a4dc04fd4e0638d0f55467",
      "abstract": "Search engine users rarely express an information need using the same query, and small differences in queries can lead to very different result sets. These user query variations have been exploited in past TREC CORE tracks to contribute diverse, highly-effective runs in offline evaluation campaigns with the goal of producing reusable test collections. In this paper, we document the query fusion runs submitted to the first and second round of TREC COVID, using ten queries per topic created by the first author. In our analysis, we focus primarily on the effects of having our second priority run omitted from the judgment pool. This run is of particular interest, as it surfaced a number of relevant documents that were not judged until later rounds of the task. If the additional judgments were included in the first round, the performance of this run increased by 35 rank positions when using RBP p=0.5, highlighting the importance of judgment depth and coverage in assessment tasks.",
      "fieldsOfStudy": [
        "Computer Science"
      ],
      "topics": [
        "assessment tasks",
        "reusable test collections",
        "judgment depth",
        "queries",
        "offline evaluation campaigns",
        "TREC COVID",
        "RBP p=0.5",
        "query fusion runs",
        "small differences",
        "relevant documents",
        "second priority run",
        "second",
        "coverage",
        "round",
        "later rounds",
        "past TREC CORE tracks",
        "TREC CORE"
      ]
    }
  },
  {
    "sim": 0.2589383157673657,
    "gen": {
      "title": "Explainable AI: Deep Reinforcement Learning Agents for Residential Demand Side Cost Savings in Smart Grids",
      "url": "https://www.semanticscholar.org/paper/1823c6060a28a9a9b61c0d63e915ad98a60d99c9",
      "abstract": "Motivated by recent advancements in Deep Reinforcement Learning (RL), we have developed an RL agent to manage the operation of storage devices in a household and is designed to maximize demand-side cost savings. The proposed technique is data-driven, and the RL agent learns from scratch how to efficiently use the energy storage device given variable tariff structures. In most of the studies, the RL agent is considered as a black box, and how the agent has learned is often ignored. We explain the learning progression of the RL agent, and the strategies it follows based on the capacity of the storage device.",
      "fieldsOfStudy": [
        "Computer Science",
        "Mathematics"
      ],
      "topics": [
        "variable tariff structures",
        "RL",
        "demand-side cost savings",
        "Deep Reinforcement Learning",
        "storage device",
        "RL agent",
        "scratch",
        "recent advancements",
        "agent",
        "black box",
        "household",
        "learning progression"
      ]
    },
    "org": {
      "title": "Evolving NoSQL Databases without Downtime",
      "url": "https://www.semanticscholar.org/paper/9bb4a0866801bf04d16cb77a2fd92b1ab6355e06",
      "abstract": "NoSQL databases like Redis, Cassandra, and Mon-goDB are increasingly popular because they are flexible, lightweight, and easy to work with. Applications that use these databases will evolve over time, sometimes necessitating (or preferring) a change to the format or organization of the data. The problem we address in this paper is: How can we support the evolution of high-availability applications and their NoSQL data online, without excessive delays or interruptions, even in the presence of backward-incompatible data format changes? We present KVolve, an extension to the popular Redis NoSQL database, as a solution to this problem. KVolve permits a developer to submit an upgrade specification that defines how to transform existing data to the newest version. This transformation is applied lazily as applications interact with the database, thus avoiding long pause times. We demonstrate that KVolve is expressive enough to support substantial practical updates, including format changes to RedisFS, a Redis-backed file system, while imposing essentially no overhead in general use and minimal pause times during updates.",
      "fieldsOfStudy": [
        "Computer Science"
      ],
      "topics": [
        "format changes",
        "minimal pause times",
        "long pause times",
        "backward-incompatible data format changes",
        "time",
        "existing data",
        "Redis NoSQL",
        "substantial practical updates",
        "general use",
        "updates",
        "organization",
        "RedisFS",
        "excessive delays",
        "popular Redis NoSQL database",
        "NoSQL databases",
        "NoSQL data"
      ]
    }
  },
  null,
  {
    "sim": 0.5753472974033711,
    "gen": {
      "title": "Robustness of power systems under a democratic fiber bundle-like model",
      "url": "https://www.semanticscholar.org/paper/06a767050e8e79310459dc857416368e9d36801d",
      "abstract": "We consider a power system with N transmission lines whose initial loads (i.e., power flows) L(1),...,L(N) are independent and identically distributed with P(L)(x)=P[L\u2264x]. The capacity C(i) defines the maximum flow allowed on line i and is assumed to be given by C(i)=(1+\u03b1)L(i), with \u03b1>0. We study the robustness of this power system against random attacks (or failures) that target a p fraction of the lines, under a democratic fiber-bundle-like model. Namely, when a line fails, the load it was carrying is redistributed equally among the remaining lines. Our contributions are as follows. (i) We show analytically that the final breakdown of the system always takes place through a first-order transition at the critical attack size p(\u2606)=1-(E[L]/max(x)(P[L>x](\u03b1x+E[L|L>x])), where E[\u00b7] is the expectation operator; (ii) we derive conditions on the distribution P(L)(x) for which the first-order breakdown of the system occurs abruptly without any preceding diverging rate of failure; (iii) we provide a detailed analysis of the robustness of the system under three specific load distributions-uniform, Pareto, and Weibull-showing that with the minimum load L(min) and mean load E[L] fixed, Pareto distribution is the worst (in terms of robustness) among the three, whereas Weibull distribution is the best with shape parameter selected relatively large; (iv) we provide numerical results that confirm our mean-field analysis; and (v) we show that p(\u2606) is maximized when the load distribution is a Dirac delta function centered at E[L], i.e., when all lines carry the same load. This last finding is particularly surprising given that heterogeneity is known to lead to high robustness against random failures in many other systems.",
      "fieldsOfStudy": [
        "Mathematics",
        "Computer Science",
        "Medicine"
      ],
      "topics": [
        "load",
        "Pareto distribution",
        "Weibull distribution",
        "line",
        "N transmission lines",
        "systems",
        "random failures",
        "specific load distributions",
        "random attacks",
        "load distribution",
        "robustness",
        "failures",
        "minimum load L(min",
        "mean load E[L"
      ]
    },
    "org": {
      "title": "Stability of the Max-Weight Protocol in Adversarial Wireless Networks",
      "url": "https://www.semanticscholar.org/paper/a76a1c234d69b22537b8354fc9d8960902ea7827",
      "abstract": "In this paper, we consider the Max-Weight protocol for routing and scheduling in wireless networks under an adversarial model. This protocol has received a significant amount of attention dating back to the papers of Tassiulas and Ephremides. In particular, this protocol is known to be throughput-optimal whenever the traffic patterns and propagation conditions are governed by a stationary stochastic process. However, the standard proof of throughput optimality (which is based on the negative drift of a quadratic potential function) does not hold when the traffic patterns and the edge capacity changes over time are governed by an arbitrary adversarial process. Such an environment appears frequently in many practical wireless scenarios when the assumption that channel conditions are governed by a stationary stochastic process does not readily apply. In this paper, we prove that even in the above adversarial setting, the Max-Weight protocol keeps the queues in the network stable (i.e., keeps the queue sizes bounded) whenever this is feasible by some routing and scheduling algorithm. However, the proof is somewhat more complex than the negative potential drift argument that applied in the stationary case. Our proof holds for any arbitrary interference relationships among edges. We also prove the same stability of \u03b5-approximate Max-Weight under the adversarial model. We conclude the paper with a discussion of queue sizes in the adversarial model as well as a set of simulation results.",
      "fieldsOfStudy": [
        "Computer Science"
      ],
      "topics": [
        "simulation results",
        "stationary stochastic process",
        "wireless networks",
        "arbitrary adversarial process",
        "channel conditions",
        "edges",
        "queue sizes",
        "propagation conditions",
        "stationary case",
        "practical wireless scenarios",
        "time",
        "scheduling",
        "Max",
        "routing",
        "adversarial model"
      ]
    }
  },
  {
    "sim": 0.5137795599682314,
    "gen": {
      "title": "On the throughput capacity of wireless multi-hop networks with ALOHA, node coloring and CSMA",
      "url": "https://www.semanticscholar.org/paper/c554b4a5e54f06d23d12269f30c7269dfcebe8be",
      "abstract": "We quantify the throughput capacity of wireless multi-hop networks with several medium access schemes. We analyze pure ALOHA scheme where simultaneous transmitters are dispatched according to a uniform Poisson distribution and exclusion schemes where simultaneous transmitters are dispatched according to an exclusion rule such as node coloring and carrier sense based schemes. We consider both no-fading and standard Rayleigh fading channel models. Our results show that, under no-fading, slotted ALOHA can achieve at least one-third (or half under Rayleigh fading) of the throughput capacity of node coloring scheme whereas carrier sense based scheme can achieve almost the same throughput capacity as node coloring.",
      "fieldsOfStudy": [
        "Computer Science"
      ],
      "topics": [
        "carrier sense based schemes",
        "node coloring scheme",
        "exclusion schemes",
        "medium access schemes",
        "pure ALOHA scheme",
        "node coloring",
        "simultaneous transmitters",
        "wireless multi-hop networks",
        "Rayleigh fading",
        "Poisson",
        "Rayleigh",
        "throughput capacity",
        "uniform Poisson distribution",
        "exclusion rule",
        "standard Rayleigh fading channel models",
        "slotted ALOHA",
        "half"
      ]
    },
    "org": {
      "title": "Dynamic Local Search for the Maximum Clique Problem",
      "url": "https://www.semanticscholar.org/paper/634c93078e1105c20ebec1ffd63eef348c88719a",
      "abstract": "In this paper, we introduce DLS-MC, a new stochastic local search algorithm for the maximum clique problem. DLS-MC alternates between phases of iterative improvement, during which suitable vertices are added to the current clique, and plateau search, during which vertices of the current clique are swapped with vertices not contained in the current clique. The selection of vertices is solely based on vertex penalties that are dynamically adjusted during the search, and a perturbation mechanism is used to overcome search stagnation. The behaviour of DLS-MC is controlled by a single parameter, penalty delay, which controls the frequency at which vertex penalties are reduced. We show empirically that DLSMC achieves substantial performance improvements over state-of-the-art algorithms for the maximum clique problem over a large range of the commonly used DIMACS benchmark instances.",
      "fieldsOfStudy": [
        "Computer Science",
        "Mathematics"
      ],
      "topics": [
        "vertex penalties",
        "search stagnation",
        "suitable vertices",
        "vertices",
        "penalty delay",
        "maximum clique problem",
        "current clique",
        "substantial performance improvements",
        "new stochastic local search algorithm",
        "DIMACS",
        "iterative improvement",
        "commonly used DIMACS benchmark instances",
        "vertices",
        "search",
        "large range",
        "plateau search"
      ]
    }
  },
  {
    "sim": 0.3545559554443397,
    "gen": {
      "title": "Mining frequent items in the time fading model",
      "url": "https://www.semanticscholar.org/paper/2988711e6ef92599e78e1c6166d3416eca270907",
      "abstract": null,
      "fieldsOfStudy": [
        "Computer Science"
      ]
    },
    "org": {
      "title": "Software Metrics Evaluation Based on Entropy",
      "url": "https://www.semanticscholar.org/paper/169bdafdc816843784c87a1d44d50a6e8d76bda0",
      "abstract": "Software engineering activities in the Industry has come a l ong way with various improve- ments brought in various stages of the software development life cycle. The complexity of modern software, the commercial constraints and the expectation f or high quality products demand the accurate fault prediction based on OO design metrics in the class level in the early stages of software development. The object oriented class metrics are used as quality predic tors in the entire OO software development life cycle even when a highly iterative, incremental model or agile software process is employed. Recent research has shown some of the OO design metrics are useful for predicting fault-proneness of classes. In this paper the empirical validation of a set of metrics propo sed by Chidamber and Kemerer is performed to assess their ability in predicting the software quality i n terms of fault proneness and degradation. We have also proposed the design complexity of object-oriented software with Weighted Methods per Class metric (WMC-CK metric) expressed in terms of Shannon entropy, and error proneness.",
      "fieldsOfStudy": [
        "Computer Science"
      ],
      "topics": [
        "software development",
        "agile software process",
        "OO design metrics",
        "modern software",
        "Software engineering activities",
        "fault proneness",
        "metrics propo",
        "software development life cycle",
        "quality predic tors",
        "high quality products",
        "stages",
        "classes",
        "OO",
        "error proneness",
        "metrics",
        "improve- ments",
        "degradation"
      ]
    }
  },
  {
    "sim": 0.5715408114463901,
    "gen": {
      "title": "Analysis of Social Group Dynamics",
      "url": "https://www.semanticscholar.org/paper/46343f02a77c01917a66de5e4287bdaf044efe64",
      "abstract": "The continuous interest in the social network area contributes to the fast development of this field. New possibilities of obtaining and storing data allows for more and more deeper analysis of the network in general, as well as groups and individuals within it. Especially interesting is studying the dynamics of changes in social groups over time. Having such knowledge ones may attempt to predict the future of the group, and then manage it properly in order to achieve presumed goals. Such ability would be a powerful tool in the hands of human resource managers, personnel recruitment, marketing, etc. The thesis presents a new method for exploring the evolution of social groups, called Group Evolution Discovery (GED). Next, the results of its use are provided together with comparison to two other algorithms in terms of accuracy, execution time, flexibility and ease of implementation. Moreover, the method was evaluated with various measures of user importance within a group. Obtained results suggest that GED is the best method for analyzing social group dynamics.",
      "fieldsOfStudy": [
        "Computer Science",
        "Physics"
      ],
      "topics": [
        "social group dynamics",
        "social groups",
        "groups",
        "presumed goals",
        "Group Evolution Discovery",
        "GED",
        "implementation",
        "execution time",
        "time",
        "marketing",
        "personnel recruitment",
        "human resource managers",
        "ease",
        "user importance",
        "individuals"
      ]
    },
    "org": {
      "title": "Big Data for Traffic Monitoring and Management",
      "url": "https://www.semanticscholar.org/paper/4b8467251495c98d62607a3c38ea0f06ebf3b06e",
      "abstract": "The last two decades witnessed tremendous advances in the Information and Communications Technologies. Beside improvements in computational power and storage capacity, communication networks carry nowadays an amount of data which was not envisaged only few years ago. Together with their pervasiveness, network complexity increased at the same pace, leaving operators and researchers with few instruments to understand what happens in the networks, and, on the global scale, on the Internet. Fortunately, recent advances in data science and machine learning come to the rescue of network analysts, and allow analyses with a level of complexity and spatial/temporal scope not possible only 10 years ago. In my thesis, I take the perspective of an Internet Service Provider (ISP), and illustrate challenges and possibilities of analyzing the traffic coming from modern operational networks. I make use of big data and machine learning algorithms, and apply them to datasets coming from passive measurements of ISP and University Campus networks. The marriage between data science and network measurements is complicated by the complexity of machine learning algorithms, and by the intrinsic multi-dimensionality and variability of this kind of data. As such, my work proposes and evaluates novel techniques, inspired from popular machine learning approaches, but carefully tailored to operate with network traffic.",
      "fieldsOfStudy": [
        "Computer Science"
      ],
      "topics": [
        "network complexity",
        "network traffic",
        "modern operational networks",
        "network measurements",
        "network analysts",
        "communication networks",
        "popular machine learning approaches",
        "instruments",
        "ISP and University Campus networks",
        "data",
        "ISP",
        "big data and machine learning algorithms",
        "passive measurements",
        "University Campus",
        "machine learning",
        "complexity"
      ]
    }
  },
  null,
  null,
  {
    "sim": 0.6611669800620856,
    "gen": {
      "title": "Bayesian policy reuse",
      "url": "https://www.semanticscholar.org/paper/f2bb61f3f8b88a76c85e98df2871e09e53fae77a",
      "abstract": null,
      "fieldsOfStudy": [
        "Computer Science"
      ]
    },
    "org": {
      "title": "Bayes-ToMoP: A Fast Detection and Best Response Algorithm Towards Sophisticated Opponents",
      "url": "https://www.semanticscholar.org/paper/27bf2cba9d7115863cf3a0be65be188014600279",
      "abstract": "Multiagent algorithms often aim to accurately predict the behaviors of other agents and find a best response accordingly. Previous works usually assume an opponent uses a stationary strategy or randomly switches among several stationary ones. However, an opponent may exhibit more sophisticated behaviors by adopting more advanced reasoning strategies, e.g., using a Bayesian reasoning strategy. This paper proposes a novel approach called Bayes-ToMoP which can efficiently detect the strategy of opponents using either stationary or higher-level reasoning strategies. Bayes-ToMoP also supports the detection of previously unseen policies and learning a best-response policy accordingly. We also propose a deep version of Bayes-ToMoP by extending Bayes-ToMoP with DRL techniques. Experimental results show both Bayes-ToMoP and deep Bayes-ToMoP outperform the state-of-the-art approaches when faced with different types of opponents in two-agent competitive games.",
      "fieldsOfStudy": [
        "Computer Science"
      ],
      "topics": [
        "stationary ones",
        "best response",
        "Bayesian reasoning strategy",
        "advanced reasoning strategies",
        "previously unseen policies",
        "stationary strategy",
        "opponents",
        "Bayes",
        "agents",
        "Bayesian",
        "different types",
        "stationary or higher-level reasoning strategies",
        "DRL techniques",
        "deep Bayes-ToMoP",
        "higher-level reasoning strategies",
        "strategy",
        "Bayes-ToMoP"
      ]
    }
  },
  null,
  {
    "sim": 0.5133651965443254,
    "gen": {
      "title": "Fast and Efficient Compressive Sensing Using Structurally Random Matrices",
      "url": "https://www.semanticscholar.org/paper/ce720b9163150692b7be2f28c1b37e36452a44d0",
      "abstract": "This paper introduces a new framework to construct fast and efficient sensing matrices for practical compressive sensing, called Structurally Random Matrix (SRM). In the proposed framework, we prerandomize the sensing signal by scrambling its sample locations or flipping its sample signs and then fast-transform the randomized samples and finally, subsample the resulting transform coefficients to obtain the final sensing measurements. SRM is highly relevant for large-scale, real-time compressive sensing applications as it has fast computation and supports block-based processing. In addition, we can show that SRM has theoretical sensing performance comparable to that of completely random sensing matrices. Numerical simulation results verify the validity of the theory and illustrate the promising potentials of the proposed sensing framework.",
      "fieldsOfStudy": [
        "Computer Science",
        "Mathematics"
      ],
      "topics": [
        "practical compressive sensing",
        "theoretical sensing performance",
        "fast and efficient sensing matrices",
        "Structurally Random Matrix",
        "completely random sensing matrices",
        "final sensing measurements",
        "SRM",
        "proposed sensing framework",
        "fast computation",
        "sensing signal",
        "large-scale, real-time compressive sensing applications",
        "subsample",
        "block-based processing",
        "randomized samples",
        "sample signs",
        "sample locations",
        "resulting transform coefficients"
      ]
    },
    "org": {
      "title": "MAMNet: Multi-path adaptive modulation network for image super-resolution",
      "url": "https://www.semanticscholar.org/paper/2801eac81abdb17fa026c824096dacd0f9ce36b1",
      "abstract": null,
      "fieldsOfStudy": [
        "Computer Science"
      ]
    }
  },
  {
    "sim": 0.5441983142362433,
    "gen": {
      "title": "Robust Distributed Multi-Source Detection and Labeling in Wireless Acoustic Sensor Networks",
      "url": "https://www.semanticscholar.org/paper/b93bd8f45786a360b0c2559e6f7b60aa426b6ea2",
      "abstract": "The growing demand in complex signal processing methods associated with low-energy large scale wireless acoustic sensor networks (WASNs) urges the shift to a new information and communication technologies (ICT) paradigm. \nThe emerging research perception aspires for an appealing wireless network communication where multiple heterogeneous devices with different interests can cooperate in various signal processing tasks (MDMT). \nContributions in this doctoral thesis focus on distributed multi-source detection and labeling applied to audio enhancement scenarios pursuing an MDMT fashioned node-specific source-of-interest signal enhancement in WASNs. \nIn fact, an accurate detection and labeling is a pre-requisite to pursue the MDMT paradigm where nodes in the WASN communicate effectively their sources-of-interest and, therefore, multiple signal processing tasks can be enhanced via cooperation. \n \nFirst, a novel framework based on a dominant source model in distributed WASNs for resolving the activity detection of multiple speech sources in a reverberant and noisy environment is introduced. \nA preliminary rank-one multiplicative non-negative independent component analysis (M-NICA) for unique dominant energy source extraction given associated node clusters is presented. \nPartitional algorithms that minimize the within-cluster mean absolute deviation (MAD) and weighted MAD objectives are proposed to determine the cluster membership of the unmixed energies, and thus establish a source specific voice activity recognition. \n \nIn a second study, improving the energy signal separation to alleviate the multiple source activity discrimination task is targeted. \nSparsity inducing penalties are enforced on iterative rank-one singular value decomposition layers to extract sparse right rotations. \nThen, sparse non-negative blind energy separation is realized using multiplicative updates. \nHence, the multiple source detection problem is converted into a sparse non-negative source energy decorrelation. \nSparsity tunes the supposedly non-active energy signatures to exactly zero-valued energies so that it is easier to identify active energies and an activity detector can be constructed in a straightforward manner. \nIn a centralized scenario, the activity decision is controlled by a fusion center that delivers the binary source activity detection for every participating energy source. \nThis strategy gives precise detection results for small source numbers. With a growing number of interfering sources, the distributed detection approach is more promising. \nConjointly, a robust distributed energy separation algorithm for multiple competing sources is proposed. \nA robust and regularized $t_{\\nu}M$-estimation of the covariance matrix of the mixed energies is employed. \nThis approach yields a simple activity decision using only the robustly unmixed energy signatures of the sources in the WASN. \nThe performance of the robust activity detector is validated with a distributed adaptive node-specific signal estimation method for speech enhancement. \nThe latter enhances the quality and intelligibility of the signal while exploiting the accurately estimated multi-source voice decision patterns. \nIn contrast to the original M-NICA for source separation, the extracted binary activity patterns with the robust energy separation significantly improve the node-specific signal estimation. \n \nDue to the increased computational complexity caused by the additional step of energy signal separation, a new approach to solving the detection question of multi-device multi-source networks is presented. \nStability selection for iterative extraction of robust right singular vectors is considered. The sub-sampling selection technique provides transparency in properly choosing the regularization variable in the Lasso optimization problem. \nIn this way, the strongest sparse right singular vectors using a robust $\\ell_1$-norm and stability selection are the set of basis vectors that describe the input data efficiently. \nActive/non-active source classification is achieved based on a robust Mahalanobis classifier. \nFor this, a robust $M$-estimator of the covariance matrix in the Mahalanobis distance is utilized. \nExtensive evaluation in centralized and distributed settings is performed to assess the effectiveness of the proposed approach. \nThus, overcoming the computationally demanding source separation scheme is possible via exploiting robust stability selection for sparse multi-energy feature extraction. \n \nWith respect to the labeling problem of various sources in a WASN, a robust approach is introduced that exploits the direction-of-arrival of the impinging source signals. \nA short-time Fourier transform-based subspace method estimates the angles of locally stationary wide band signals using a uniform linear array. \nThe median of angles estimated at every frequency bin is utilized to obtain the overall angle for each participating source. \nThe features, in this case, exploit the similarity across devices in the particular frequency bins that produce reliable direction-of-arrival estimates for each source. \nReliability is defined with respect to the median across frequencies. \nAll source-specific frequency bands that contribute to correct estimated angles are selected. \nA feature vector is formed for every source at each device by storing the frequency bin indices that lie within the upper and lower interval of the median absolute deviation scale of the estimated angle. \nLabeling is accomplished by a distributed clustering of the extracted angle-based feature vectors using consensus averaging.",
      "fieldsOfStudy": [
        "Computer Science"
      ],
      "topics": [
        "source separation",
        "distributed multi-source detection",
        "multiple speech sources",
        "sources",
        "unique dominant energy source extraction",
        "multiple competing sources",
        "interfering sources",
        "multi-device multi-source networks",
        "energy signal separation",
        "small source numbers",
        "sparse non-negative blind energy separation",
        "multiple signal processing tasks",
        "active energies",
        "sparse non-negative source energy decorrelation",
        "signal processing tasks"
      ]
    },
    "org": {
      "title": "Adaptive Convex Combination of APA and ZA-APA algorithms for Sparse System Identification",
      "url": "https://www.semanticscholar.org/paper/2df6ed6ae67b867979036f24be41623ee748f695",
      "abstract": "In general, one often encounters the systems that have sparse impulse response, with time varying system sparsity. Conventional adaptive filters which perform well for identification of non-sparse systems fail to exploit the system sparsity for improving the performance as the sparsity level increases. This paper presents a new approach that uses an adaptive convex combination of Affine Projection Algorithm (APA) and Zero-attracting Affine Projection Algorithm (ZA-APA)algorithms for identifying the sparse systems, which adapts dynamically to the sparsity of the system. Thus works well in both sparse and non-sparse environments and also the usage of affine projection makes it robust against colored input. It is shown that, for non-sparse systems, the proposed combination always converges to the APA algorithm, while for semi-sparse systems, it converges to a solution that produces lesser steady state EMSE than produced by either of the component filters. For highly sparse systems, depending on the value of the proportionality constant ($\\rho$) in ZA-APA algorithm, the proposed combined filter may either converge to the ZA-APA based filter or produce a solution similar to the semi-sparse case i.e., outerperforms both the constituent filters.",
      "fieldsOfStudy": [
        "Computer Science",
        "Mathematics"
      ],
      "topics": [
        "non-sparse systems",
        "time varying system sparsity",
        "-sparse environments",
        "sparse impulse response",
        "Conventional adaptive filters",
        "Affine Projection Algorithm",
        "APA",
        "lesser steady state EMSE",
        "colored input",
        "semi-sparse case",
        "ZA",
        "highly sparse systems",
        "system sparsity",
        "sparse and non-sparse environments",
        "proposed combined filter"
      ]
    }
  },
  {
    "sim": 0.4969023786857274,
    "gen": {
      "title": "Octopus: Privacy-Preserving Collaborative Evaluation of Loan Stacking",
      "url": "https://www.semanticscholar.org/paper/d78ac98bc09294c45eceb92f426e0043f98365fd",
      "abstract": "With the rise of online lenders, the loan stacking problem has become a significant issue in the financial industry. One of the key steps in the fight against it is the querying of the loan history of a borrower from peer lenders. This is especially important in markets without a trusted credit bureau. To protect participants privacy and business interests, we want to hide borrower identities and lenders data from the loan originator, while simultaneously verifying that the borrower authorizes the query. In this paper, we propose Octopus, a distributed system to execute the query while meeting all the above security requirements. Theoretically, Octopus is sound. Practically, it integrates multiple optimizations to reduce communication and computation overhead. Evaluation shows that Octopus can run on 800 geographically distributed servers and can perform a query within about 0.5 seconds on average.",
      "fieldsOfStudy": [
        "Computer Science"
      ],
      "topics": [
        "lenders data",
        "peer lenders",
        "borrower identities",
        "online lenders",
        "computation",
        "loan stacking problem",
        "business interests",
        "financial industry",
        "loan originator",
        "above security requirements",
        "loan history",
        "communication",
        "multiple optimizations",
        "participants privacy",
        "Octopus",
        "communication and computation overhead",
        "query"
      ]
    },
    "org": {
      "title": "Bayesian Algorithms for Decentralized Stochastic Bandits",
      "url": "https://www.semanticscholar.org/paper/82e21b6d59d656dd8ffaa40f30de6113711dfd05",
      "abstract": "We study a decentralized cooperative multi-agent multi-armed bandit (MAB) problem with <inline-formula> <tex-math notation=\"LaTeX\">$K$ </tex-math></inline-formula> arms and <inline-formula> <tex-math notation=\"LaTeX\">$N$ </tex-math></inline-formula> agents connected over a network. In this model, each arm\u2019s reward distribution is the same for every agent, and rewards are drawn independently across agents and over time steps. At each iteration, agents independently choose an arm to play and exchange at most <inline-formula> <tex-math notation=\"LaTeX\">$\\mathsf {poly}(K)$ </tex-math></inline-formula> real-valued messages with their neighbors. Existing lower bound on the average per-agent regret over the network shows that cooperation with other agents can achieve a reduction of <inline-formula> <tex-math notation=\"LaTeX\">$O\\left({\\frac {1}{N}}\\right)$ </tex-math></inline-formula> over when playing in isolation. Motivated by this, we study a message-passing algorithm that can be combined with existing Bayesian MAB algorithms. Using this, we propose a decentralized Thompson Sampling (TS) algorithm and a decentralized Bayes-UCB algorithm. Under decentralized TS for bounded rewards, we establish a problem-dependent upper bound on the average per-agent regret in terms of the number of agents in the network and the network topology. For Bernoulli rewards, our upper bound asymptotically matches the lower bound. We empirically show that the proposed decentralized TS algorithm incurs significantly lesser per-agent regret than previously proposed algorithms. Furthermore, we show that the proposed decentralized TS can be extended to general bandit problems, where posterior distribution cannot be computed in closed form. We combine our decentralized TS algorithm with Variational Inference to apply our proposed algorithm to complex realistic reward distributions in a computationally efficient manner. We implement our proposed decentralized TS under gossip protocol and over time-varying networks, where each communication link has a fixed probability of failure and show that it incurs logarithmic regret.",
      "fieldsOfStudy": [
        "Computer Science",
        "Mathematics"
      ],
      "topics": [
        "agents",
        "agents",
        "existing Bayesian MAB algorithms",
        "complex realistic reward distributions",
        "logarithmic regret",
        "general bandit problems",
        "decentralized TS",
        "closed form",
        "bounded rewards",
        "posterior distribution",
        "rewards",
        "Bernoulli rewards",
        "Bayesian MAB",
        "TS algorithm",
        "lower bound",
        "proposed decentralized TS algorithm"
      ]
    }
  },
  {
    "sim": 0.5508007598707059,
    "gen": {
      "title": "Optimal Relay Power Allocation for Amplify-and-Forward Relay Networks with Non-linear Power Amplifiers",
      "url": "https://www.semanticscholar.org/paper/5d0d1e2123be3e37aaa0e5af392426e297d05a8e",
      "abstract": "In this paper, we propose an optimal relay power allocation of an Amplify-and-Forward relay networks with non-linear power amplifiers. Based on Bussgang Linearization Theory, we depict the non-linear amplifying process into a linear system, which lets analyzing system performance easier. To obtain spatial diversity, we design a complete practical framework of a non-linear distortion aware receiver. Consider a total relay power constraint, we propose an optimal power allocation scheme to maximum the receiver signal-to-noise ratio. Simulation results show that proposed optimal relay power allocation indeed can improve the system capacity and resist the non-linear distortion. It is also verified that the proposed transmission scheme outperforms other transmission schemes without considering non-linear distortion.",
      "fieldsOfStudy": [
        "Computer Science",
        "Mathematics"
      ],
      "topics": [
        "non-linear power amplifiers",
        "non-linear distortion aware receiver",
        "non-linear distortion",
        "system performance",
        "non-linear amplifying process",
        "transmission schemes",
        "noise",
        "optimal power allocation scheme",
        "linear system",
        "total relay power constraint",
        "Bussgang Linearization Theory",
        "proposed transmission scheme"
      ]
    },
    "org": {
      "title": "Min-Max Design of FIR Digital Filters by Semidefinite Programming",
      "url": "https://www.semanticscholar.org/paper/7767898d582954b83135512c96d0fbf29751f919",
      "abstract": "Robustness is a fundamental issue in signal processing; unmodeled dynamics and unexpected noise in systems and signals are inevitable in designing systems and signals. Against such uncertainties, min-max optimization, or worst case optimization is a powerful tool. In this light, we propose an efficient design method of FIR (finite impulse response) digital filters for approximating and inverting given digital filters. The design is formulated by min-max optimization in the frequency domain. More precisely, we design an FIR filter which minimizes the maximum gain of the frequency response of an error system. This design has a direct relation with H\u221e optimization (Francis, 1987). Since the space H\u221e is not a Hilbert space, the familiar projection method in conventional signal processing cannot be applied. However, many studies have been made on the H\u221e optimization, and nowadays the optimal solution to the H\u221e problem is deeply analysed and can be easily obtained by numerical computation. Moreover, as an extension of H\u221e optimization, a min-max optimization on a finite frequency interval has been proposed recently (Iwasaki & Hara, 2005). In both optimization, the Kalman-Yakubovich-Popov (KYP) lemma (Anderson, 1967; Rantzer, 1996; Tuqan & Vaidyanathan, 1998) and the generalized KYP lemma (Iwasaki & Hara, 2005) give an easy and fast way of numerical computation; semidefinite programming (Boyd & Vandenberghe, 2004). Semidefinite programming can be efficiently solved by numerical optimization softwares. In this chapter, we consider two fundamental problems of signal processing: FIR approximation of IIR (infinite impulse response) filters and inverse FIR filtering of FIR/IIR filters. Each problems are formulated in two types of optimization: H\u221e optimization and finite-frequency min-max one. These problems are reduced to semidefinite programming in a similar way. For this, we introduce state-space representation. Semidefinite programming is obtained by the generalized KYP lemma. We will give MATLAB codes for the proposed design, and will show design examples.",
      "fieldsOfStudy": [
        "Mathematics",
        "Computer Science"
      ],
      "topics": [
        "digital filters",
        "signal processing",
        "designing systems",
        "conventional signal processing",
        "min-max optimization",
        "filters",
        "infinite impulse response",
        "signals",
        "inverse FIR filtering",
        "finite impulse response",
        "optimization",
        "numerical optimization softwares",
        "worst case optimization",
        "numerical computation",
        "given digital filters",
        "systems",
        "semidefinite programming"
      ]
    }
  },
  {
    "sim": 0.5387699339409553,
    "gen": {
      "title": "Optimal control of information epidemics modeled as Maki Thompson rumors",
      "url": "https://www.semanticscholar.org/paper/d15fa9866bfe4dcf1b9b1edba5418ed9b26b563f",
      "abstract": null,
      "fieldsOfStudy": [
        "Computer Science",
        "Mathematics"
      ]
    },
    "org": {
      "title": "Scale-variant topological information for characterizing complex networks",
      "url": "https://www.semanticscholar.org/paper/3a60d3178f8405e51410e81d1ae05217b6dfde5f",
      "abstract": "The structure of real-world networks is usually difficult to characterize owing to the variation of topological scales, the nondyadic complex interactions, and the fluctuations in the network. We aim to address these problems by introducing a general framework using a method based on topological data analysis. By considering the diffusion process at a single specified timescale in a network, we map the network nodes to a finite set of points that contains the topological information of the network at a single scale. Subsequently, we study the shape of these point sets over variable timescales that provide scale-variant topological information, to understand the varying topological scales and the complex interactions in the network. We conduct experiments on synthetic and real-world data to demonstrate the effectiveness of the proposed framework in identifying network models, classifying real-world networks, and detecting transition points in time-evolving networks. Overall, our study presents a unified analysis that can be applied to more complex network structures, as in the case of multilayer and multiplex networks.",
      "fieldsOfStudy": [
        "Physics",
        "Computer Science",
        "Mathematics",
        "Medicine"
      ],
      "topics": [
        "multiplex networks",
        "network models",
        "topological scales",
        "topological data analysis",
        "complex network structures",
        "real-world networks",
        "time-evolving networks",
        "network nodes",
        "transition points",
        "scale-variant topological information",
        "network",
        "network",
        "points",
        "varying topological scales",
        "variable timescales",
        "multilayer and multiplex networks"
      ]
    }
  },
  {
    "sim": 0.3846735630119682,
    "gen": {
      "title": "Using Deep Learning for Image-Based Plant Disease Detection",
      "url": "https://www.semanticscholar.org/paper/e30d9b8ce108d982169621b88a5e3fb69fec70e1",
      "abstract": "Crop diseases are a major threat to food security, but their rapid identification remains difficult in many parts of the world due to the lack of the necessary infrastructure. The combination of increasing global smartphone penetration and recent advances in computer vision made possible by deep learning has paved the way for smartphone-assisted disease diagnosis. Using a public dataset of 54,306 images of diseased and healthy plant leaves collected under controlled conditions, we train a deep convolutional neural network to identify 14 crop species and 26 diseases (or absence thereof). The trained model achieves an accuracy of 99.35% on a held-out test set, demonstrating the feasibility of this approach. Overall, the approach of training deep learning models on increasingly large and publicly available image datasets presents a clear path toward smartphone-assisted crop disease diagnosis on a massive global scale.",
      "fieldsOfStudy": [
        "Computer Science",
        "Medicine"
      ],
      "topics": [
        "deep learning models",
        "deep learning",
        "Crop diseases",
        "increasing global smartphone penetration",
        "smartphone-assisted disease diagnosis",
        "parts",
        "computer vision",
        "recent advances",
        "controlled conditions",
        "food security",
        "deep convolutional neural network",
        "massive global scale",
        "absence",
        "necessary infrastructure"
      ]
    },
    "org": {
      "title": "Real-time Monitoring and Forecasting of Ecological Processes",
      "url": "https://www.semanticscholar.org/paper/1005f9568313fd9f8f8d8573ede6bb628c8e71a0",
      "abstract": "The paper introduces a real-time monitoring and forecasting system for ecological phenomena. The process yields a collection of ecological parameters viewed as distributed time series, which are measured by means of wireless network of sensors. The acquired data are preliminary processed and modeled by using complex algorithms in view of prediction. There are three graphical user interfaces implemented within the monitoring and forecasting system: eko-View and eko-Greenhouse (which directly interacts with the process) and eko-Forecast (which estimates the future evolution of some ecological parameters). The monitoring system was effectively integrated in an industrial application dealing with automatic irrigation of a small greenhouse. The forecasting simulation results with real data and a comparative assessment of predictor performances are presented in the end.",
      "fieldsOfStudy": [
        "Computer Science"
      ],
      "topics": [
        "distributed time series",
        "wireless network",
        "sensors",
        "ecological phenomena",
        "automatic irrigation",
        "view",
        "complex algorithms",
        "predictor performances",
        "real data",
        "means",
        "prediction",
        "ecological parameters",
        "eko-View",
        "small greenhouse",
        "real-time monitoring and forecasting system"
      ]
    }
  },
  null,
  {
    "sim": 0.648764907719361,
    "gen": {
      "title": "Privacy-Preserving Stream Aggregation with Fault Tolerance",
      "url": "https://www.semanticscholar.org/paper/555a239220db7e3fdcbf43b4de7f2bb7c6d101fe",
      "abstract": null,
      "fieldsOfStudy": [
        "Computer Science"
      ]
    },
    "org": {
      "title": "Collecting and Analyzing Multidimensional Data with Local Differential Privacy",
      "url": "https://www.semanticscholar.org/paper/2fbb4ba85869a333637f2b5d762ad69fb58bc7af",
      "abstract": "Local differential privacy (LDP) is a recently proposed privacy standard for collecting and analyzing data, which has been used, e.g., in the Chrome browser, iOS and macOS. In LDP, each user perturbs her information locally, and only sends the randomized version to an aggregator who performs analyses, which protects both the users and the aggregator against private information leaks. Although LDP has attracted much research attention in recent years, the majority of existing work focuses on applying LDP to complex data and/or analysis tasks. In this paper, we point out that the fundamental problem of collecting multidimensional data under LDP has not been addressed sufficiently, and there remains much room for improvement even for basic tasks such as computing the mean value over a single numeric attribute under LDP. Motivated by this, we first propose novel LDP mechanisms for collecting a numeric attribute, whose accuracy is at least no worse (and usually better) than existing solutions in terms of worst-case noise variance. Then, we extend these mechanisms to multidimensional data that can contain both numeric and categorical attributes, where our mechanisms always outperform existing solutions regarding worst-case noise variance. As a case study, we apply our solutions to build an LDP-compliant stochastic gradient descent algorithm (SGD), which powers many important machine learning tasks. Experiments using real datasets confirm the effectiveness of our methods, and their advantages over existing solutions.",
      "fieldsOfStudy": [
        "Computer Science",
        "Geology"
      ],
      "topics": [
        "existing solutions",
        "important machine learning tasks",
        "novel LDP mechanisms",
        "basic tasks",
        "LDP",
        "existing work",
        "private information leaks",
        "worst-case noise variance",
        "multidimensional data",
        "complex data and/or analysis tasks",
        "single numeric attribute",
        "data",
        "research attention",
        "analyses",
        "room",
        "numeric attribute"
      ]
    }
  },
  null,
  {
    "sim": 0.5013438716569637,
    "gen": {
      "title": "Differentially Private Confidence Intervals",
      "url": "https://www.semanticscholar.org/paper/517bd7656447163a901fbd78597b2484a95583d0",
      "abstract": "Confidence intervals for the population mean of normally distributed data are some of the most standard statistical outputs one might want from a database. In this work we give practical differentially private algorithms for this task. We provide five algorithms and then compare them to each other and to prior work. We give concrete, experimental analysis of their accuracy and find that our algorithms provide much more accurate confidence intervals than prior work. For example, in one setting (with {\\epsilon} = 0.1 and n = 2782) our algorithm yields an interval that is only 1/15th the size of the standard set by prior work.",
      "fieldsOfStudy": [
        "Mathematics",
        "Computer Science"
      ],
      "topics": [
        "prior work",
        "Confidence intervals",
        "practical differentially private algorithms",
        "experimental analysis",
        "work",
        "algorithms",
        "algorithms",
        "accurate confidence intervals",
        "concrete",
        "database",
        "\\epsilon",
        "standard statistical outputs",
        "normally distributed data",
        "task",
        "concrete, experimental analysis",
        "interval"
      ]
    },
    "org": {
      "title": "Generalized Low Rank Models",
      "url": "https://www.semanticscholar.org/paper/7ab20bbd532a5f8a2591569bbeb31f15aec27260",
      "abstract": "Principal components analysis (PCA) is a well-known technique for approximating a tabular data set by a low rank matrix. Here, we extend the idea of PCA to handle arbitrary data sets consisting of numerical, Boolean, categorical, ordinal, and other data types. This framework encompasses many well-known techniques in data analysis, such as nonnegative matrix factorization, matrix completion, sparse and robust PCA, k-means, k-SVD, and maximum margin matrix factorization. The method handles heterogeneous data sets, and leads to coherent schemes for compressing, denoising, and imputing missing entries across all data types simultaneously. It also admits a number of interesting interpretations of the low rank factors, which allow clustering of examples or of features. We propose several parallel algorithms for fitting generalized low rank models, and describe implementations and numerical results.",
      "fieldsOfStudy": [
        "Computer Science",
        "Mathematics"
      ],
      "topics": [
        "maximum margin matrix factorization",
        "nonnegative matrix factorization",
        "matrix completion",
        "arbitrary data sets",
        "data analysis",
        "heterogeneous data sets",
        "robust PCA",
        "fitting generalized low rank models",
        "numerical results",
        "PCA",
        "sparse",
        "low rank matrix",
        "missing entries",
        "features",
        "data types",
        "generalized low rank models",
        "sparse and robust PCA",
        "coherent schemes"
      ]
    }
  },
  {
    "sim": 0.3863083278352153,
    "gen": {
      "title": "Time Series Analysis and Forecasting of the Hand-Foot-Mouth Disease Morbidity in China Using An Advanced Exponential Smoothing State Space TBATS Model",
      "url": "https://www.semanticscholar.org/paper/1642e13b9f80fdfe5fa86158e7a8b1cb9779afc7",
      "abstract": "Objective The high morbidity, complex seasonality, and recurring risk of hand-foot-and-mouth disease (HFMD) exert a major burden in China. Forecasting its epidemic trends is greatly instrumental in informing vaccine and targeted interventions. This study sets out to investigate the usefulness of an advanced exponential smoothing state space framework by combining Box-Cox transformations, Fourier representations with time-varying coefficients and autoregressive moving average (ARMA) error correction (TBATS) method to assess the temporal trends of HFMD in China. Methods Data from January 2009 to December 2019 were drawn, and then they were split into two segments comprising the in-sample training data and out-of-sample testing data to develop and validate the TBATS model, and its fitting and forecasting abilities were compared with the most frequently used seasonal autoregressive integrated moving average (SARIMA) method. Results Following the modelling procedures of the SARIMA and TBATS methods, the SARIMA (1,0,1)(0,1,1)12 and TBATS (0.024, {1,1}, 0.855, {<12,4>}) specifications were recognized as being the optimal models, respectively, for the 12-step ahead forecasting, along with the SARIMA (1,0,1)(0,1,1)12 and TBATS (0.062, {1,3}, 0.86, {<12,4>}) models as being the optimal models, respectively, for the 24-step ahead forecasting. Among them, the optimal TBATS models produced lower error rates in both 12-step and 24-step ahead forecasting aspects compared to the preferred SARIMA models. Descriptive analysis of the data showed a significantly high level and a marked dual seasonal pattern in the HFMD morbidity. Conclusion The TBATS model has the capacity to outperform the most frequently used SARIMA model in forecasting the HFMD incidence in China, and it can be recommended as a flexible and useful tool in the decision-making process of HFMD prevention and control in China.",
      "fieldsOfStudy": [
        "Medicine"
      ],
      "topics": [
        "China",
        "HFMD prevention",
        "HFMD",
        "TBATS",
        "control",
        "lower error rates",
        "targeted interventions",
        "preferred SARIMA models",
        "flexible and useful tool",
        "HFMD morbidity",
        "SARIMA",
        "1,0,1)(0,1,1)12",
        "HFMD incidence",
        "optimal models",
        "optimal TBATS models",
        "Methods",
        "TBATS model",
        "Fourier representations"
      ]
    },
    "org": {
      "title": "Effective information spreading based on local information in correlated networks",
      "url": "https://www.semanticscholar.org/paper/cb15e50b8e2d2d3243caa6b7cc76504a9c5394ba",
      "abstract": null,
      "fieldsOfStudy": [
        "Computer Science",
        "Medicine",
        "Physics",
        "Mathematics"
      ]
    }
  },
  {
    "sim": 0.1633074451455312,
    "gen": {
      "title": "Patterns of Culture: Re-aligning Library Culture with User Needs",
      "url": "https://www.semanticscholar.org/paper/84b13a00bb88b23b0301e331941a665e38770df2",
      "abstract": "Radical changes in technology and information access have given rise to new academic disciplinary connections, new research and teaching practices, and new modes of communication. With the support of the Andrew W. Mellon Foundation, Syracuse University Library has undertaken a research project to better understand these changes at the University\u2019s S.I. Newhouse School of Public Communications. We intend to develop an indepth understanding of one multi-disciplinary academic culture and then to examine the library\u2019s culture and work practices to discover where services and resources are meeting needs and where they are not. The qualitative methods used in the Patterns of Culture project is informed by the ethnographic work conducted at the University of Rochester. The research team, four librarians and a graduate assistant, received training in interview and observational techniques from anthropologist Nancy Foster. Our data gathering, conducted from spring 2007 to spring 2008, involved interviews with faculty, librarians, and students about their work practice, eliciting photographic diaries from students and conducting observations in classrooms and public spaces. The goal of the Patterns of Culture (after Ruth Benedict\u2019s landmark work) is threefold: to gain a better understanding of the needs, research, and work practices of the faculty and students and to gain the same type of understanding of library staff; to develop a plan to align library culture, resources, and services more closely with the needs of faculty and students; and to produce a model for data gathering and analysis that can be applied by the library to other academic settings. Our project is unusual in that it applies the same ethnographic methods to three groups, using comparison as a means for deeper understanding. Introduction Syracuse University Library received funding in October 2007 by the Andrew W. Mellon foundation to support ethnographic research for better understanding of the cultures, practices, and stories at Syracuse University. We would use our results to inform ways of synchronizing library services more closely with user needs. Although our initial effort was the S.I. Newhouse School of Communications, we planned to use the project as a test case, evaluating the methodology as a model for use in other schools on campus. Finally, we wanted to explore the ways in which a research effort employing ethnographic techniques might serve as a change agent, affecting the ways librarians listen to and work with users. Background Literature Emerging from the field of anthropology, the ethnographic method utilizes interviews and participant observation to discover the unspoken \u201cculture,\u201d or values, belief, and practices of a group. Ethnography can also be useful in design because it provides insight into the worldview of users\u2014how they work, behave, and what they value. This type of information is exceedingly valuable to marketers and designers, as well as usability engineers. In the 1980s, a group of anthropologists at the Xerox Palo Alto Research Center helped to pioneer the use of ethnography in studying how people use software and interact with computers.1 Jones argues for a larger role of ethnography in design.2 She points out how ethnographic methods can draw attention to the environmental characteristics, practical applications of abstract ideas, the sociality of design spaces, and models of how people work. Applying ethnographic methods as a method for assessing library services and facilities is relatively new. Ethnographic methods have been used to assess digital library services,3 student library behavior,4and faculty attitudes toward library instruction.5 In 2005, Nancy Foster used ethnographic techniques to study how faculty at the University of Rochester used institutional repositories.6 The University of Rochester has been conducting additional projects that use 2008 Library Assessment Conference 188 ethnographic methods to inform library design, services and student space.7 Context Syracuse University is a private, independent fouryear college located in Syracuse, New York. Founded in 1870, Syracuse University serves 18,000 students, including approximately 13,000 undergraduates. Syracuse University Library supports the teaching, learning, and research at the university by providing a wide array of on-site and online resources and associated research support services. The Library\u2019s collections include more than 2.9 million volumes, over 21,000 online and print journals, over 400 reference databases, as well as extensive collections of microforms, maps, images, music scores, sound recordings, video, rare books, and manuscripts. The library staff is comprised of 55 librarians and professional-managerial staff and 125 unionized support staff. The public desks are staffed 104 hours a week; a learning commons provides 24-hour access during the school year. Libraries are equipped with wireless access, laptops for loan, and provide a variety of study spaces including group study rooms, individual study carrels, and designated quiet study areas. The largest SU library is E.S. Bird Library, which houses non-science disciplines, library administrative offices, and the Special Collections Research Center. There is a separate Science and Technology Library and branch libraries for earth science and mathematics. In the process of creating a learning commons area on the first three floors of the building, the library has opened a cafe on the first floor of the Bird Library and is re-designing its common space and service areas. With the opening of Newhouse III in October 2007, the S.I. Newhouse School of Public Communications is now comprised of three buildings linked together by a cafe and includes computer facilities, editing suites and presentation rooms. The school has 65 faculty plus many adjuncts, enrolling about 1800 undergraduates and 200 graduate students. As a professional school, faculty constitute a mix of \u201cprofessors of practice\u201d with backgrounds and professional networks in the industry and research faculty who publish in the more scholarly academic literature. All faculty, including administrators, teach. Departments at Newhouse include public relations, broadcast and print journalism, advertising, television, radio & film, and new media. The school supports programs and centers for arts journalism, free speech, legal reporting and television and popular culture in addition to the collaborative work conducted in partnership with campus schools of business, law, visual and performing arts and public citizenship. Methodology We conducted pilot interviews with faculty prior to writing the planning grant proposal. From those conversations, we developed these questions: Is ethnography a feasible method for learning about our users? Can ethnographic data be used as a framework for looking at our own organizational culture? Can we compare library and academic \u201ccultures\u201d in a meaningful way? Do we share a common understanding with our users as to what the \u201clibrary\u201d means? We wanted to use ethnography because it is a non-evaluative approach to assessment. Rather than instructing users in how to use the library, our interviews became opportunities for us to listen and observe how users do their work, in very specific ways, and discover the kinds of barriers they experience as they\u2019re doing that work. Examples of our interview questions about work practice include: Tell me about a recent article or piece of information that you read. How did you find it? What did you do to prepare for your most recent class? When you started work in your office today, what was the first thing you did? These questions were adapted slightly for use with students and with librarians\u2014for instance, students used digital cameras and brought those pictures to the interview as prompts in talking about how they do their work in finding information and carrying out course assignments. The librarian interviews focused less on research and teaching, as librarians at SU don\u2019t routinely do extensive academic research for publication or teach credit-bearing classes. We interviewed 38 faculty members at Newhouse from all departments. We interviewed 18 librarians, 5 of which were also manager or department heads. We had 9 students\u20145 graduate students and 4 undergraduates. We took a Grounded Theory",
      "fieldsOfStudy": null,
      "topics": [
        "library culture",
        "work practices",
        "library services",
        "student space.7 Context Syracuse University",
        "synchronizing library services",
        "extensive academic research",
        "ethnographic research",
        "library staff",
        "associated research support services",
        "library design",
        "group study rooms",
        "Introduction Syracuse University Library",
        "digital library",
        "Libraries",
        "library",
        "library instruction.5",
        "branch libraries",
        "library administrative offices"
      ]
    },
    "org": {
      "title": "Topological Machine Learning for Mixed Numeric and Categorical Data",
      "url": "https://www.semanticscholar.org/paper/b675e153156767ca49ad6359e3f2f16240e88469",
      "abstract": "Topological data analysis is a relatively new branch of machine learning that excels in studying high dimensional data, and is theoretically known to be robust against noise. Meanwhile, data objects with mixed numeric and categorical attributes are ubiquitous in real-world applications. However, topological methods are usually applied to point cloud data, and to the best of our knowledge there is no available framework for the classification of mixed data using topological methods. In this paper, we propose a novel topological machine learning method for mixed data classification. In the proposed method, we use theory from topological data analysis such as persistent homology, persistence diagrams and Wasserstein distance to study mixed data. The performance of the proposed method is demonstrated by experiments on a real-world heart disease dataset. Experimental results show that our topological method outperforms several state-of-the-art algorithms in the prediction of heart disease.",
      "fieldsOfStudy": [
        "Computer Science",
        "Mathematics"
      ],
      "topics": [
        "mixed data classification",
        "mixed data",
        "topological data analysis",
        "high dimensional data",
        "cloud data",
        "heart disease",
        "topological methods",
        "noise",
        "Topological",
        "Wasserstein distance",
        "persistence diagrams",
        "persistent homology",
        "novel topological machine learning method",
        "mixed numeric and categorical attributes",
        "data objects",
        "point cloud data",
        "machine learning",
        "real-world heart disease dataset"
      ]
    }
  },
  {
    "sim": 0.5191578293548028,
    "gen": {
      "title": "Dynamic Routing and Wavelength Assignment Using Cost Based Heuristics in WDM Optical Networks",
      "url": "https://www.semanticscholar.org/paper/57018987ad57896b06b211722c4d3b2fcc46f823",
      "abstract": null,
      "fieldsOfStudy": [
        "Computer Science"
      ]
    },
    "org": {
      "title": "Sub-O(log n) Out-of-Order Sliding-Window Aggregation",
      "url": "https://www.semanticscholar.org/paper/91cfb179676ba50bdba7a0491538a9efdb2034ea",
      "abstract": "Sliding-window aggregation summarizes the most recent information in a data stream. Users specify how that summary is computed, usually as an associative binary operator because this is the most general known form for which it is possible to avoid naively scanning every window. For strictly in-order arrivals, there are algorithms with $O(1)$ time per window change assuming associative operators. Meanwhile, it is common in practice for streams to have data arriving slightly out of order, for instance, due to clock drifts or communication delays. Unfortunately, for out-of-order streams, one has to resort to latency-prone buffering or pay $O(\\log n)$ time per insert or evict, where $n$ is the window size. \nThis paper presents the design, analysis, and implementation of FiBA, a novel sliding-window aggregation algorithm with an amortized upper bound of $O(\\log d)$ time per insert or evict, where $d$ is the distance of the inserted or evicted value to the closer end of the window. This means $O(1)$ time for in-order arrivals and nearly $O(1)$ time for slightly out-of-order arrivals, with a smooth transition towards $O(\\log n)$ as $d$ approaches $n$. We also prove a matching lower bound on running time, showing optimality. Our algorithm is as general as the prior state-of-the-art: it requires associativity, but not invertibility nor commutativity. At the heart of the algorithm is a careful combination of finger-searching techniques, lazy rebalancing, and position-aware partial aggregates. We further show how to answer range queries that aggregate subwindows for window sharing. Finally, our experimental evaluation shows that FiBA performs well in practice and supports the theoretical findings.",
      "fieldsOfStudy": [
        "Computer Science",
        "Mathematics"
      ],
      "topics": [
        "window change",
        "associative operators",
        "running time",
        "O(\\log",
        "streams",
        "communication delays",
        "Sliding-window aggregation",
        "window size",
        "insert",
        "commutativity",
        "order",
        "n$.",
        "evict",
        "d$",
        "novel sliding-window aggregation algorithm",
        "$O(\\log d)$ time",
        "lazy rebalancing",
        "clock drifts"
      ]
    }
  },
  {
    "sim": 0.39952505106716285,
    "gen": {
      "title": "Dropout as data augmentation",
      "url": "https://www.semanticscholar.org/paper/3316f4a3070c9213604c56f887ef530b35c29295",
      "abstract": "Dropout is typically interpreted as bagging a large number of models sharing parameters. We show that using dropout in a network can also be interpreted as a kind of data augmentation in the input space without domain knowledge. We present an approach to projecting the dropout noise within a network back into the input space, thereby generating augmented versions of the training data, and we show that training a deterministic network on the augmented samples yields similar results. Finally, we propose a new dropout noise scheme based on our observations and show that it improves dropout results without adding significant computational cost.",
      "fieldsOfStudy": [
        "Computer Science",
        "Mathematics"
      ],
      "topics": [
        "significant computational cost",
        "dropout results",
        "similar results",
        "augmented versions",
        "domain knowledge",
        "dropout",
        "parameters",
        "data augmentation",
        "new dropout noise scheme",
        "models",
        "deterministic network",
        "augmented samples",
        "input space",
        "dropout noise",
        "training data"
      ]
    },
    "org": {
      "title": "Characterizing Directed and Undirected Networks via Multidimensional Walks with Jumps",
      "url": "https://www.semanticscholar.org/paper/88e23390e40ed162e7ee4a6c3a9bd4e813e3dcc2",
      "abstract": "Estimating distributions of node characteristics (labels) such as number of connections or citizenship of users in a social network via edge and node sampling is a vital part of the study of complex networks. Due to its low cost, sampling via a random walk (RW) has been proposed as an attractive solution to this task. Most RW methods assume either that the network is undirected or that walkers can traverse edges regardless of their direction. Some RW methods have been designed for directed networks where edges coming into a node are not directly observable. In this work, we propose Directed Unbiased Frontier Sampling (DUFS), a sampling method based on a large number of coordinated walkers, each starting from a node chosen uniformly at random. It applies to directed networks with invisible incoming edges because it constructs, in real time, an undirected graph consistent with the walkers trajectories, and its use of random jumps to prevent walkers from being trapped. DUFS generalizes previous RW methods and is suited for undirected networks and to directed networks regardless of in-edge visibility. We also propose an improved estimator of node label distribution that combines information from initial walker locations with subsequent RW observations. We evaluate DUFS, compare it to other RW methods, investigate the impact of its parameters on estimation accuracy and provide practical guidelines for choosing them. In estimating out-degree distributions, DUFS yields significantly better estimates of the head of the distribution than other methods, while matching or exceeding estimation accuracy of the tail. Last, we show that DUFS outperforms uniform sampling when estimating distributions of node labels of the top 10% largest degree nodes, even when sampling a node uniformly has the same cost as RW steps.",
      "fieldsOfStudy": [
        "Computer Science",
        "Physics",
        "Mathematics"
      ],
      "topics": [
        "undirected networks",
        "complex networks",
        "random jumps",
        "directed networks",
        "node sampling",
        "node labels",
        "networks",
        "coordinated walkers",
        "initial walker locations",
        "walkers",
        "invisible incoming edges",
        "RW methods",
        "previous RW methods",
        "edges",
        "node label distribution",
        "Most RW methods",
        "node characteristics",
        "subsequent RW observations",
        "RW steps"
      ]
    }
  },
  {
    "sim": 0.5189234220617001,
    "gen": {
      "title": "Connecting User and Item Perspectives in Popularity Debiasing for Collaborative Recommendation",
      "url": "https://www.semanticscholar.org/paper/4463a9de6ae5b6ed9f6bca153845c693ef4399d7",
      "abstract": null,
      "fieldsOfStudy": [
        "Computer Science"
      ]
    },
    "org": {
      "title": "Differentially Private Obfuscation Mechanisms for Hiding Probability Distributions",
      "url": "https://www.semanticscholar.org/paper/561d1a426c7be7bc8e8f32ce61f9976bd0f90328",
      "abstract": null,
      "fieldsOfStudy": [
        "Computer Science",
        "Mathematics"
      ]
    }
  },
  {
    "sim": 0.6235908427521643,
    "gen": {
      "title": "Kernel Interpolation for Scalable Structured Gaussian Processes (KISS-GP)",
      "url": "https://www.semanticscholar.org/paper/767d0625c4767c6b8afa0b1b30deafed7e0e8f08",
      "abstract": "We introduce a new structured kernel interpolation (SKI) framework, which generalises and unifies inducing point methods for scalable Gaussian processes (GPs). SKI methods produce kernel approximations for fast computations through kernel interpolation. The SKI framework clarifies how the quality of an inducing point approach depends on the number of inducing (aka interpolation) points, interpolation strategy, and GP covariance kernel. SKI also provides a mechanism to create new scalable kernel methods, through choosing different kernel interpolation strategies. Using SKI, with local cubic kernel interpolation, we introduce KISSGP, which is 1) more scalable than inducing point alternatives, 2) naturally enables Kronecker and Toeplitz algebra for substantial additional gains in scalability, without requiring any grid data, and 3) can be used for fast and expressive kernel learning. KISS-GP costs O(n) time and storage for GP inference. We evaluate KISS-GP for kernel matrix approximation, kernel learning, and natural sound modelling.",
      "fieldsOfStudy": [
        "Computer Science",
        "Mathematics"
      ],
      "topics": [
        "kernel interpolation",
        "different kernel interpolation strategies",
        "local cubic kernel interpolation",
        "kernel learning",
        "new scalable kernel methods",
        "kernel matrix approximation",
        "kernel",
        "interpolation strategy",
        "natural sound modelling",
        "point methods",
        "GP covariance kernel",
        "point alternatives",
        "new structured kernel interpolation",
        "substantial additional gains",
        "inducing point methods",
        "inducing point alternatives",
        "fast and expressive kernel learning",
        "GP inference",
        "scalable Gaussian processes"
      ]
    },
    "org": {
      "title": "Generative Model With Dynamic Linear Flow",
      "url": "https://www.semanticscholar.org/paper/c592a7076cda683cc2e2891f00297119b8747701",
      "abstract": "Flow-based generative models are a family of exact log-likelihood models with tractable sampling and latent-variable inference, hence conceptually attractive for modeling complex distributions. However, flow-based models are limited by density estimation performance issues as compared to state-of-the-art autoregressive models. Autoregressive models, which also belong to the family of likelihood-based methods, however suffer from limited parallelizability. In this paper, we propose <italic>Dynamic Linear Flow (DLF)</italic>, a new family of invertible transformations with partially autoregressive structure. Our method benefits from the efficient computation of flow-based methods and high density estimation performance of autoregressive methods. We demonstrate that the proposed DLF yields state-of-the-art performance on ImageNet <inline-formula> <tex-math notation=\"LaTeX\">$32\\times 32$ </tex-math></inline-formula> and <inline-formula> <tex-math notation=\"LaTeX\">$64\\times 64$ </tex-math></inline-formula> out of all flow-based methods. Additionally, DLF converges significantly faster than previous flow-based methods such as Glow.",
      "fieldsOfStudy": [
        "Computer Science",
        "Mathematics"
      ],
      "topics": [
        "Autoregressive models",
        "high density estimation performance",
        "complex distributions",
        "limited parallelizability",
        "Flow-based generative models",
        "flow-based models",
        "likelihood-based methods",
        "invertible transformations",
        "exact log-likelihood models",
        "DLF)</italic>",
        "Dynamic Linear Flow",
        "tractable sampling",
        "Glow"
      ]
    }
  },
  {
    "sim": 0.3304908834802437,
    "gen": {
      "title": "EXPERIMENTALISM IN THE AGE OF THE SHARING ECONOMY by",
      "url": "https://www.semanticscholar.org/paper/aab62992b5b38628fb75aad0727b0ea0a7754734",
      "abstract": "id=2271971; Georgios Zervas, Davide Proserpio & John W. Byers, The Rise of the Sharing Economy: Estimating the Impact of Airbnb on the Hotel Industry 2, 8, 20 (Bos. U. Sch. of Mgmt., Research Paper No. 2013-16, 2015), http://papers.ssrn.com/sol3/ LCB_19_4_Art_1_Ranchordas (Do Not Delete) 4/16/2016 4:38 PM 874 LEWIS & CLARK LAW REVIEW [Vol. 19:4 part of the legal and economic literature has argued that a stronger selfregulatory system could replace traditional regulation. In the age of the sharing economy, the regulatory responsibility should be reallocated to parties and not to the government. At the other end of the scale is a more restrictive position defended by courts around the world and a number of industries affected by the sharing economy, which declares its skepticism toward these practices, contending that existing legal categories and frameworks should be applied to the sharing economy. That is, innovation claims and peer-to-peer transactions do not exempt entrepreneurs from regulatory burdens, namely compliance with public health and safety measures. In this Article, I argue that this debate has been highly polarized, neglecting that the heart of the matter is not whether the state should be in or out of the innovation game, but rather when and how it should be involved. As I have argued in previous work, when traditional regulation (designed for a professional\u2013consumer relationship at a time when there were no navigation systems, Internet platforms, or transparent reputational mechanisms) is applied to innovative collaborative practices, regulators risk stifling innovative practices that challenge this model. In this Article, I contend that, considering the uncertainty that characterizes this sector, regulators should take a step back, analyze how the innovation papers.cfm?abstract_id=2366898 (analyzing the role of reputational mechanisms and the impact of Airbnb on hotel occupation rates). 7 Christopher Koopman, Matthew Mitchell & Adam Thierer, The Sharing Economy and Consumer Protection Regulation: The Case for Policy Change 17 (Mercatus Ctr. at Geo. Mason Univ., Working Paper 2014), http://mercatus.org/publication/ sharing-economy-and-consumer-protection-regulation-case-policy-change. 8 Molly Cohen & Arun Sundararajan, Self-Regulation and Innovation in the Peer-toPeer Sharing Economy, 82 U. Chi. L. Rev. Dialogue 116, 116 (2015). 9 Some sharing economy platforms and their users have been sanctioned in a number of countries for refusing to comply with home-sharing (e.g., rental permits) and ride-sharing (e.g., taxi licenses) requirements. In 2014 and 2015, peer-to-peer Uber services known as UberX or UberPOP in Europe lost a number of judicial challenges in Europe. See, e.g., Landgericht [LG][L\u00e4nder Court] Mar. 18, 2015, 3-08 O 136/14, http://www.lareda.hessenrecht.hessen.de/lexsoft/default/hessenrecht_ lareda.html#docid:7405026 (Ger.); CBB, 8 december 2014, ECLI:NL:CBB:2014:450 (Uber Int\u2019l B.V./Minister van Infrastructuur en Milieu) (Neth.) (qualifying Uber as an illegal taxi company and stating that Uber should comply with the same rules applicable to other transportation services). In France, Uber has also faced similar challenges and its arguments were recently rejected by the French Constitutional Court. See Conseil constitutionnel [CC][Constitutional Court] decision No. 2015-484 QPC, Sept. 22, 2015, (Fr.). 10 See Sofia Ranchord\u00e1s, Does Sharing Mean Caring? Regulating Innovation in the Sharing Economy, 16 Minn. J.L. Sci. & Tech. 413, 471 (2015). 11 Chris Dellarocas, The Digitization of Word-of-Mouth: Promise and Challenges of Online Reputation Systems, 49 Mgmt. Sci. 1407 (2003) (discussing the opportunities and challenges of online reputation mechanisms and how they differ from the traditional word-of-mouth). LCB_19_4_Art_1_Ranchordas (Do Not Delete) 4/16/2016 4:38 PM 2015] INNOVATION EXPERIMENTALISM 875 process works, what information they have at that time, and reflect upon the timing of their regulatory action. Drawing inspiration from the evolving nature of the sharing economy, this Article suggests two timing approaches. First, in some cases, regulators should enact temporary and experimental regulations to offer a prompt but adaptable regulatory response to innovative products and services. As this Article explains, a number of regulators throughout the United States have enacted temporary regulatory frameworks to regulate sharing-economy platforms such as Uber and Lyft. Second, in other cases, I argue that regulators should delay the regulatory intervention to a later stage, allowing rules on sharing-economy platforms to \u201csunrise\u201d if they prove to be necessary, once more information on the opportunities and risks of the sector becomes available. Experimental regulations are temporary dispositions that are enacted to try new legal solutions on a small-scale basis. These dispositions are retrospectively evaluated at the end of a certain period. Experimental rules are a first step to more informed, often better, and more evidencebased regulation. Experimental regulations can be attractive to both regulators and innovators because they give innovation a chance, without putting consumers at risk. During the experimental period, regulators can gather more information on the effectiveness of these temporary rules, observe how technology is evolving, and update regulations taking into account potential novelties, side-effects of these regulations, or input from consumers and firms. This experimental approach converts regulation into an iterative learning path, where uncertainty and change are regarded as opportunities to improve regulation by trying new rules and observing what works and what does not. The uncertainty of the innovation process is thus not an excuse to regulate in the dark or delay prompt regulatory action while waiting for further information. Sunrise clauses are the second instrument that can assist regulators in the mission of keeping up with innovation. While experimental regulations and other forms of temporary legislation, such as sunset clauses, avoid regulation that lags behind innovation, sunrise clauses avoid regu12 For a thorough analysis of the concept of \u201cexperimental legislation\u201d and the distinction between this and other forms of temporary legislation, see my previous work, Sofia Ranchord\u00e1s, Constitutional Sunsets and Experimental Legislation: A Comparative Perspective (2014). 13 New State Ice Co. v. Liebmann, 285 U.S. 262, 300 (1931) (Brandeis, J., dissenting). 14 See Rob van Gestel & Gijs van Dijck, Better Regulation Through Experimental Legislation, 17 Eur. Pub. L. 539, 539 (2011) (discussing experimental legislation as a means to improve the quality of regulation and fulfill the requirements of the socalled \u201cBetter Regulation\u201d). 15 For more on regulating without sufficient information, see Roberta Romano, Regulating in the Dark, in Regulatory Breakdown: The Crisis of Confidence in U.S. Regulation 86, 96 (Cary Coglianese ed., 2012). LCB_19_4_Art_1_Ranchordas (Do Not Delete) 4/16/2016 4:38 PM 876 LEWIS & CLARK LAW REVIEW [Vol. 19:4 lation that is precocious in relation to the state of technology and its commercialization. A sunrise clause is a disposition that provides that a part of a statute or regulation only comes into force after a specific date in the future and its coming into effect is contingent upon the verification of specific conditions. Sunrise clauses are thus a form of contingent legislation or regulation because the coming into effect of a certain provision depends on the fulfillment of a certain condition (e.g., generalized compliance with certain safety regulations). This Article makes a theoretical claim on the timing of regulatory interventions, which could be applied beyond the realm of sharingeconomy practices. Sharing-economy practices are used to illustrate this Article\u2019s argument because of the innovative and disruptive character of this sector, the limited amount of legal research in this area, the polarized debate between regulators, different interest groups (e.g., taxi drivers) defending the maintenance of existing legislation and digital platforms criticizing its obsolete and innovation-stifling character. In addition, while sharing-economy platforms are being accused of circumventing regulations and are seeking new and negotiated alternatives with public authorities (for example, by establishing partnerships with cities or negotiating rulemaking), federal regulators (such as the Federal Trade Commission) seem to be still observing the evolution of the sector. This Article proceeds as follows. In Part I, I define \u201cinnovation\u201d and explain the characteristics of its process. For the purposes of this Article, innovation is defined as the concretization of new ideas and their translation into welfare-enhancing commercial or social outcomes by using new processes, products, or services. In this Article, I analyze the complexities of the innovation process and its relevance for regulation, beyond the traditional intellectual-property-law debate on this topic. I argue that innovation is a complex and evolving process that requires state intervention but which also can be easily stifled if regulations are not permeable 16 See my previous work on the use of sunset clauses in the context of innovation policy and regulation Sofia Ranchord\u00e1s, Innovation-friendly Regulation: The Sunset of Regulation, the Sunrise of Innovation, 55 Jurimetrics 201 (2015). 17 See Mark Freeman, Necessary Evils: Amnesties and the Search for Justice 142 (2009). 18 For more on home-sharing, see Airbnb, Tel Aviv to Create Interactive City Guide, Times Isr. (Sept. 11, 2015), http://www.timesofisrael.com/airbnb-tel-aviv-to-createinteractive-city-guide-2/. Airbnb is currently establishing partnerships not only with multiple large companies including KLM, T-Mobile or American Express but also with cities such as Tel Aviv. For more on ride-sharing, see Germany: Uber Wins ",
      "fieldsOfStudy": null,
      "topics": [
        "traditional regulation",
        "certain safety regulations",
        "regulations",
        "experimental regulations",
        "regulation Sofia Ranchord\u00e1s",
        "economy platforms",
        "innovation policy",
        "temporary regulatory frameworks",
        "innovation",
        "innovation claims",
        "Better Regulation",
        "innovative practices",
        "Times Isr.",
        "U.S. Regulation"
      ]
    },
    "org": {
      "title": "A Traffic Model Based on Fuzzy Cellular Automata",
      "url": "https://www.semanticscholar.org/paper/2095922900af3eb6426d6daa3d7d21ccff13b5f5",
      "abstract": "Cellular automata (CA) play an important role in the development of computationally efficient microscopic traffic models and recently have gained considerable importance as a mean of optimising traffic control strategies. However, real-time application of the available CA models in traffic control systems is a difficult task due to their discrete and stochastic nature. This paper introduces a novel method for simulation of signalised traffic streams, which combines CA and fuzzy numbers. The introduced traffic simulation algorithm eliminates main drawbacks of the CA approach, i.e. necessity of multiple Monte Carlo simulations and calibration issues. Computational cost of traffic simulation for the proposed algorithm is considerably lower than the cost of simulation based on stochastic CA. Thus, the simulation results can be obtained in a much shorter time. Experiments confirmed that the simulation results for the introduced algorithm are consistent with that observed for stochastic CA. The proposed simulation algorithm is suitable for real-time applications in traffic control systems.",
      "fieldsOfStudy": [
        "Computer Science"
      ],
      "topics": [
        "stochastic CA",
        "traffic simulation",
        "traffic control strategies",
        "CA",
        "multiple Monte Carlo simulations",
        "signalised traffic streams",
        "simulation",
        "fuzzy numbers",
        "calibration issues",
        "The introduced traffic simulation algorithm",
        "Monte Carlo",
        "computationally efficient microscopic traffic models",
        "available CA models",
        "considerable importance",
        "CA approach"
      ]
    }
  },
  {
    "sim": 0.354900530996479,
    "gen": {
      "title": "A Fast Summation Method for translation invariant kernels",
      "url": "https://www.semanticscholar.org/paper/aca6f58dec66e8fc6250f3a78c5a378b06473324",
      "abstract": "We derive a Fast Multipole Method (FMM) where a low-rank approximation of the kernel is obtained using the Empirical Interpolation Method (EIM). Contrary to classical interpolation-based FMM, where the interpolation points and basis are fixed beforehand, the EIM is a nonlinear approximation method which constructs interpolation points and basis which are adapted to the kernel under consideration. The basis functions are obtained using evaluations of the kernel itself. We restrict ourselves to translation-invariant kernels, for which a modified version of the EIM approximation can be used in a multilevel FMM context; we call the obtained algorithm Empirical Interpolation Fast Multipole Method (EIFMM). An important feature of the EIFMM is a built-in error estimation of the interpolation error made by the low-rank approximation of the far-field behavior of the kernel: the algorithm selects the optimal number of interpolation points required to ensure a given accuracy for the result, leading to important gains for inhomogeneous kernels.",
      "fieldsOfStudy": [
        "Mathematics"
      ],
      "topics": [
        "inhomogeneous kernels",
        "EIM",
        "Fast Multipole Method",
        "FMM",
        "important gains",
        "EIM approximation",
        "classical interpolation-based FMM",
        "basis",
        "Empirical Interpolation Method",
        "translation-invariant kernels",
        "nonlinear approximation method",
        "interpolation points",
        "interpolation error",
        "EIFMM"
      ]
    },
    "org": {
      "title": "Watermarking Graph Neural Networks by Random Graphs",
      "url": "https://www.semanticscholar.org/paper/3cbf2652a22c2add7936f53a9d0931339cc7430c",
      "abstract": "Many learning tasks require us to deal with graph data which contains rich relational information among elements, leading increasing graph neural network (GNN) models to be deployed in industrial products for improving service quality. However, they also raise challenges to model authentication. It is necessary to protect the ownership of the GNN models, which motivates us to watermark GNN models. In this work, an Erdos-Renyi (ER) random graph with random node feature vectors and labels is randomly generated as a trigger to train the GNN to be protected together with the normal samples. During model training, the secret watermark is embedded into the label predictions of graph nodes. During model verification, by activating a marked GNN with the trigger ER graph, the watermark can be reconstructed from the output to verify the ownership. Since the ER graph was randomly generated, by feeding it to a non-marked GNN, the label predictions of graph nodes are random, resulting in a low false alarm rate (of proposed work). Experimental results have also shown that, the performance of a marked GNN on its original task will not be impaired. And, it is robust against model compression and fine-tuning, which has shown superiority and applicability.",
      "fieldsOfStudy": [
        "Computer Science"
      ],
      "topics": [
        "graph nodes",
        "graph data",
        "random node feature vectors",
        "GNN",
        "model compression",
        "model training",
        "proposed work",
        "service quality",
        "leading increasing graph neural network (GNN) models",
        "labels",
        "rich relational information",
        "trigger ER graph",
        "GNN models",
        "non-marked GNN",
        "model authentication",
        "model verification",
        "graph neural network (GNN"
      ]
    }
  },
  null,
  null,
  null,
  {
    "sim": 0.6216553081547732,
    "gen": {
      "title": "Improving Generalization of Deep Reinforcement Learning-based TSP Solvers",
      "url": "https://www.semanticscholar.org/paper/bcd69677a409ae64fde218108a425d8855580855",
      "abstract": "Recent work applying deep reinforcement learning (DRL) to solve traveling salesman problems (TSP) has shown that DRL-based solvers can be fast and competitive with TSP heuristics for small instances, but do not generalize well to larger instances. In this work, we propose a novel approach named MAGIC that includes a deep learning architecture and a DRL training method. Our architecture, which integrates a multilayer perceptron, a graph neural network, and an attention model, defines a stochastic policy that sequentially generates a TSP solution. Our training method includes several innovations: (1) we interleave DRL policy gradient updates with local search (using a new local search technique), (2) we use a novel simple baseline, and (3) we apply curriculum learning. Finally, we empirically demonstrate that MAGIC is superior to other DRL-based methods on random TSP instances, both in terms of performance and generalizability. Moreover, our method compares favorably against TSP heuristics and other state-of-the-art approach in terms of performance and computational time.",
      "fieldsOfStudy": [
        "Computer Science"
      ],
      "topics": [
        "larger instances",
        "random TSP instances",
        "small instances",
        "DRL policy gradient",
        "computational time",
        "TSP heuristics",
        "local search",
        "deep reinforcement learning",
        "curriculum learning",
        "DRL",
        "traveling salesman problems",
        "performance",
        "TSP",
        "terms",
        "generalizability",
        "DRL policy gradient updates",
        "DRL-based methods"
      ]
    },
    "org": {
      "title": "Reasoning about Unforeseen Possibilities During Policy Learning",
      "url": "https://www.semanticscholar.org/paper/7d5a0c8ac654e4a285bf2067a2417d7c813653fd",
      "abstract": "Methods for learning optimal policies in autonomous agents often assume that the way the domain is conceptualised---its possible states and actions and their causal structure---is known in advance and does not change during learning. This is an unrealistic assumption in many scenarios, because new evidence can reveal important information about what is possible, possibilities that the agent was not aware existed prior to learning. We present a model of an agent which both discovers and learns to exploit unforeseen possibilities using two sources of evidence: direct interaction with the world and communication with a domain expert. We use a combination of probabilistic and symbolic reasoning to estimate all components of the decision problem, including its set of random variables and their causal dependencies. Agent simulations show that the agent converges on optimal polices even when it starts out unaware of factors that are critical to behaving optimally.",
      "fieldsOfStudy": [
        "Computer Science"
      ],
      "topics": [
        "autonomous agents",
        "unforeseen possibilities",
        "Agent simulations",
        "possibilities",
        "random variables",
        "optimal polices",
        "direct interaction",
        "important information",
        "evidence",
        "advance",
        "actions",
        "communication",
        "factors",
        "learning",
        "optimal policies",
        "causal dependencies"
      ]
    }
  },
  {
    "sim": 0.40503460105826017,
    "gen": {
      "title": "An empirical study of latency in an emerging class of edge computing applications for wearable cognitive assistance",
      "url": "https://www.semanticscholar.org/paper/f16e216aa66af2b76152877372f9e4e6cfb89845",
      "abstract": "An emerging class of interactive wearable cognitive assistance applications is poised to become one of the key demonstrators of edge computing infrastructure. In this paper, we design seven such applications and evaluate their performance in terms of latency across a range of edge computing configurations, mobile hardware, and wireless networks, including 4G LTE. We also devise a novel multi-algorithm approach that leverages temporal locality to reduce end-to-end latency by 60% to 70%, without sacrificing accuracy. Finally, we derive target latencies for our applications, and show that edge computing is crucial to meeting these targets.",
      "fieldsOfStudy": [
        "Computer Science"
      ],
      "topics": [
        "edge computing infrastructure",
        "edge computing configurations",
        "edge computing",
        "target latencies",
        "latency",
        "interactive wearable cognitive assistance applications",
        "wireless networks",
        "mobile hardware",
        "end",
        "accuracy",
        "temporal locality",
        "4G LTE",
        "terms",
        "novel multi-algorithm approach",
        "seven such applications"
      ]
    },
    "org": {
      "title": "Seeding for pervasively overlapping communities",
      "url": "https://www.semanticscholar.org/paper/fba29d6ad8991f0844cf9c745788399195f5e27b",
      "abstract": "In some social and biological networks, the majority of nodes belong to multiple communities. It has recently been shown that a number of the algorithms specifically designed to detect overlapping communities do not perform well in such highly overlapping settings. Here, we consider one class of these algorithms, those which optimize a local fitness measure, typically by using a greedy heuristic to expand a seed into a community. We perform synthetic benchmarks which indicate that an appropriate seeding strategy becomes more important as the extent of community overlap increases. We find that distinct cliques provide the best seeds. We find further support for this seeding strategy with benchmarks on a Facebook network and the yeast interactome.",
      "fieldsOfStudy": [
        "Computer Science",
        "Physics",
        "Medicine"
      ],
      "topics": [
        "community overlap increases",
        "overlapping communities",
        "multiple communities",
        "highly overlapping settings",
        "synthetic benchmarks",
        "community",
        "benchmarks",
        "appropriate seeding strategy",
        "Facebook",
        "nodes",
        "best seeds",
        "distinct cliques",
        "support",
        "yeast interactome",
        "local fitness measure",
        "seeding strategy"
      ]
    }
  },
  null,
  {
    "sim": 0.3783143840679142,
    "gen": {
      "title": "Occupancy-driven energy management for smart building automation",
      "url": "https://www.semanticscholar.org/paper/fa37dad7dbb14c796811f501132065f82803b29b",
      "abstract": "Buildings are among the largest consumers of electricity in the US. A significant portion of this energy use in buildings can be attributed to HVAC systems used to maintain comfort for occupants. In most cases these building HVAC systems run on fixed schedules and do not employ any fine grained control based on detailed occupancy information. In this paper we present the design and implementation of a presence sensor platform that can be used for accurate occupancy detection at the level of individual offices. Our presence sensor is low-cost, wireless, and incrementally deployable within existing buildings. Using a pilot deployment of our system across ten offices over a two week period we identify significant opportunities for energy savings due to periods of vacancy. Our energy measurements show that our presence node has an estimated battery lifetime of over five years, while detecting occupancy accurately. Furthermore, using a building simulation framework and the occupancy information from our testbed, we show potential energy savings from 10% to 15% using our system.",
      "fieldsOfStudy": [
        "Computer Science",
        "Engineering"
      ],
      "topics": [
        "detailed occupancy information",
        "potential energy savings",
        "energy savings",
        "accurate occupancy detection",
        "occupancy",
        "individual offices",
        "HVAC systems",
        "significant opportunities",
        "periods",
        "existing buildings",
        "buildings",
        "occupancy information",
        "fixed schedules",
        "occupants",
        "vacancy"
      ]
    },
    "org": {
      "title": "Prediction of fitness in bacteria with causal jump dynamic mode decomposition",
      "url": "https://www.semanticscholar.org/paper/69786f07bf965650cb70dd4473997d5c8b7fb06a",
      "abstract": "In this paper, we consider the problem of learning a predictive model for population cell growth dynamics as a function of the media conditions. We first introduce a generic data-driven framework for training operator-theoretic models to predict cell growth rate. We then introduce the experimental design and data generated in this study, namely growth curves of Pseudomonas putida as a function of casein and glucose concentrations. We use a data driven approach for model identification, specifically the nonlinear autoregressive (NAR) model to represent the dynamics. We show theoretically that Hankel DMD can be used to obtain a solution of the NAR model. We show that it identifies a constrained NAR model and to obtain a more general solution, we define a causal state space system using 1-step, 2-step,\u2026, \u03c4-step predictors of the NAR model and identify a Koopman operator for this model using extended dynamic mode decomposition. The hybrid scheme we call causal-jump dynamic mode decomposition, which we illustrate on a growth profile or fitness prediction challenge as a function of different input growth conditions. We show that our model is able to recapitulate training growth curve data with 96.6% accuracy and predict test growth curve data with 91% accuracy.",
      "fieldsOfStudy": [
        "Computer Science",
        "Mathematics",
        "Engineering",
        "Biology"
      ],
      "topics": [
        "different input growth conditions",
        "test growth curve data",
        "training growth curve data",
        "population cell growth dynamics",
        "cell growth rate",
        "model identification",
        "extended dynamic mode decomposition",
        "fitness prediction challenge",
        "growth curves",
        "data",
        "growth profile",
        "constrained NAR model",
        "NAR",
        "operator-theoretic models",
        "causal-jump dynamic mode decomposition",
        "Pseudomonas putida",
        "NAR model"
      ]
    }
  },
  {
    "sim": 0.3308657832960218,
    "gen": {
      "title": "Uncertainty Relations and Sparse Signal Recovery for Pairs of General Signal Sets",
      "url": "https://www.semanticscholar.org/paper/4f9c7db3562361f1222f4edd6c51e4642237c553",
      "abstract": "We present an uncertainty relation for the representation of signals in two different general (possibly redundant or incomplete) signal sets. This uncertainty relation is relevant for the analysis of signals containing two distinct features each of which can be described sparsely in a suitable general signal set. Furthermore, the new uncertainty relation is shown to lead to im- proved sparsity thresholds for recovery of signals that are sparse in general dictionaries. Specifically, our results improve on the well-known (1 + 1/d)/2-threshold for dictionaries with coherence d by up to a factor of two. Furthermore, we provide probabilistic recovery guarantees for pairs of general dictionaries that also allow us to understand which parts of a general dictionary one needs to randomize over to \"weed out\" the sparsity patterns that prohibit breaking the square-root bottleneck.",
      "fieldsOfStudy": [
        "Computer Science",
        "Mathematics"
      ],
      "topics": [
        "dictionaries",
        "signals",
        "proved sparsity thresholds",
        "suitable general signal set",
        "coherence d",
        "general dictionary",
        "probabilistic recovery guarantees",
        "recovery",
        "square-root bottleneck",
        "sparsity patterns",
        "possibly redundant or incomplete) signal sets",
        "pairs",
        "distinct features",
        "factor",
        "im- proved sparsity thresholds",
        "coherence",
        "different general (possibly redundant or incomplete) signal sets"
      ]
    },
    "org": {
      "title": "A hybrid particle volume-of-fluid method for curvature estimation in multiphase flows",
      "url": "https://www.semanticscholar.org/paper/61730870a14800417e4b21f366f129340bd11587",
      "abstract": null,
      "fieldsOfStudy": [
        "Physics",
        "Computer Science",
        "Mathematics"
      ]
    }
  },
  {
    "sim": 0.7872663847970792,
    "gen": {
      "title": "Reinforcement Learning with Efficient Active Feature Acquisition",
      "url": "https://www.semanticscholar.org/paper/9ad367922821b20701dd79164f9319d8864d1af0",
      "abstract": "Solving real-life sequential decision making problems under partial observability involves an exploration-exploitation problem. To be successful, an agent needs to efficiently gather valuable information about the state of the world for making rewarding decisions. However, in real-life, acquiring valuable information is often highly costly, e.g., in the medical domain, information acquisition might correspond to performing a medical test on a patient. This poses a significant challenge for the agent to perform optimally for the task while reducing the cost for information acquisition. In this paper, we propose a model-based reinforcement learning framework that learns an active feature acquisition policy to solve the exploration-exploitation problem during its execution. Key to the success is a novel sequential variational auto-encoder that learns high-quality representations from partially observed states, which are then used by the policy to maximize the task reward in a cost efficient manner. We demonstrate the efficacy of our proposed framework in a control domain as well as using a medical simulator. In both tasks, our proposed method outperforms conventional baselines and results in policies with greater cost efficiency.",
      "fieldsOfStudy": [
        "Computer Science"
      ],
      "topics": [
        "problems",
        "information acquisition",
        "partial observability",
        "greater cost efficiency",
        "valuable information",
        "rewarding decisions",
        "policies",
        "active feature acquisition policy",
        "exploration-exploitation problem",
        "cost efficient manner",
        "conventional baselines",
        "real-life sequential decision",
        "medical domain",
        "medical test",
        "real-life sequential decision making problems",
        "medical simulator"
      ]
    },
    "org": {
      "title": "MULEX: Disentangling Exploitation from Exploration in Deep RL",
      "url": "https://www.semanticscholar.org/paper/3da2d9633d3ab38205e9baf356ce263358963975",
      "abstract": "An agent learning through interactions should balance its action selection process between probing the environment to discover new rewards and using the information acquired in the past to adopt useful behaviour. This trade-off is usually obtained by perturbing either the agent's actions (e.g., e-greedy or Gibbs sampling) or the agent's parameters (e.g., NoisyNet), or by modifying the reward it receives (e.g., exploration bonus, intrinsic motivation, or hand-shaped rewards). Here, we adopt a disruptive but simple and generic perspective, where we explicitly disentangle exploration and exploitation. Different losses are optimized in parallel, one of them coming from the true objective (maximizing cumulative rewards from the environment) and others being related to exploration. Every loss is used in turn to learn a policy that generates transitions, all shared in a single replay buffer. Off-policy methods are then applied to these transitions to optimize each loss. We showcase our approach on a hard-exploration environment, show its sample-efficiency and robustness, and discuss further implications.",
      "fieldsOfStudy": [
        "Computer Science",
        "Mathematics"
      ],
      "topics": [
        "new rewards",
        "cumulative rewards",
        "implications",
        "useful behaviour",
        "exploitation",
        "robustness",
        "reward",
        "hard-exploration environment",
        "transitions",
        "Different losses",
        "NoisyNet",
        "single replay buffer",
        "Gibbs",
        "intrinsic motivation",
        "hand-shaped rewards"
      ]
    }
  },
  {
    "sim": 0.5411283933282024,
    "gen": {
      "title": "Who Invents?: Evidence from the Japan-U.S. inventor survey",
      "url": "https://www.semanticscholar.org/paper/e53bfae33cf8da07d4680e144120639cf4c52cfc",
      "abstract": "Human resources are increasingly seen as a key to innovation competitiveness, and there is a need for detailed, systematic data on the demographics of inventors, their motivations, and their careers. To gain systematic data on who invents, we collected detailed information on a sample of inventors in the U.S. and Japan (the RIETI-Georgia Tech inventor survey). The data come from a unique set of matched surveys of U.S. and Japanese inventors of triadic patents, i.e., patents from patent families with granted patents in the U.S. and applications filed in Japan and in the EPO, with data from over 1900 responses from the U.S. and over 3600 responses from Japan. Based on these survey data, we compare the profiles, motivations, mobility and performance of inventors in the U.S. and Japan. Overall, we find some important similarities between inventors in the U.S. and Japan. The distribution across functional affiliations within the firm, by gender, by educational fields and their motivations, are all quite similar. In particular, in both countries we find inventors emphasizing task motivations over pecuniary motivations. Firm-centered motivation (e.g., generating value for my firm) is also an important reason for inventing and this reason is relatively more important in the U.S. than Japan. Their distribution across types of organizations is quite similar. The percent of university inventors is nearly the same in the two countries, and the distribution of these inventors across technology classes is also quite similar. However, the percent from very small firms is significantly higher in the U.S. There are a few important differences. American inventors are much more likely to have a Ph.D. American inventors are older (even controlling for differences in the share of the inventors with Ph.D.s). The modal Japanese inventor has his first invention in his 20s, while for the U.S., the mode is the early 30s, and we also find many more American inventors over age 55 at the time of their triadic patent invention. In both countries, older inventors tend to produce higher value patents. American inventors are also much more mobile (although Japanese inventors with Ph.D.s also have high rates of mobility, mainly in the form of secondments). In the U.S., mobility tends to decline with age, while in Japan, mobility is higher for older inventors (likely due to the differences in retirement ages in the two countries). In both countries, mobility is associated with greater access to outside information. Finally, we find that foreign-born inventors are very important in the U.S. (we did not collect data on country of origin for Japan). Overall, these results suggest that inventor characteristics may be important for firm performance, and that institutional differences may affect the profile of inventors in each country, although the inventors of the two countries are very similar in many respects. Future work will examine how these cross-national differences in inventor profiles affect innovation in each country.",
      "fieldsOfStudy": [
        "Engineering"
      ],
      "topics": [
        "inventor profiles",
        "older inventors",
        "American inventors",
        "inventors",
        "American inventors",
        "inventor characteristics",
        "university inventors",
        "U.S.",
        "higher value patents",
        "Japan",
        "triadic patents",
        "U.S. and Japanese inventors",
        "granted patents",
        "patent families"
      ]
    },
    "org": {
      "title": "A Text-Embedding-based Approach to Measure Patent-to-Patent Technological Similarity -- Workflow, Code, and Applications",
      "url": "https://www.semanticscholar.org/paper/63d31571b5046da51789761a04147137ee838cbb",
      "abstract": "This paper describes an e ciently scalable approach to measure technological similarity between patents by combining embedding techniques from natural language processing with nearest-neighbor approximation. Using this methodology we are able to compute existing similarities between all patents, which in turn enables us to represent the whole patent universe as a technological network. We validate both technological signature and similarity in various ways, and demonstrate at the case of electric vehicle technologies their usefulness to measure knowledge flows, map technological change, and create patent quality indicators. Thereby the paper contributes to the growing literature on text-based indicators for patent analysis. We provide thorough documentations of the method, including all code, indicators, and intermediate outputs at https://github.com/ANONYMEOUS_FOR_REVIEW).",
      "fieldsOfStudy": [
        "Computer Science"
      ],
      "topics": [
        "patent quality indicators",
        "patent analysis",
        "technological similarity",
        "patents",
        "technological change",
        "indicators",
        "natural language processing",
        "https://github.com/ANONYMEOUS_FOR_REVIEW",
        "intermediate outputs",
        "existing similarities",
        "knowledge flows",
        "electric vehicle",
        "similarity",
        "patent universe",
        "ways",
        "electric vehicle technologies",
        "embedding techniques"
      ]
    }
  },
  null,
  {
    "sim": 0.37986118780679345,
    "gen": {
      "title": "An Improved Authentication Protocol Dependent on Registration Center for Multiserver Environment",
      "url": "https://www.semanticscholar.org/paper/40d88ab01849a587ee3c640c45463891032ab886",
      "abstract": "In many protocols, registration center (RC) is just responsible for users' and servers' registration, so a malicious server may cheat users to serve as another legal server. It's necessary to verify the legality of both users and servers with the help of a third trusted party such as RC. In 2012, Li et al. proposed a dynamic identity based authentication protocol for multiserver environment. But their protocol is susceptible to insider attack, eavesdropping attack, masquerade attack and password guessing attack. Besides, their protocol can't provide user's anonymity. In this paper, we propose an improved authentication protocol dependent on RC for multiserver environment to remedy the aforementioned security flaws. Compared with previous protocols independent on RC, in our protocol RC is one of parties in authentication phase to verify the legality of users and servers, which can effectively avoid server spoofing attack. Our protocol only uses nonce, one-way hash function and exclusive-OR operation in its implementation, and it is much more secure and practical.",
      "fieldsOfStudy": [
        "Computer Science"
      ],
      "topics": [
        "password guessing attack",
        "masquerade attack",
        "eavesdropping attack",
        "attack",
        "insider attack",
        "servers",
        "RC",
        "users",
        "authentication phase",
        "multiserver environment",
        "previous protocols",
        "protocols",
        "parties",
        "legal server",
        "server spoofing attack",
        "registration center"
      ]
    },
    "org": {
      "title": "Collective defense of honeybee colonies: experimental results and theoretical modeling",
      "url": "https://www.semanticscholar.org/paper/807cd06a40c5190b13d889b9da56b3379e322315",
      "abstract": null,
      "fieldsOfStudy": [
        "Biology",
        "Computer Science"
      ]
    }
  },
  null,
  {
    "sim": 0.5478637676847297,
    "gen": {
      "title": "Generative Teaching Networks: Accelerating Neural Architecture Search by Learning to Generate Synthetic Training Data",
      "url": "https://www.semanticscholar.org/paper/16a3dd5ab3e8570f6083ddf6f88aa5e916450fef",
      "abstract": "This paper investigates the intriguing question of whether we can create learning algorithms that automatically generate training data, learning environments, and curricula in order to help AI agents rapidly learn. We show that such algorithms are possible via Generative Teaching Networks (GTNs), a general approach that is, in theory, applicable to supervised, unsupervised, and reinforcement learning, although our experiments only focus on the supervised case. GTNs are deep neural networks that generate data and/or training environments that a learner (e.g. a freshly initialized neural network) trains on for a few SGD steps before being tested on a target task. We then differentiate through the entire learning process via meta-gradients to update the GTN parameters to improve performance on the target task. GTNs have the beneficial property that they can theoretically generate any type of data or training environment, making their potential impact large. This paper introduces GTNs, discusses their potential, and showcases that they can substantially accelerate learning. We also demonstrate a practical and exciting application of GTNs: accelerating the evaluation of candidate architectures for neural architecture search (NAS), which is rate-limited by such evaluations, enabling massive speed-ups in NAS. GTN-NAS improves the NAS state of the art, finding higher performing architectures when controlling for the search proposal mechanism. GTN-NAS also is competitive with the overall state of the art approaches, which achieve top performance while using orders of magnitude less computation than typical NAS methods. Speculating forward, GTNs may represent a first step toward the ambitious goal of algorithms that generate their own training data and, in doing so, open a variety of interesting new research questions and directions.",
      "fieldsOfStudy": [
        "Computer Science",
        "Mathematics"
      ],
      "topics": [
        "learning environments",
        "typical NAS methods",
        "training data",
        "NAS",
        "neural architecture search",
        "higher performing architectures",
        "interesting new research questions",
        "candidate architectures",
        "deep neural networks",
        "evaluations",
        "AI agents",
        "orders",
        "algorithms",
        "learning algorithms",
        "training environments",
        "learning",
        "data"
      ]
    },
    "org": {
      "title": "Moving Target Defense for Deep Visual Sensing against Adversarial Examples",
      "url": "https://www.semanticscholar.org/paper/a843282f11009db16f90e941665271694f86c6b9",
      "abstract": "Deep learning based visual sensing has achieved attractive accuracy but is shown vulnerable to adversarial example attacks. Specifically, once the attackers obtain the deep model, they can construct adversarial examples to mislead the model to yield wrong classification results. Deployable adversarial examples such as small stickers pasted on the road signs and lanes have been shown effective in misleading advanced driver-assistance systems. Many existing countermeasures against adversarial examples build their security on the attackers' ignorance of the defense mechanisms. Thus, they fall short of following Kerckhoffs's principle and can be subverted once the attackers know the details of the defense. This paper applies the strategy of moving target defense (MTD) to generate multiple new deep models after system deployment, that will collaboratively detect and thwart adversarial examples. Our MTD design is based on the adversarial examples' minor transferability to models differing from the one (e.g., the factory-designed model) used for attack construction. The post-deployment quasi-secret deep models significantly increase the bar for the attackers to construct effective adversarial examples. We also apply the technique of serial data fusion with early stopping to reduce the inference time by a factor of up to 5 while maintaining the sensing and defense performance. Extensive evaluation based on three datasets including a road sign image database and a GPU-equipped Jetson embedded computing board shows the effectiveness of our approach.",
      "fieldsOfStudy": [
        "Computer Science"
      ],
      "topics": [
        "effective adversarial examples",
        "Deployable adversarial examples",
        "multiple new deep models",
        "attack construction",
        "system deployment",
        "target defense",
        "wrong classification results",
        "based visual sensing",
        "The post-deployment quasi-secret deep models",
        "computing board",
        "small stickers",
        "adversarial examples minor transferability",
        "deep model",
        "models",
        "Deep learning",
        "visual sensing"
      ]
    }
  },
  {
    "sim": 0.4338626803146235,
    "gen": {
      "title": "A statistical perspective on algorithmic leveraging",
      "url": "https://www.semanticscholar.org/paper/d3a4594317fa9051f3817d3a016b58367d1119bf",
      "abstract": "One popular method for dealing with large-scale data sets is sampling. Using the empirical statistical leverage scores as an importance sampling distribution, the method of algorithmic leveraging samples and rescales data matrices to reduce the data size before performing computations on the subproblem. Existing work has focused on algorithmic issues, but none of it addresses statistical aspects of this method. Here, we provide an effective framework to evaluate the statistical properties of algorithmic leveraging in the context of estimating parameters in a linear regression model. In particular, for several versions of leverage-based sampling, we derive results for the bias and variance. We show that from the statistical perspective of bias and variance, neither leverage-based sampling nor uniform sampling dominates the other. This result is particularly striking, given the well-known result that, from the algorithmic perspective of worst-case analysis, leverage-based sampling provides uniformly superior worst-case algorithmic results, when compared with uniform sampling. Based on these theoretical results, we propose and analyze two new leveraging algorithms: one constructs a smaller least-squares problem with \"shrinked\" leverage scores (SLEV), and the other solves a smaller and unweighted (or biased) least-squares problem (LEVUNW). The empirical results indicate that our theory is a good predictor of practical performance of existing and new leverage-based algorithms and that the new algorithms achieve improved performance.",
      "fieldsOfStudy": [
        "Computer Science",
        "Mathematics"
      ],
      "topics": [
        "uniform sampling",
        "algorithmic leveraging",
        "algorithmic issues",
        "sampling",
        "improved performance",
        "practical performance",
        "results",
        "empirical statistical leverage scores",
        "statistical aspects",
        "data matrices",
        "existing and new leverage-based algorithms",
        "new leveraging algorithms",
        "leverage-based sampling",
        "importance sampling distribution"
      ]
    },
    "org": {
      "title": "JetsonLEAP: a Framework to Measure Power on a Heterogeneous System-on-a-Chip Device",
      "url": "https://www.semanticscholar.org/paper/8b48c6daee7f96ec133de34943d6f0349c292190",
      "abstract": null,
      "fieldsOfStudy": [
        "Computer Science"
      ]
    }
  },
  {
    "sim": 0.28842368553368947,
    "gen": {
      "title": "Optimal Rate Sampling in 802.11 systems",
      "url": "https://www.semanticscholar.org/paper/bf9c989b9c9216407f7dd9889c5d4f1e12e71372",
      "abstract": "Rate Adaptation (RA) is a fundamental mechanism in 802.11 systems. It allows transmitters to adapt the coding and modulation scheme as well as the MIMO transmission mode to the radio channel conditions, and in turn, to learn and track the (mode, rate) pair providing the highest throughput. So far, the design of RA mechanisms has been mainly driven by heuristics. In contrast, in this paper, we rigorously formulate such design as an online stochastic optimisation problem. We solve this problem and present ORS (Optimal Rate Sampling), a family of (mode, rate) pair adaptation algorithms that provably learn as fast as it is possible the best pair for transmission. We study the performance of ORS algorithms in stationary radio environments where the successful packet transmission probabilities at the various (mode, rate) pairs do not vary over time, and in non-stationary environments where these probabilities evolve. We show that under ORS algorithms, the throughput loss due to the need to explore sub-optimal (mode, rate) pairs does not depend on the number of available pairs. This is a crucial advantage as evolving 802.11 standards offer an increasingly large number of (mode, rate) pairs. We illustrate the efficiency of ORS algorithms (compared to the state-of-the-art algorithms) using simulations and traces extracted from 802.11 test-beds.",
      "fieldsOfStudy": [
        "Computer Science",
        "Mathematics"
      ],
      "topics": [
        "available pairs",
        "Optimal Rate Sampling",
        "Rate Adaptation",
        "stationary radio environments",
        "mode, rate) pair adaptation algorithms",
        "ORS algorithms",
        "transmission",
        "sub-optimal (mode, rate) pairs",
        "MIMO transmission mode",
        "best pair",
        "successful packet transmission probabilities",
        "(mode, rate) pairs",
        "present ORS",
        "(mode, rate) pairs",
        "(mode, rate) pair adaptation algorithms",
        "time",
        "(mode, rate) pair"
      ]
    },
    "org": {
      "title": "The Handbook of Engineering Self-Aware and Self-Expressive Systems",
      "url": "https://www.semanticscholar.org/paper/824604ac0dc055f072ef9a82f6c979067fdaeb9f",
      "abstract": "When faced with the task of designing and implementing a new self-aware and self-expressive computing system, researchers and practitioners need a set of guidelines on how to use the concepts and foundations developed in the Engineering Proprioception in Computing Systems (EPiCS) project. This report provides such guidelines on how to design self-aware and self-expressive computing systems in a principled way. We have documented different categories of self-awareness and self-expression level using architectural patterns. We have also documented common architectural primitives, their possible candidate techniques and attributes for architecting self-aware and self-expressive systems. Drawing on the knowledge obtained from the previous investigations, we proposed a pattern driven methodology for engineering self-aware and self-expressive systems to assist in utilising the patterns and primitives during design. The methodology contains detailed guidance to make decisions with respect to the possible design alternatives, providing a systematic way to build self-aware and self-expressive systems. Then, we qualitatively and quantitatively evaluated the methodology using two case studies. The results reveal that our pattern driven methodology covers the main aspects of engineering self-aware and self-expressive systems, and that the resulted systems perform significantly better than the non-self-aware systems.",
      "fieldsOfStudy": [
        "Computer Science"
      ],
      "topics": [
        "non-self-aware systems",
        "new self-aware and self-expressive computing system",
        "architectural patterns",
        "self-awareness and self-expression level",
        "common architectural primitives",
        "resulted systems",
        "design",
        "guidelines",
        "primitives",
        "pattern driven methodology",
        "practitioners",
        "Computing Systems"
      ]
    }
  },
  {
    "sim": 0.5925544226971249,
    "gen": {
      "title": "Adaptive Three Operator Splitting",
      "url": "https://www.semanticscholar.org/paper/9ac9c5795a47230991a8b75b838daf905808609d",
      "abstract": "We propose and analyze an adaptive step-size variant of the Davis-Yin three operator splitting. This method can solve optimization problems composed by a sum of a smooth term for which we have access to its gradient and an arbitrary number of potentially non-smooth terms for which we have access to their proximal operator. The proposed method sets the step-size based on local information of the objective --hence allowing for larger step-sizes--, only requires two extra function evaluations per iteration and does not depend on any step-size hyperparameter besides an initial estimate. We provide an iteration complexity analysis that matches the best known results for the non-adaptive variant: sublinear convergence for general convex functions and linear convergence under strong convexity of the smooth term and smoothness of one of the proximal terms. Finally, an empirical comparison with related methods on 6 different problems illustrates the computational advantage of the proposed method.",
      "fieldsOfStudy": [
        "Mathematics",
        "Computer Science"
      ],
      "topics": [
        "general convex functions",
        "potentially non-smooth terms",
        "linear convergence",
        "strong convexity",
        "smooth term",
        "smooth term",
        "non-adaptive variant",
        "proximal terms",
        "convergence",
        "related methods",
        "local information",
        "proximal operator",
        "optimization problems",
        "smoothness",
        "larger step-sizes--",
        "sublinear convergence"
      ]
    },
    "org": {
      "title": "A splitting approach for the Kadomtsev-Petviashvili equation",
      "url": "https://www.semanticscholar.org/paper/348f08f0a61a62af72ae2f162ad9c95337d3b1fa",
      "abstract": null,
      "fieldsOfStudy": [
        "Physics",
        "Computer Science",
        "Mathematics"
      ]
    }
  },
  null,
  {
    "sim": 0.5595915300910216,
    "gen": {
      "title": "Preserving privacy between features in distributed estimation",
      "url": "https://www.semanticscholar.org/paper/b3a355097bb3b4f5e130fe38c50384e428a0db2d",
      "abstract": "Privacy is crucial in many applications of machine learning. Legal, ethical and societal issues restrict the sharing of sensitive data, making it difficult to learn from data sets that are partitioned between many parties. One important instance of such a distributed setting arises when information about each record in the data set is held by different data owners (the design matrix is \u201cvertically partitioned\u201d).In this setting, few approaches exist for private data sharing for the purpose of statistical estimation, and the classical set\u2010up of differential privacy with a \u201ctrusted curator\u201d preparing the data does not apply. We work with the notion of (\u03f5,\u03b4)\u2010distributed differential privacy, which extends single\u2010party differential privacy to the distributed, vertically partitioned case. We propose PRIDE, a scalable framework for distributed estimation where each party communicates perturbed random projections of their locally held features ensuring (\u03f5,\u03b4)\u2010distributed differential privacy is preserved. For \u21132\u2010penalized supervised learning problems, PRIDE\u2009has bounded estimation error compared with the optimal estimates obtained without privacy constraints in the non\u2010distributed setting. We confirm this empirically on real world and synthetic data sets. \u00a9 2018 John Wiley & Sons, Ltd.",
      "fieldsOfStudy": [
        "Computer Science",
        "Mathematics"
      ],
      "topics": [
        "data sets",
        "private data sharing",
        "different data owners",
        "synthetic data sets",
        "differential privacy",
        "sensitive data",
        "single\u2010party differential privacy",
        "privacy constraints",
        "Privacy",
        "parties",
        "distributed estimation",
        "statistical estimation",
        "estimation error",
        "machine learning",
        "random projections",
        "perturbed random projections",
        "applications"
      ]
    },
    "org": {
      "title": "Improving the Diversity of Top-N Recommendation via Determinantal Point Process",
      "url": "https://www.semanticscholar.org/paper/ea938f2675beb27460d9706b4bad51f73c2c3f49",
      "abstract": "Recommender systems take the key responsibility to help users discover items that they might be interested in. Many recommendation algorithms are built upon similarity measures, which usually result in low intra-list diversity. The deficiency in capturing the whole range of user interest often leads to poor satisfaction. To solve this problem, increasing attention has been paid on improving the diversity of recommendation results in recent years. \nIn this paper, we propose a novel method to improve the diversity of top-$N$ recommendation results based on the determinantal point process (DPP), which is an elegant model for characterizing the repulsion phenomenon. We propose an acceleration algorithm to greatly speed up the process of the result inference, making our algorithm practical for large-scale scenarios. We also incorporate a tunable parameter into the DPP model which allows the users to smoothly control the level of diversity. More diversity metrics are introduced to better evaluate diversification algorithms. We have evaluated our algorithm on several public datasets, and compared it thoroughly with other reference algorithms. Results show that our proposed algorithm provides a much better accuracy-diversity trade-off with comparable efficiency.",
      "fieldsOfStudy": [
        "Computer Science"
      ],
      "topics": [
        "low intra-list diversity",
        "reference algorithms",
        "diversity",
        "Many recommendation algorithms",
        "diversification algorithms",
        "More diversity metrics",
        "recommendation results",
        "comparable efficiency",
        "Results",
        "poor satisfaction",
        "recent years",
        "user interest",
        "public datasets",
        "users"
      ]
    }
  },
  null,
  {
    "sim": 0.5721314894854853,
    "gen": {
      "title": "Optimization Based Solutions for Control and State Estimation in Non-holonomic Mobile Robots: Stability, Distributed Control, and Relative Localization",
      "url": "https://www.semanticscholar.org/paper/6e73fa3da7e94b0d3c000d6acd3f44c6d61ada7b",
      "abstract": "Interest in designing, manufacturing, and using autonomous robots has been rapidly growing \nduring the most recent decade. The main motivation for this interest is the wide range \nof potential applications these autonomous systems can serve in. The applications include, \nbut are not limited to, area coverage, patrolling missions, perimeter surveillance, search \nand rescue missions, and situational awareness. In this thesis, the area of control and \nstate estimation in non-holonomic mobile robots is tackled. Herein, optimization based \nsolutions for control and state estimation are designed, analyzed, and implemented to such \nsystems. One of the main motivations for considering such solutions is their ability of \nhandling constrained and nonlinear systems such as non-holonomic mobile robots. Moreover, \nthe recent developments in dynamic optimization algorithms as well as in computer \nprocessing facilitated the real-time implementation of such optimization based methods \nin embedded computer systems. \nTwo control problems of a single non-holonomic mobile robot are considered first; these \ncontrol problems are point stabilization (regulation) and path-following. Here, a model \npredictive control (MPC) scheme is used to fulfill these control tasks. More precisely, a \nspecial class of MPC is considered in which terminal constraints and costs are avoided. \nSuch constraints and costs are traditionally used in the literature to guarantee the asymptotic \nstability of the closed loop system. In contrast, we use a recently developed stability \ncriterion in which the closed loop asymptotic stability can be guaranteed by appropriately \nchoosing the prediction horizon length of the MPC controller. This method is based on finite time controllability as well as bounds on the MPC value function. \nAfterwards, a regulation control of a multi-robot system (MRS) is considered. In this \ncontrol problem, the objective is to stabilize a group of mobile robots to form a pattern. \nWe achieve this task using a distributed model predictive control (DMPC) scheme based \non a novel communication approach between the subsystems. This newly introduced \nmethod is based on the quantization of the robots\u2019 operating region. Therefore, the \nproposed communication technique allows for exchanging data in the form of integers \ninstead of floating-point numbers. Additionally, we introduce a differential communication \nscheme to achieve a further reduction in the communication load. \nFinally, a moving horizon estimation (MHE) design for the relative state estimation \n(relative localization) in an MRS is developed in this thesis. In this framework, robots \nwith less payload/computational capacity, in a given MRS, are localized and tracked \nusing robots fitted with high-accuracy sensory/computational means. More precisely, \nrelative measurements between these two classes of robots are used to localize the less \n(computationally) powerful robotic members. As a complementary part of this study, the \nMHE localization scheme is combined with a centralized MPC controller to provide an \nalgorithm capable of localizing and controlling an MRS based only on relative sensory \nmeasurements. The validity and the practicality of this algorithm are assessed by realtime \nlaboratory experiments. \nThe conducted study fills important gaps in the application area of autonomous navigation \nespecially those associated with optimization based solutions. Both theoretical as \nwell as practical contributions have been introduced in this research work. Moreover, this \nthesis constructs a foundation for using MPC without stabilizing constraints or costs in \nthe area of non-holonomic mobile robots.",
      "fieldsOfStudy": [
        "Mathematics",
        "Computer Science"
      ],
      "topics": [
        "mobile robots",
        "autonomous robots",
        "optimization based methods",
        "robots",
        "optimization based solutions",
        "embedded computer systems",
        "solutions",
        "Such constraints",
        "single non-holonomic mobile robot",
        "MRS",
        "dynamic optimization algorithms",
        "control",
        "MPC",
        "state estimation",
        "systems",
        "control problems"
      ]
    },
    "org": {
      "title": "Optimizing Simulations with Noise-Tolerant Structured Exploration",
      "url": "https://www.semanticscholar.org/paper/02d9c2949d340127818a111e4e66d62f88062278",
      "abstract": "We propose a simple drop-in noise-tolerant replacement for the standard finite difference procedure used ubiquitously in blackbox optimization. In our approach, parameter perturbation directions are defined by a family of structured orthogonal matrices. We show that at the small cost of computing a Fast Walsh-Hadamard/Fourier Transform (FWHT/FFT), such structured finite differences consistently give higher quality approximation of gradients and Jacobians in comparison to vanilla approaches that use coordinate directions or random Gaussian perturbations. We find that trajectory optimizers like Iterative LQR and Differential Dynamic Programming require fewer iterations to solve several classic continuous control tasks when our methods are used to linearize noisy, blackbox dynamics instead of standard finite differences. By embedding structured exploration in a quasi-Newton optimizer (LBFGS), we are able to learn agile walking and turning policies for quadruped locomotion, that successfully transfer from simulation to actual hardware. We theoretically justify our methods via bounds on the quality of gradient reconstruction and provide a basis for applying them also to nonsmooth problems.",
      "fieldsOfStudy": [
        "Computer Science",
        "Engineering"
      ],
      "topics": [
        "structured finite differences",
        "standard finite differences",
        "structured orthogonal matrices",
        "random Gaussian perturbations",
        "parameter perturbation directions",
        "actual hardware",
        "higher quality approximation",
        "blackbox optimization",
        "classic continuous control tasks",
        "blackbox dynamics",
        "coordinate directions",
        "structured exploration",
        "standard finite difference procedure",
        "vanilla approaches",
        "quadruped locomotion",
        "nonsmooth problems",
        "gradient reconstruction"
      ]
    }
  },
  {
    "sim": 0.4195335130575344,
    "gen": {
      "title": "A Secure Source Routing Protocol for Mobile Ad Hoc Networks",
      "url": "https://www.semanticscholar.org/paper/26ffb1dc02ecd470ff4bf0848273be91a54747c1",
      "abstract": "Routing protocols for Mobile Ad Hoc Networks (MANETs) have been \nextensively studied. Some of the well-known \nsource routing protocols presented in the literature that claim to \nestablish secure routes are susceptible to hidden channel attacks. In this \npaper, we address this issue and present a novel secure routing \nprotocol, based on sanitizable signatures, that is not susceptible to \nhidden channel attacks.",
      "fieldsOfStudy": [
        "Computer Science"
      ],
      "topics": [
        "hidden channel attacks",
        "secure routes",
        "Routing protocols",
        "sanitizable signatures",
        "Mobile Ad Hoc Networks (MANETs",
        "novel secure routing protocol",
        "well-known source routing protocols",
        "literature",
        "issue",
        "paper",
        "Some",
        "Mobile Ad Hoc Networks",
        "MANETs",
        "novel secure routing",
        "protocol"
      ]
    },
    "org": {
      "title": "In cloud, do MTC or HTC service providers benefit from the economies of scale?",
      "url": "https://www.semanticscholar.org/paper/8693080526ae9936b7a3cd0bbb9c66cfdea27f7c",
      "abstract": "Cloud computing, which is advocated as an economic platform for daily computing, has become a hot topic for both industrial and academic communities in the last couple of years. The basic idea behind cloud computing is that resource providers, which own the cloud platform, offer elastic resources to end users. In this paper, we intend to answer one key question to the success of cloud computing: in cloud, do many task computing (MTC) or high throughput computing (HTC) service providers, which offer the corresponding computing service to end users, benefit from the economies of scale? To the best of our knowledge, no previous work designs and implements the enabling system to consolidate MTC and HTC workloads on the cloud platform and no one answers the above question. Our research contributions are threefold: first, we propose an innovative usage model, called dynamic service provision (DSP) model, for MTC or HTC service providers. In the DSP model, the resource provider provides the service of creating and managing runtime environments for MTC or HTC service providers, and consolidates heterogeneous MTC or HTC workloads on the cloud platform; second, based on the DSP model, we design and implement Dawningcloud, which provides automatic management for heterogeneous workloads; third, a comprehensive evaluation of Dawningcloud has been performed in an emulatation experiment. We found that for typical workloads, in comparison with the previous two cloud solutions, Dawningcloud saves the resource consumption maximally by 46.4% (HTC) and 74.9% (MTC) for the service providers, and saves the total resource consumption maximally by 29.7% for the resource provider. At the same time, comparing with the traditional solution that provides MTC or HTC services with dedicated systems, Dawningcloud is more cost-effective. To this end, we conclude that for typical MTC and HTC workloads, on the cloud platform, MTC and HTC service providers and the resource service provider can benefit from the economies of scale.",
      "fieldsOfStudy": [
        "Computer Science"
      ],
      "topics": [
        "resource providers",
        "cloud computing",
        "MTC or HTC service providers",
        "called dynamic service provision",
        "HTC",
        "MTC",
        "task computing",
        "elastic resources",
        "resource service provider",
        "daily computing",
        "heterogeneous workloads",
        "scale",
        "typical MTC and HTC workloads",
        "cloud",
        "MTC or HTC services"
      ]
    }
  },
  {
    "sim": 0.6200554531670246,
    "gen": {
      "title": "Tell Me Where to Look: Guided Attention Inference Network",
      "url": "https://www.semanticscholar.org/paper/2622d2467f19bc60427f8ea495515e7da82316c9",
      "abstract": "Weakly supervised learning with only coarse labels can obtain visual explanations of deep neural network such as attention maps by back-propagating gradients. These attention maps are then available as priors for tasks such as object localization and semantic segmentation. In one common framework we address three shortcomings of previous approaches in modeling such attention maps: We (1) make attention maps an explicit and natural component of the end-to-end training for the first time, (2) provide self-guidance directly on these maps by exploring supervision from the network itself to improve them, and (3) seamlessly bridge the gap between using weak and extra supervision if available. Despite its simplicity, experiments on the semantic segmentation task demonstrate the effectiveness of our methods. We clearly surpass the state-of-the-art on PASCAL VOC 2012 test and val. sets. Besides, the proposed framework provides a way not only explaining the focus of the learner but also feeding back with direct guidance towards specific tasks. Under mild assumptions our method can also be understood as a plug-in to existing weakly supervised learners to improve their generalization performance.",
      "fieldsOfStudy": [
        "Computer Science"
      ],
      "topics": [
        "existing weakly supervised learners",
        "attention maps",
        "specific tasks",
        "tasks",
        "deep neural network",
        "Weakly supervised learning",
        "object localization",
        "attention",
        "direct guidance",
        "supervision",
        "semantic segmentation task",
        "visual explanations",
        "end",
        "val. sets",
        "weak and extra supervision",
        "These attention maps"
      ]
    },
    "org": {
      "title": "Training Robust Deep Neural Networks via Adversarial Noise Propagation",
      "url": "https://www.semanticscholar.org/paper/09804e8f56d848008f3a2e15c9db2aba1414a5ac",
      "abstract": "In practice, deep neural networks have been found to be vulnerable to various types of noise, such as adversarial examples and corruption. Various adversarial defense methods have accordingly been developed to improve adversarial robustness for deep models. However, simply training on data mixed with adversarial examples, most of these models still fail to defend against the generalized types of noise. Motivated by the fact that hidden layers play a highly important role in maintaining a robust model, this paper proposes a simple yet powerful training algorithm, named Adversarial Noise Propagation (ANP), which injects noise into the hidden layers in a layer-wise manner. ANP can be implemented efficiently by exploiting the nature of the backward-forward training style. Through thorough investigations, we determine that different hidden layers make different contributions to model robustness and clean accuracy, while shallow layers are comparatively more critical than deep layers. Moreover, our framework can be easily combined with other adversarial training methods to further improve model robustness by exploiting the potential of hidden layers. Extensive experiments on MNIST, CIFAR-10, CIFAR-10-C, CIFAR-10-P, and ImageNet demonstrate that ANP enables the strong robustness for deep models against both adversarial and corrupted ones, and also significantly outperforms various adversarial defense methods.",
      "fieldsOfStudy": [
        "Computer Science",
        "Medicine"
      ],
      "topics": [
        "deep layers",
        "hidden layers",
        "model robustness",
        "different hidden layers",
        "deep models",
        "adversarial robustness",
        "shallow layers",
        "adversarial defense methods",
        "adversarial training methods",
        "adversarial examples",
        "types",
        "deep neural networks",
        "noise",
        "Adversarial Noise Propagation"
      ]
    }
  },
  {
    "sim": 0.5698331289282601,
    "gen": {
      "title": "A unified framework to generate optimized compact finite difference schemes",
      "url": "https://www.semanticscholar.org/paper/1cb9154079940a56170efebbc7caf0bd8e5d6095",
      "abstract": null,
      "fieldsOfStudy": [
        "Computer Science",
        "Mathematics",
        "Physics"
      ]
    },
    "org": {
      "title": "Fast logarithmic Fourier-Laplace transform of nonintegrable functions.",
      "url": "https://www.semanticscholar.org/paper/f446d7365856b29bc13e60d4facc71cf3efaf592",
      "abstract": "We present an efficient and very flexible numerical fast Fourier-Laplace transform that extends the logarithmic Fourier transform introduced by Haines and Jones [Geophys. J. Int. 92, 171 (1988)GJINEA0956-540X10.1111/j.1365-246X.1988.tb01131.x] for functions varying over many scales to nonintegrable functions. In particular, these include cases of the asymptotic form f(\u03bd\u21920)\u223c\u03bd^{a} and f(|\u03bd|\u2192\u221e)\u223c\u03bd^{b} with arbitrary real a>b. Furthermore, we prove that the numerical transform converges exponentially fast in the number of data points, provided that the function is analytic in a cone |Im\u03bd|<\u03b8|Re\u03bd| with a finite opening angle \u03b8 around the real axis and satisfies |f(\u03bd)f(1/\u03bd)|<\u03bd^{c} as \u03bd\u21920 with a positive constant c, which is the case for the class of functions with power-law tails. Based on these properties we derive ideal transformation parameters and discuss how the logarithmic Fourier transform can be applied to convolutions. The ability of the logarithmic Fourier transform to perform these operations on multiscale (nonintegrable) functions with power-law tails with exponentially small errors makes it the method of choice for many physical applications, which we demonstrate on typical examples. These include benchmarks against known analytical results inaccessible to other numerical methods, as well as physical models near criticality.",
      "fieldsOfStudy": [
        "Mathematics",
        "Computer Science",
        "Physics",
        "Medicine"
      ],
      "topics": [
        "nonintegrable functions",
        "physical applications",
        "functions",
        "typical examples",
        "numerical methods",
        "scales",
        "physical models",
        "criticality",
        "power-law tails",
        "b.",
        "choice",
        "arbitrary real",
        "cases",
        "data points",
        "ideal transformation parameters",
        "arbitrary real a",
        "logarithmic Fourier transform"
      ]
    }
  },
  {
    "sim": 0.5804077281571743,
    "gen": {
      "title": "GRACE: Generating Concise and Informative Contrastive Sample to Explain Neural Network Model's Prediction",
      "url": "https://www.semanticscholar.org/paper/07c161c33b0f7663b6458d52b1a2a061c00c8e87",
      "abstract": "Despite the recent development in the topic of explainable AI/ML for image and text data, the majority of current solutions are not suitable to explain the prediction of neural network models when the datasets are tabular and their features are in high-dimensional vectorized formats. To mitigate this limitation, therefore, we borrow two notable ideas (i.e., \"explanation by intervention\" from causality and \"explanation are contrastive\" from philosophy) and propose a novel solution, named as GRACE, that better explains neural network models' predictions for tabular datasets. In particular, given a model's prediction as label X, GRACE intervenes and generates a minimally-modified contrastive sample to be classified as Y, with an intuitive textual explanation, answering the question of \"Why X rather than Y?\" We carry out comprehensive experiments using eleven public datasets of different scales and domains (e.g., # of features ranges from 5 to 216) and compare GRACE with competing baselines on different measures: fidelity, conciseness, info-gain, and influence. The user-studies show that our generated explanation is not only more intuitive and easy-to-understand but also facilitates end-users to make as much as 60% more accurate post-explanation decisions than that of Lime.",
      "fieldsOfStudy": [
        "Computer Science"
      ],
      "topics": [
        "neural network models",
        "tabular datasets",
        "different measures",
        "current solutions",
        "different scales",
        "GRACE intervenes",
        "influence",
        "Lime",
        "competing baselines",
        "intuitive textual explanation",
        "neural network models predictions",
        "features",
        "GRACE",
        "high-dimensional vectorized formats",
        "explanation",
        "label X"
      ]
    },
    "org": {
      "title": "Multi-Scale Convolutional Neural Networks for Time Series Classification",
      "url": "https://www.semanticscholar.org/paper/9e8cce4d2d0bc575c6a24e65398b43bf56ac150a",
      "abstract": "Time series classification (TSC), the problem of predicting class labels of time series, has been around for decades within the community of data mining and machine learning, and found many important applications such as biomedical engineering and clinical prediction. However, it still remains challenging and falls short of classification accuracy and efficiency. Traditional approaches typically involve extracting discriminative features from the original time series using dynamic time warping (DTW) or shapelet transformation, based on which an off-the-shelf classifier can be applied. These methods are ad-hoc and separate the feature extraction part with the classification part, which limits their accuracy performance. Plus, most existing methods fail to take into account the fact that time series often have features at different time scales. To address these problems, we propose a novel end-to-end neural network model, Multi-Scale Convolutional Neural Networks (MCNN), which incorporates feature extraction and classification in a single framework. Leveraging a novel multi-branch layer and learnable convolutional layers, MCNN automatically extracts features at different scales and frequencies, leading to superior feature representation. MCNN is also computationally efficient, as it naturally leverages GPU computing. We conduct comprehensive empirical evaluation with various existing methods on a large number of benchmark datasets, and show that MCNN advances the state-of-the-art by achieving superior accuracy performance than other leading methods.",
      "fieldsOfStudy": [
        "Computer Science"
      ],
      "topics": [
        "superior feature representation",
        "leading methods",
        "superior accuracy performance",
        "feature extraction",
        "time series",
        "discriminative features",
        "features",
        "classification accuracy",
        "Time series classification",
        "existing methods",
        "Multi-Scale Convolutional Neural Networks",
        "different scales",
        "important applications",
        "classification",
        "different time scales",
        "dynamic time warping",
        "clinical prediction"
      ]
    }
  },
  {
    "sim": 0.33979338843349827,
    "gen": {
      "title": "On the Complexity of Motion Planning for Multiple Independent Objects; PSPACE- Hardness of the \"Warehouseman's Problem\"",
      "url": "https://www.semanticscholar.org/paper/0ee77f9de17ea20a54ffb0f5dcb5030de8a5eecf",
      "abstract": "Coordinated motion planning for a large number af three-di mensional objects in the presence of obstacles is a computa tional problem whose complexity is important to calibrate. In this paper we show that even the restricted two-dimensional problem for arbitrarily many rectangles in a rectangular region is PSPACE-hard. This result should be viewed as a guide to the difficulty, of the general problem and should lead researchers to consider more tractable restricted classes of motion problems of practical interest.",
      "fieldsOfStudy": [
        "Mathematics"
      ],
      "topics": [
        "motion problems",
        "practical interest",
        "Coordinated motion planning",
        "computa tional problem",
        "PSPACE",
        "obstacles",
        "researchers",
        "general problem",
        "tractable restricted classes",
        "rectangular region",
        "arbitrarily many rectangles",
        "three-di mensional objects",
        "large number",
        "restricted two-dimensional problem",
        "complexity",
        "presence"
      ]
    },
    "org": {
      "title": "Network equivalence in the presence of an eavesdropper",
      "url": "https://www.semanticscholar.org/paper/e0411907c62a7974df8305f642d5466b778b1bf9",
      "abstract": "We consider networks of noisy degraded wiretap channels in the presence of an eavesdropper. For the case where the eavesdropper can wiretap at most one channel at a time, we show that the secrecy capacity region, for a broad class of channels and any given network topology and communication demands, is equivalent to that of a corresponding network where each noisy wiretap channel is replaced by a noiseless wiretap channel. Thus in this case there is a separation between wiretap channel coding on each channel and secure network coding on the resulting noiseless network. We show with an example that such separation does not hold when the eavesdropper can access multiple channels at the same time, for which case we provide upper and lower bounding noiseless networks.",
      "fieldsOfStudy": [
        "Computer Science",
        "Mathematics"
      ],
      "topics": [
        "noiseless networks",
        "multiple channels",
        "channels",
        "networks",
        "noiseless wiretap channel",
        "noisy degraded wiretap channels",
        "resulting noiseless network",
        "corresponding network",
        "given network topology and communication demands",
        "channel",
        "separation",
        "channel",
        "time",
        "secrecy capacity region",
        "wiretap channel coding",
        "secure network coding",
        "noisy wiretap channel",
        "communication demands",
        "given network topology",
        "upper and lower bounding noiseless networks"
      ]
    }
  },
  {
    "sim": 0.6051024454067191,
    "gen": {
      "title": "Performance Modeling and Analysis of a Hyperledger-based System Using GSPN",
      "url": "https://www.semanticscholar.org/paper/6d1212f2adcc5d43967ab686264907e477b84eed",
      "abstract": null,
      "fieldsOfStudy": [
        "Computer Science"
      ]
    },
    "org": {
      "title": "Promoting Distributed Trust in Machine Learning and Computational Simulation",
      "url": "https://www.semanticscholar.org/paper/c1df27e39b822a20846453897b1eeeeeeaf887ca",
      "abstract": "Policy decisions are increasingly dependent on the outcomes of simulations and/or machine learning models. The ability to share and interact with these outcomes is relevant across multiple fields and is especially critical in the disease modeling community where models are often only accessible and workable to the researchers that generate them. This work presents a blockchain-enabled system that establishes a decentralized trust between parties involved in a modeling process. Utilizing the OpenMalaria framework, we demonstrate the ability to store, share and maintain auditable logs and records of each step in the simulation process, showing how to validate results generated by computational workers. We also show how the system monitors worker outputs to rank and identify faulty workers via comparison to nearest neighbors or historical reward spaces as a means of ensuring model quality.",
      "fieldsOfStudy": [
        "Computer Science"
      ],
      "topics": [
        "computational workers",
        "faulty workers",
        "worker outputs",
        "model quality",
        "historical reward spaces",
        "nearest neighbors",
        "models",
        "comparison",
        "simulations and/or machine learning models",
        "results",
        "auditable logs",
        "multiple fields",
        "community",
        "simulation process",
        "modeling process",
        "machine learning models",
        "simulations",
        "disease modeling community"
      ]
    }
  },
  null,
  null,
  {
    "sim": 0.4267161745182013,
    "gen": {
      "title": "Consensus of Multiagent Systems and Synchronization of Complex Networks: A Unified Viewpoint",
      "url": "https://www.semanticscholar.org/paper/c7f671d7c047f77aa68258486c3edd535bb0b0b2",
      "abstract": "This paper addresses the consensus problem of multiagent systems with a time-invariant communication topology consisting of general linear node dynamics. A distributed observer-type consensus protocol based on relative output measurements is proposed. A new framework is introduced to address in a unified way the consensus of multiagent systems and the synchronization of complex networks. Under this framework, the consensus of multiagent systems with a communication topology having a spanning tree can be cast into the stability of a set of matrices of the same low dimension. The notion of consensus region is then introduced and analyzed. It is shown that there exists an observer-type protocol solving the consensus problem and meanwhile yielding an unbounded consensus region if and only if each agent is both stabilizable and detectable. A multistep consensus protocol design procedure is further presented. The consensus with respect to a time-varying state and the robustness of the consensus protocol to external disturbances are finally discussed. The effectiveness of the theoretical results is demonstrated through numerical simulations, with an application to low-Earth-orbit satellite formation flying.",
      "fieldsOfStudy": [
        "Computer Science",
        "Mathematics"
      ],
      "topics": [
        "consensus region",
        "general linear node dynamics",
        "node dynamics",
        "multiagent systems",
        "complex networks",
        "relative output measurements",
        "external disturbances",
        "numerical simulations",
        "low dimension",
        "A multistep consensus protocol design procedure",
        "unbounded consensus region",
        "low-Earth-orbit satellite formation",
        "matrices",
        "consensus problem",
        "low-Earth-orbit satellite formation flying",
        "A distributed observer-type consensus protocol"
      ]
    },
    "org": {
      "title": "Coherent-classical estimation versus purely-classical estimation for linear quantum systems",
      "url": "https://www.semanticscholar.org/paper/9d2decc230e90bbe21ce9b7ba24018bac60c53c0",
      "abstract": "We consider a coherent-classical estimation scheme for a class of linear quantum systems. It comprises an estimator that is a mixed quantum-classical system without involving coherent feedback. The estimator yields a classical estimate of a variable for the quantum plant. We demonstrate that for a passive plant that can be characterized by annihilation operators only, such coherent-classical estimation provides no improvement over purely-classical estimation. An example is also given which shows that if the plant is not assumed to be an annihilation operator only quantum system, it is possible to get better estimates with such coherent-classical estimation compared with purely-classical estimation.",
      "fieldsOfStudy": [
        "Physics",
        "Computer Science",
        "Mathematics"
      ],
      "topics": [
        "linear quantum systems",
        "coherent feedback",
        "better estimates",
        "coherent-classical estimation scheme",
        "classical estimate",
        "mixed quantum-classical system",
        "linear",
        "quantum system",
        "purely-classical estimation",
        "quantum plant",
        "annihilation operator",
        "passive plant",
        "plant",
        "annihilation operator only quantum system",
        "annihilation operators",
        "class",
        "improvement"
      ]
    }
  },
  null,
  {
    "sim": 0.2670998367384899,
    "gen": {
      "title": "Quantifying the Alignment of Graph and Features in Deep Learning",
      "url": "https://www.semanticscholar.org/paper/b0797def844c727f93b9927b3982c703088d6513",
      "abstract": "We show that the classification performance of graph convolutional networks (GCNs) is related to the alignment between features, graph, and ground truth, which we quantify using a subspace alignment measure (SAM) corresponding to the Frobenius norm of the matrix of pairwise chordal distances between three subspaces associated with features, graph, and ground truth. The proposed measure is based on the principal angles between subspaces and has both spectral and geometrical interpretations. We showcase the relationship between the SAM and the classification performance through the study of limiting cases of GCNs and systematic randomizations of both features and graph structure applied to a constructive example and several examples of citation networks of different origins. The analysis also reveals the relative importance of the graph and features for classification purposes.",
      "fieldsOfStudy": [
        "Computer Science",
        "Medicine",
        "Physics",
        "Mathematics"
      ],
      "topics": [
        "graph convolutional networks",
        "graph structure",
        "graph",
        "ground truth",
        "features",
        "subspaces",
        "examples",
        "different origins",
        "citation networks",
        "pairwise chordal distances",
        "classification purposes",
        "subspace alignment measure",
        "systematic randomizations",
        "GCNs",
        "geometrical",
        "limiting cases",
        "SAM"
      ]
    },
    "org": {
      "title": "Efficient Space Skipping and Adaptive Sampling of Unstructured Volumes Using Hardware Accelerated Ray Tracing",
      "url": "https://www.semanticscholar.org/paper/860ef28bdce47806daa66701fef60ac87306f8d1",
      "abstract": "Sample based ray marching is an effective method for direct volume rendering of unstructured meshes. However, sampling such meshes remains expensive, and strategies to reduce the number of samples taken have received relatively little attention. In this paper, we introduce a method for rendering unstructured meshes using a combination of a coarse spatial acceleration structure and hardware-accelerated ray tracing. Our approach enables efficient empty space skipping and adaptive sampling of unstructured meshes, and outperforms a reference ray marcher by up to 7\u00d7.",
      "fieldsOfStudy": [
        "Computer Science"
      ],
      "topics": [
        "unstructured meshes",
        "Sample based ray marching",
        "meshes",
        "direct volume rendering",
        "7\u00d7.",
        "efficient empty space skipping",
        "adaptive sampling",
        "coarse spatial acceleration structure",
        "samples",
        "reference ray",
        "strategies",
        "relatively little attention",
        "effective method",
        "method",
        "combination",
        "hardware-accelerated ray tracing",
        "reference ray marcher"
      ]
    }
  },
  {
    "sim": 0.49238305967449447,
    "gen": {
      "title": "A Private and Efficient Mechanism for Data Uploading in Smart Cyber-Physical Systems",
      "url": "https://www.semanticscholar.org/paper/516415a995fd1514abdc246549871e00e93f8601",
      "abstract": "To provide fine-grained access to different dimensions of the physical world, the data uploading in smart cyber-physical systems suffers novel challenges on both energy conservation and privacy preservation. It is always critical for participants to consume as little energy as possible for data uploading. However, simply pursuing energy efficiency may lead to extreme disclosure of private information, especially when the uploaded contents from participants are more informative than ever. In this article, we propose a novel mechanism for data uploading in smart cyber-physical systems, which considers both energy conservation and privacy preservation. The mechanism preserves privacy by concealing abnormal behaviors of participants, while still achieves an energy-efficient scheme for data uploading by introducing an acceptable number of extra contents. To derive an optimal uploading scheme is proved to be NP-hard. Accordingly, we propose a heuristic algorithm and analyze its effectiveness. The evaluation results towards a real-world dataset demonstrate that the performance of the proposed algorithm is comparable with the optimal results.",
      "fieldsOfStudy": [
        "Computer Science"
      ],
      "topics": [
        "little energy",
        "extra contents",
        "data uploading",
        "energy efficiency",
        "privacy preservation",
        "participants",
        "data",
        "privacy",
        "abnormal behaviors",
        "novel challenges",
        "private information",
        "smart cyber-physical systems",
        "extreme disclosure",
        "energy conservation",
        "NP",
        "optimal uploading scheme"
      ]
    },
    "org": {
      "title": "A Practical Distributed Universal Construction with Unknown Participants",
      "url": "https://www.semanticscholar.org/paper/0470ef414a9ae1d8593e0d9f5270ce039c088238",
      "abstract": null,
      "fieldsOfStudy": [
        "Computer Science"
      ]
    }
  },
  null,
  {
    "sim": 0.5131938178835234,
    "gen": {
      "title": "Rejoinder: Learning Optimal Distributionally Robust Individualized Treatment Rules",
      "url": "https://www.semanticscholar.org/paper/bbb503fe1aefc7e051dd3ff0e23e3f1528f492aa",
      "abstract": "We thank the opportunity offered by editors for this discussion and the discussants for their insightful comments and thoughtful contributions. We also want to congratulate Kallus (2020) for his inspiring work in improving the effciency of policy learning by retargeting. Motivated from the discussion in Dukes and Vansteelandt (2020), we first point out interesting connections and distinctions between our work and Kallus (2020) in Section 1. In particular, the assumptions and sources of variation for consideration in these two papers lead to different research problems with different scopes and focuses. In Section 2, following the discussions in Li et al. (2020); Liang and Zhao (2020), we also consider the efficient policy evaluation problem when we have some data from the testing distribution available at the training stage. We show that under the assumption that the sample sizes from training and testing are growing in the same order, efficient value function estimates can deliver competitive performance. We further show some connections of these estimates with existing literature. However, when the growth of testing sample size available for training is in a slower order, efficient value function estimates may not perform well anymore. In contrast, the requirement of the testing sample size for DRITR is not as strong as that of efficient policy evaluation using the combined data. Finally, we highlight the general applicability and usefulness of DRITR in Section 3.",
      "fieldsOfStudy": [
        "Computer Science",
        "Mathematics",
        "Medicine"
      ],
      "topics": [
        "efficient value function estimates",
        "different research problems",
        "competitive performance",
        "policy learning",
        "different scopes",
        "efficient policy evaluation problem",
        "thoughtful contributions",
        "testing",
        "training",
        "existing literature",
        "focuses",
        "testing sample size",
        "Section",
        "interesting connections"
      ]
    },
    "org": {
      "title": "On the Limitation of MagNet Defense Against L1-Based Adversarial Examples",
      "url": "https://www.semanticscholar.org/paper/b11bc1d3629cfdefec1e523d7c10ef9764889393",
      "abstract": "In recent years, defending adversarial perturbations to natural examples in order to build robust machine learning models trained by deep neural networks (DNNs) has become an emerging research field in the conjunction of deep learning and security. In particular, MagNet consisting of an adversary detector and a data reformer is by far one of the strongest defenses in the black-box oblivious attack setting, where the attacker aims to craft transferable adversarial examples from an undefended DNN model to bypass an unknown defense module deployed on the same DNN model. Under this setting, MagNet can successfully defend a variety of attacks in DNNs, including the high-confidence adversarial examples generated by the Carlini and Wagner's attack based on the L2 distortion metric. However, in this paper, under the same attack setting we show that adversarial examples crafted based on the L1 distortion metric can easily bypass MagNet and mislead the target DNN image classifiers on MNIST and CIFAR-10. We also provide explanations on why the considered approach can yield adversarial examples with superior attack performance and conduct extensive experiments on variants of MagNet to verify its lack of robustness to L1 distortion based attacks. Notably, our results substantially weaken the assumption of effective threat models on MagNet that require knowing the deployed defense technique when attacking DNNs (i.e., the gray-box attack setting).",
      "fieldsOfStudy": [
        "Computer Science",
        "Mathematics"
      ],
      "topics": [
        "L1 distortion based attacks",
        "adversarial examples",
        "transferable adversarial examples",
        "attacks",
        "superior attack performance",
        "robust machine learning models",
        "deep learning",
        "deep neural networks",
        "natural examples",
        "adversarial perturbations",
        "MagNet",
        "effective threat models",
        "CIFAR-10",
        "DNNs",
        "L1",
        "attack setting"
      ]
    }
  },
  null,
  {
    "sim": 0.5896836769010336,
    "gen": {
      "title": "Deep Learning for Stock Selection Based on High Frequency Price-Volume Data",
      "url": "https://www.semanticscholar.org/paper/70d80011aff52a66095d0acb0af7e30cb80fa26b",
      "abstract": "Training a practical and effective model for stock selection has been a greatly concerned problem in the field of artificial intelligence. Even though some of the models from previous works have achieved good performance in the U.S. market by using low-frequency data and features, training a suitable model with high-frequency stock data is still a problem worth exploring. Based on the high-frequency price data of the past several days, we construct two separate models-Convolution Neural Network and Long Short-Term Memory-which can predict the expected return rate of stocks on the current day, and select the stocks with the highest expected yield at the opening to maximize the total return. In our CNN model, we propose improvements on the CNNpred model presented by E. Hoseinzade and S. Haratizadeh in their paper which deals with low-frequency features. Such improvements enable our CNN model to exploit the convolution layer's ability to extract high-level factors and avoid excessive loss of original information at the same time. Our LSTM model utilizes Recurrent Neural Network'advantages in handling time series data. Despite considerable transaction fees due to the daily changes of our stock position, annualized net rate of return is 62.27% for our CNN model, and 50.31% for our LSTM model.",
      "fieldsOfStudy": [
        "Mathematics",
        "Computer Science",
        "Economics"
      ],
      "topics": [
        "time series data",
        "stock selection",
        "high-frequency stock data",
        "stocks",
        "return",
        "artificial intelligence",
        "annualized net rate",
        "low-frequency data",
        "Recurrent Neural Networkadvantages",
        "Recurrent Neural",
        "features",
        "expected return rate",
        "original information",
        "CNN model",
        "suitable model",
        "Convolution Neural Network"
      ]
    },
    "org": {
      "title": "Structural Clustering of Volatility Regimes for Dynamic Trading Strategies",
      "url": "https://www.semanticscholar.org/paper/85c8faa74a13db6fed6f60626629e5304b937547",
      "abstract": "ABSTRACT We develop a new method to find the number of volatility regimes in a nonstationary financial time series by applying unsupervised learning to its volatility structure. We use change point detection to partition a time series into locally stationary segments and then compute a distance matrix between segment distributions. The segments are clustered into a learned number of discrete volatility regimes via an optimization routine. Using this framework, we determine the volatility clustering structure for financial indices, large-cap equities, exchange-traded funds and currency pairs. Our method overcomes the rigid assumptions necessary to implement many parametric regime-switching models while effectively distilling a time series into several characteristic behaviours. Our results provide a significant simplification of these time series and a strong descriptive analysis of prior behaviours of volatility. Finally, we create and validate a dynamic trading strategy that learns the optimal match between the current distribution of a time series and its past regimes, thereby making online risk-avoidance decisions at present.",
      "fieldsOfStudy": [
        "Economics",
        "Computer Science"
      ],
      "topics": [
        "discrete volatility regimes",
        "volatility",
        "currency pairs",
        "characteristic behaviours",
        "financial indices",
        "prior behaviours",
        "segment distributions",
        "nonstationary financial time series",
        "present",
        "structure",
        "time series",
        "time series",
        "volatility structure",
        "unsupervised learning",
        "volatility clustering structure",
        "parametric regime-switching models"
      ]
    }
  },
  null,
  {
    "sim": 0.36123108575610674,
    "gen": {
      "title": "The influence of customer experience on customer loyalty for the mobile telecommunication services",
      "url": "https://www.semanticscholar.org/paper/9eabd6f48bbfe6dec0e0aa4d5bfef44af7025033",
      "abstract": "The term customer experience (CE) is used to describe an emerging trend where the sellers and buyers are connected through the past and current product and service experiences. Customer loyalty is an important tool for current and future businesses in telecommunication field. Customer loyalty can help to capture and develop future strategies. Moreover, the effect of customer satisfaction on customer loyalty is also very vital for the sustainability and stability of service provider survival in competitive markets. While customer loyalty and customer satisfaction have been examined widely in different research contexts, the issue of customer loyalty in the context of mobile telecommunication services currently represents a gap in the literature. Therefore, the objective of this study is to identify the influence of customer experience on customers loyalty in mobile telecommunication services. This research will gather survey data and applies structural equation modelling SEM to analyse the data. The findings will provide insights for mobile telecommunication service industries in developing strategies for improved implementation of mobile products and services as well as the design of marketing strategies along with improved business models.",
      "fieldsOfStudy": [
        "Business",
        "Computer Science"
      ],
      "topics": [
        "mobile telecommunication service industries",
        "service provider survival",
        "survey data",
        "structural equation",
        "services",
        "improved business models",
        "mobile products",
        "developing strategies",
        "future strategies",
        "telecommunication field",
        "marketing strategies",
        "customer experience",
        "customer loyalty",
        "structural equation modelling SEM",
        "improved implementation"
      ]
    },
    "org": {
      "title": "Fuzzy approach on modelling cyber attacks patterns on data transfer in industrial control systems",
      "url": "https://www.semanticscholar.org/paper/f6be96f678e3fdeb9155871e88f6ab3b9de0ac07",
      "abstract": "Cybersecurity of industrial control system is a very complex and challenging research topic, due to the integration of these systems in national critical infrastructures. The control systems are now interconnected in industrial networks and frequently to the Internet. In this context they are becoming targets of various cyber attacks conducted by malicious people such as hackers, script kiddies, industrial spies and even foreign armies and intelligence agencies. In this paper the authors propose a way to model the most frequent attacker profiles and to estimate the success rate of an attack conducted in given conditions. The authors use a fuzzy approach for generating attacker profiles based on attacker attributes such as knowledge, technical resources and motivation. The attack success rate is obtained by using another fuzzy inference system that analyzes the attacker profile and system intrinsic characteristics.",
      "fieldsOfStudy": [
        "Computer Science"
      ],
      "topics": [
        "industrial control system",
        "intelligence agencies",
        "national critical infrastructures",
        "industrial spies",
        "industrial networks",
        "motivation",
        "technical resources",
        "cyber attacks",
        "script kiddies",
        "intrinsic characteristics",
        "given conditions",
        "malicious people",
        "attacker profile",
        "fuzzy inference system",
        "system intrinsic characteristics",
        "attacker attributes"
      ]
    }
  },
  null,
  null,
  null,
  {
    "sim": 0.7036025014538838,
    "gen": {
      "title": "Learning to Control Highly Accelerated Ballistic Movements on Muscular Robots",
      "url": "https://www.semanticscholar.org/paper/51752a2bbd1ba3abd2818e12ca5139c6a661c7f8",
      "abstract": null,
      "fieldsOfStudy": [
        "Computer Science"
      ]
    },
    "org": {
      "title": "Simultaneous contact and aerodynamic force estimation (s-CAFE) for aerial robots",
      "url": "https://www.semanticscholar.org/paper/df56430e848790153c33c0861cde032f3aca39ae",
      "abstract": "In this article, we consider the problem of multirotor flying robots physically interacting with the environment under influence of wind. The results are the first algorithms for simultaneous online estimation of contact and aerodynamic wrenches acting on the robot based on real-world data, without the need for dedicated sensors. For this purpose, we investigated two model-based techniques for discriminating between aerodynamic and interaction forces. The first technique is based on aerodynamic and contact torque models, and uses the external force to estimate wind speed. Contacts are then detected based on the residual between estimated external torque and expected (modeled) aerodynamic torque. Upon detecting contact, wind speed is assumed to change very slowly. From the estimated interaction wrench, we are also able to determine the contact location. This is embedded into a particle filter framework to further improve contact location estimation. The second algorithm uses the propeller aerodynamic power and angular speed as measured by the speed controllers to obtain an estimate of the airspeed. An aerodynamics model is then used to determine the aerodynamic wrench. Both methods rely on accurate aerodynamics models. Therefore, we evaluate data-driven and physics-based models as well as offline system identification for flying robots. For obtaining ground-truth data, we performed autonomous flights in a 3D wind tunnel. Using this data, aerodynamic model selection, parameter identification, and discrimination between aerodynamic and contact forces could be performed. Finally, the developed methods could serve as useful estimators for interaction control schemes with simultaneous compensation of wind disturbances.",
      "fieldsOfStudy": [
        "Computer Science"
      ],
      "topics": [
        "wind speed",
        "aerodynamic wrenches",
        "wind disturbances",
        "contact location estimation",
        "wind",
        "aerodynamic power",
        "angular speed",
        "contact",
        "estimated external torque",
        "aerodynamic and contact torque models",
        "flying robots",
        "multirotor flying robots",
        "simultaneous online estimation",
        "dedicated sensors",
        "aerodynamic model selection",
        "aerodynamic and contact forces"
      ]
    }
  },
  null,
  {
    "sim": 0.7330488206357357,
    "gen": {
      "title": "Automated Mechanism Design: A New Application Area for Search Algorithms",
      "url": "https://www.semanticscholar.org/paper/9089fa01266f3362565ae522e17b54e2e7f00a6f",
      "abstract": null,
      "fieldsOfStudy": [
        "Computer Science"
      ]
    },
    "org": {
      "title": "A Sampling-Based Approach to Computing Equilibria in Succinct Extensive-Form Games",
      "url": "https://www.semanticscholar.org/paper/40570ef9dede55251d4c3a8de05da82a56fdc7b0",
      "abstract": "A central task of artificial intelligence is the design of artificial agents that act towards specified goals in partially observed environments. Since such environments frequently include interaction over time with other agents with their own goals, reasoning about such interaction relies on sequential game-theoretic models such as extensive-form games or some of their succinct representations such as multi-agent influence diagrams. The current algorithms for calculating equilibria either work with inefficient representations, possibly doubly exponential in the number of time steps, or place strong assumptions on the game structure. In this paper, we propose a sampling-based approach, which calculates extensive-form correlated equilibria with small representations without placing such strong assumptions. Thus, it is practical in situations where the previous approaches would fail. In addition, our algorithm allows control over characteristics of the target equilibrium, e.g., we can ask for an equilibrium with high social welfare. Our approach is based on a multiplicative-weight update algorithm analogous to AdaBoost, and Markov chain Monte Carlo sampling. We prove convergence guarantees and explore the utility of our approach on several moderately sized multi-player games.",
      "fieldsOfStudy": [
        "Computer Science"
      ],
      "topics": [
        "environments",
        "multi-agent influence diagrams",
        "strong assumptions",
        "small representations",
        "inefficient representations",
        "time steps",
        "high social welfare",
        "moderately sized multi-player games",
        "agents",
        "sequential game-theoretic models",
        "specified goals",
        "interaction",
        "artificial agents",
        "Markov chain Monte Carlo sampling",
        "equilibria"
      ]
    }
  },
  null,
  {
    "sim": 0.26760318898476554,
    "gen": {
      "title": "A Scalable and Privacy-Aware IoT Service for Live Video Analytics",
      "url": "https://www.semanticscholar.org/paper/6ef2054f7719070b92d24614b2861021f4af02d2",
      "abstract": "We present OpenFace, our new open-source face recognition system that approaches state-of-the-art accuracy. Integrating OpenFace with inter-frame tracking, we build RTFace, a mechanism for denaturing video streams that selectively blurs faces according to specified policies at full frame rates. This enables privacy management for live video analytics while providing a secure approach for handling retrospective policy exceptions. Finally, we present a scalable, privacy-aware architecture for large camera networks using RTFace.",
      "fieldsOfStudy": [
        "Computer Science"
      ],
      "topics": [
        "frame rates",
        "retrospective policy exceptions",
        "specified policies",
        "video streams",
        "live video analytics",
        "RTFace",
        "faces",
        "large camera networks",
        "inter-frame tracking",
        "privacy management",
        "new open-source face recognition system",
        "secure approach",
        "OpenFace",
        "mechanism",
        "scalable, privacy-aware architecture",
        "state-of-the-art accuracy"
      ]
    },
    "org": {
      "title": "Finding desirable objects under group categorical preferences",
      "url": "https://www.semanticscholar.org/paper/24b57d5817f6bf2c39261dceb68200fe021048de",
      "abstract": null,
      "fieldsOfStudy": [
        "Computer Science",
        "Mathematics"
      ]
    }
  },
  {
    "sim": 0.653863569475941,
    "gen": {
      "title": "Balanced Cooperative Target Search of Mobile Sensor Fleet under Localization Uncertainty",
      "url": "https://www.semanticscholar.org/paper/e8e48c2159472499c50e91543853acc17cf958ac",
      "abstract": "This paper deals with the problem of multiple sensors installed on mobile platforms cooperating to search for the target while each sensor is subject to localization uncertainty. The balance between a couple of conflicting objectives: i) suppressing the growth of self-localization uncertainty as an individual mobile agent; ii) reducing target uncertainty as a team, is sought under the information-theoretic framework. The conventional particle filter approach that concerns only the target state is augmented with self-localization by jointly estimating both. The self-localization is facilitated by introducing additional radio signals emitted from the origin, which should be a practical assumption considering limited resources and the restricted capability of small mobile sensors in case of a GNSS outage. The proposed method is featured with the numerical calculation of mutual information based on the principle of kernel conditional density estimation, and the unified comparison scheme between conflicting objectives. The numerical experiment shows that the proposed probing control law can automatically switch between target ascertaining and observability enhancing based on the mutual information-based utility function in a distributed fashion.",
      "fieldsOfStudy": null,
      "topics": [
        "target uncertainty",
        "small mobile sensors",
        "mobile platforms",
        "mutual information",
        "target",
        "conflicting objectives",
        "multiple sensors",
        "kernel conditional density estimation",
        "additional radio signals",
        "limited resources",
        "self-localization uncertainty",
        "individual mobile agent",
        "self-localization",
        "GNSS",
        "target ascertaining",
        "observability enhancing"
      ]
    },
    "org": {
      "title": "A Multi-Resolution Frontier-Based Planner for Autonomous 3D Exploration",
      "url": "https://www.semanticscholar.org/paper/954ccedc45c801490fb787c745b656b6cc42ae07",
      "abstract": "In this letter we propose a planner for 3D exploration that is suitable for applications using state-of-the-art 3D sensors, such as LiDARs, that produce large point clouds with each scan. The planner is based on the detection of a frontier - a boundary between the explored and the unknown part of the environment - and consists of the algorithm for detecting frontier points, followed by the clustering of frontier points and the selection of the best frontier point to be explored. Compared to existing frontier-based approaches, the planner is more scalable, i.e., it requires less time for the same environment size while ensuring similar exploration time. The performance is achieved by relying not on data obtained directly from the 3D sensor, but on data obtained by a mapping algorithm. In order to cluster the frontier points, we exploit the properties of the Octree environment representation, which allows easy analysis with different resolutions. The planner is tested in the simulation environment and in an outdoor test area with a UAV equipped with a LiDAR sensor. The results show the advantages of the approach compared to current state-of-the-art approaches.",
      "fieldsOfStudy": [
        "Computer Science"
      ],
      "topics": [
        "large point clouds",
        "similar exploration time",
        "3D exploration",
        "different resolutions",
        "time",
        "frontier points",
        "LiDARs",
        "easy analysis",
        "existing frontier-based approaches",
        "environment size",
        "Octree environment representation",
        "data",
        "LiDAR sensor",
        "3D sensor"
      ]
    }
  },
  {
    "sim": 0.3908693741069442,
    "gen": {
      "title": "Discovering Urban Functional Zones Using Latent Activity Trajectories",
      "url": "https://www.semanticscholar.org/paper/710b724423fa8a2df15067e1f188e97e4eda76fb",
      "abstract": "The step of urbanization and modern civilization fosters different functional zones in a city, such as residential areas, business districts, and educational areas. In a metropolis, people commute between these functional zones every day to engage in different socioeconomic activities, e.g., working, shopping, and entertaining. In this paper, we propose a data-driven framework to discover functional zones in a city. Specifically, we introduce the concept of latent activity trajectory (LAT), which captures socioeconomic activities conducted by citizens at different locations in a chronological order. Later, we segment an urban area into disjointed regions according to major roads, such as highways and urban expressways. We have developed a topic-modeling-based approach to cluster the segmented regions into functional zones leveraging mobility and location semantics mined from LAT. Furthermore, we identify the intensity of each functional zone using Kernel Density Estimation. Extensive experiments are conducted with several urban scale datasets to show that the proposed framework offers a powerful ability to capture city dynamics and provides valuable calibrations to urban planners in terms of functional zones.",
      "fieldsOfStudy": [
        "Computer Science"
      ],
      "topics": [
        "different functional zones",
        "functional zones",
        "educational areas",
        "residential areas",
        "socioeconomic activities",
        "urban expressways",
        "urban planners",
        "urban scale datasets",
        "different locations",
        "city dynamics",
        "Kernel Density Estimation",
        "latent activity trajectory",
        "disjointed regions",
        "business districts"
      ]
    },
    "org": {
      "title": "TSRuleGrowth : Extraction de r\u00e8gles de pr\u00e9diction semi-ordonn\u00e9es \u00e0 partir d'une s\u00e9rie temporelle d'\u00e9l\u00e9ments discrets, application dans un contexte d'intelligence ambiante",
      "url": "https://www.semanticscholar.org/paper/19fc752660dec329d4f0fbb6e76a52677b7990f2",
      "abstract": "This paper presents a new algorithm: TSRuleGrowth, looking for partially-ordered rules over a time series. This algorithm takes principles from the state of the art of rule mining and applies them to time series via a new notion of support. We apply this algorithm to real data from a connected environment, which extract user habits through different connected objects.",
      "fieldsOfStudy": [
        "Computer Science",
        "Philosophy"
      ],
      "topics": [
        "different connected objects",
        "user habits",
        "support",
        "rule mining",
        "real data",
        "time series",
        "connected environment",
        "new notion",
        "new algorithm",
        "principles",
        "partially-ordered rules",
        "algorithm",
        "art",
        "TSRuleGrowth"
      ]
    }
  },
  null,
  {
    "sim": 0.36847556538652415,
    "gen": {
      "title": "Psychosomatic Impact of Social Networking Sites on Society and its Subtle But Real Consequences",
      "url": "https://www.semanticscholar.org/paper/0cc33523bc125687612cf29361c20a2e76b67926",
      "abstract": null,
      "fieldsOfStudy": [
        "Sociology"
      ]
    },
    "org": {
      "title": "Social Media and Information Overload: Survey Results",
      "url": "https://www.semanticscholar.org/paper/1002ef7a675defeb6ac53ab77f081d5b6b0bf98a",
      "abstract": "A UK-based online questionnaire investigating aspects of usage of user-generated media (UGM), such as Facebook, LinkedIn and Twitter, attracted 587 participants. Results show a high degree of engagement with social networking media such as Facebook, and a significant engagement with other media such as professional media, microblogs and blogs. Participants who experience information overload are those who engage less frequently with the media, rather than those who have fewer posts to read. Professional users show different behaviours to social users. Microbloggers complain of information overload to the greatest extent. Two thirds of Twitter-users have felt that they receive too many posts, and over half of Twitter-users have felt the need for a tool to filter out the irrelevant posts. Generally speaking, participants express satisfaction with the media, though a significant minority express a range of concerns including information overload and privacy.",
      "fieldsOfStudy": [
        "Psychology",
        "Computer Science",
        "Physics"
      ],
      "topics": [
        "social networking media",
        "professional media",
        "social users",
        "media",
        "fewer posts",
        "Professional users",
        "information overload",
        "Twitter",
        "user-generated media",
        "participants",
        "half",
        "Facebook",
        "Twitter-users",
        "different behaviours",
        "irrelevant posts"
      ]
    }
  },
  {
    "sim": 0.3247706661118859,
    "gen": {
      "title": "AdaShift: Decorrelation and Convergence of Adaptive Learning Rate Methods",
      "url": "https://www.semanticscholar.org/paper/9f88722cbc4107e3c3d0e1c7934cc7f1d5ae4fdb",
      "abstract": "Adam is shown not being able to converge to the optimal solution in certain cases. Researchers recently propose several algorithms to avoid the issue of non-convergence of Adam, but their efficiency turns out to be unsatisfactory in practice. In this paper, we provide new insight into the non-convergence issue of Adam as well as other adaptive learning rate methods. We argue that there exists an inappropriate correlation between gradient $g_t$ and the second-moment term $v_t$ in Adam ($t$ is the timestep), which results in that a large gradient is likely to have small step size while a small gradient may have a large step size. We demonstrate that such biased step sizes are the fundamental cause of non-convergence of Adam, and we further prove that decorrelating $v_t$ and $g_t$ will lead to unbiased step size for each gradient, thus solving the non-convergence problem of Adam. Finally, we propose AdaShift, a novel adaptive learning rate method that decorrelates $v_t$ and $g_t$ by temporal shifting, i.e., using temporally shifted gradient $g_{t-n}$ to calculate $v_t$. The experiment results demonstrate that AdaShift is able to address the non-convergence issue of Adam, while still maintaining a competitive performance with Adam in terms of both training speed and generalization.",
      "fieldsOfStudy": [
        "Computer Science",
        "Mathematics"
      ],
      "topics": [
        "small step size",
        "unbiased step size",
        "Adam",
        "biased step sizes",
        "non-convergence issue",
        "non-convergence problem",
        "certain cases",
        "large step size",
        "temporal shifting",
        "generalization",
        "practice",
        "terms",
        "novel adaptive learning rate method",
        "convergence",
        "adaptive learning rate methods",
        "non",
        "g_t$",
        "v_t$."
      ]
    },
    "org": {
      "title": "SideLine: How Delay-Lines (May) Leak Secrets from your SoC",
      "url": "https://www.semanticscholar.org/paper/58fd250188a81f470595a3fac920d1fb46679a21",
      "abstract": null,
      "fieldsOfStudy": [
        "Computer Science"
      ]
    }
  },
  {
    "sim": 0.7212270841140461,
    "gen": {
      "title": "Incremental Algorithm for Association Rule Mining under Dynamic Threshold",
      "url": "https://www.semanticscholar.org/paper/ae63d51cdb52deb02f2a5367eeec145e7428f7ed",
      "abstract": "Data mining is essentially applied to discover new knowledge from a database through an iterative process. The mining process may be time consuming for massive datasets. A widely used method related to knowledge discovery domain refers to association rule mining (ARM) approach, despite its shortcomings in mining large databases. As such, several approaches have been prescribed to unravel knowledge. Most of the proposed algorithms addressed data incremental issues, especially when a hefty amount of data are added to the database after the latest mining process. Three basic manipulation operations performed in a database include add, delete, and update. Any method devised in light of data incremental issues is bound to embed these three operations. The changing threshold is a long-standing problem within the data mining field. Since decision making refers to an active process, the threshold is indeed changeable. Accordingly, the present study proposes an algorithm that resolves the issue of rescanning a database that had been mined previously and allows retrieval of knowledge that satisfies several thresholds without the need to learn the process from scratch. The proposed approach displayed high accuracy in experimentation, as well as reduction in processing time by almost two-thirds of the original mining execution time.",
      "fieldsOfStudy": [
        "Computer Science"
      ],
      "topics": [
        "massive datasets",
        "large databases",
        "Data mining",
        "processing time",
        "unravel knowledge",
        "knowledge discovery domain",
        "knowledge",
        "thresholds",
        "data incremental issues",
        "original mining execution time",
        "approaches",
        "The mining process",
        "update",
        "scratch",
        "data mining field",
        "rule mining"
      ]
    },
    "org": {
      "title": "Evaluation of Frequent Itemset Mining Platforms Using Apriori and FP-Growth Algorithm",
      "url": "https://www.semanticscholar.org/paper/a483a9dcf70e43981f5fa70b7f1a949b326f4bdb",
      "abstract": "With the overwhelming amount of complex and heterogeneous data pouring from any-where, any-time, and any-device, there is undeniably an era of Big Data. The emergence of the Big Data as a disruptive technology for next generation of intelligent systems, has brought many issues of how to extract and make use of the knowledge obtained from the data within short times, limited budget and under high rates of data generation. Companies are recognizing that big data can be used to make more accurate predictions, and can be used to enhance the business with the help of appropriate association rule mining algorithm. To help these organizations, with which software and algorithm is more appropriate for them depending on their dataset, we compared the most famous three MapReduce based software Hadoop, Spark, Flink on two widely used algorithms Apriori and Fp-Growth on different scales of dataset.",
      "fieldsOfStudy": [
        "Computer Science"
      ],
      "topics": [
        "appropriate association rule mining algorithm",
        "data generation",
        "generation",
        "issues",
        "intelligent systems",
        "short times",
        "Fp-Growth",
        "algorithm",
        "accurate predictions",
        "different scales",
        "dataset",
        "big data",
        "high rates",
        "limited budget",
        "MapReduce",
        "software",
        "Hadoop"
      ]
    }
  },
  null,
  {
    "sim": 0.3269885903040448,
    "gen": {
      "title": "Monte Carlo Tree Search with heuristic evaluations using implicit minimax backups",
      "url": "https://www.semanticscholar.org/paper/6c372fbbf228bb9146a50cfbda0bb9fbd760b8d6",
      "abstract": "Monte Carlo Tree Search (MCTS) has improved the performance of game engines in domains such as Go, Hex, and general game playing. MCTS has been shown to outperform classic \u03b1\u03b2 search in games where good heuristic evaluations are difficult to obtain. In recent years, combining ideas from traditional minimax search in MCTS has been shown to be advantageous in some domains, such as Lines of Action, Amazons, and Breakthrough. In this paper, we propose a new way to use heuristic evaluations to guide the MCTS search by storing the two sources of information, estimated win rates and heuristic evaluations, separately. Rather than using the heuristic evaluations to replace the playouts, our technique backs them up implicitly during the MCTS simulations. These minimax values are then used to guide future simulations. We show that using implicit minimax backups leads to stronger play performance in Kalah, Breakthrough, and Lines of Action.",
      "fieldsOfStudy": [
        "Computer Science",
        "Mathematics"
      ],
      "topics": [
        "Breakthrough",
        "Action",
        "good heuristic evaluations",
        "Amazons",
        "Lines",
        "general game playing",
        "Lines of Action",
        "game engines",
        "MCTS",
        "traditional minimax search",
        "games",
        "estimated win rates",
        "future simulations",
        "classic \u03b1\u03b2 search",
        "stronger play performance"
      ]
    },
    "org": {
      "title": "FRSign: A Large-Scale Traffic Light Dataset for Autonomous Trains",
      "url": "https://www.semanticscholar.org/paper/4eb25625ac45244342fd39ef34a98ff0370e54d3",
      "abstract": "In the realm of autonomous transportation, there have been many initiatives for open-sourcing self-driving cars datasets, but much less for alternative methods of transportation such as trains. In this paper, we aim to bridge the gap by introducing FRSign, a large-scale and accurate dataset for vision-based railway traffic light detection and recognition. Our recordings were made on selected running trains in France and benefited from carefully hand-labeled annotations. An illustrative dataset which corresponds to ten percent of the acquired data to date is published in open source with the paper. It contains more than 100,000 images illustrating six types of French railway traffic lights and their possible color combinations, together with the relevant information regarding their acquisition such as date, time, sensor parameters, and bounding boxes. This dataset is published in open-source at the address \\url{this https URL}. We compare, analyze various properties of the dataset and provide metrics to express its variability. We also discuss specific challenges and particularities related to autonomous trains in comparison to autonomous cars.",
      "fieldsOfStudy": [
        "Computer Science"
      ],
      "topics": [
        "bounding boxes",
        "French railway traffic lights",
        "autonomous cars",
        "autonomous transportation",
        "sensor parameters",
        "selected running trains",
        "open source",
        "alternative methods",
        "trains",
        "date",
        "vision-based railway traffic light detection",
        "transportation",
        "recognition",
        "initiatives",
        "self-driving cars datasets"
      ]
    }
  },
  null,
  null,
  null,
  {
    "sim": 0.6396941804070323,
    "gen": {
      "title": "Congestion Control for P2P Live Streaming",
      "url": "https://www.semanticscholar.org/paper/305b3bfbf76ad4c8bacee59109c2963d8e5f085b",
      "abstract": "In recent years, research efforts tried to exploit peer-to-peer (P2P) systems in order to provide Live Streaming (LS) and Video-on-Demand (VoD) services. Most of these research efforts focus on the development of distributed P2P block schedulers for content exchange among the participating peers and on the characteristics of the overlay graph (P2P overlay) that interconnects the set of these peers. Currently, researchers try to combine peer-to-peer systems with cloud infrastructures. They developed monitoring and control architectures that use resources from the cloud in order to enhance QoS and achieve an attractive trade-off between stability and low cost operation. However, there is a lack of research effort on the congestion control of these systems and the existing congestion control architectures are not suitable for P2P live streaming traffic (small sequential non persistent traffic towards multiple network locations). This paper proposes a P2P live streaming traffic aware congestion control protocol that: i) is capable to manage sequential traffic heading to multiple network destinations , ii) efficiently exploits the available bandwidth, iii) accurately measures the idle peer resources, iv) avoids network congestion, and v) is friendly to traditional TCP generated traffic. The proposed P2P congestion control has been implemented, tested and evaluated through a series of real experiments powered across the BonFIRE infrastructure.",
      "fieldsOfStudy": [
        "Computer Science"
      ],
      "topics": [
        "traffic aware congestion control protocol",
        "sequential traffic",
        "network congestion",
        "traditional TCP generated traffic",
        "streaming traffic",
        "peer",
        "multiple network destinations",
        "P2P overlay",
        "P2P congestion control",
        "low cost operation",
        "cloud infrastructures",
        "distributed P2P block schedulers",
        "P2P",
        "(small sequential non persistent traffic",
        "P2P live streaming traffic",
        "small sequential non persistent traffic",
        "P2P live streaming traffic aware congestion control protocol",
        "idle peer resources",
        "The proposed P2P congestion control"
      ]
    },
    "org": {
      "title": "Sorting Reordered Packets with Interrupt Coalescing",
      "url": "https://www.semanticscholar.org/paper/be6c3e8bdf25a7d157a8efe65de68e3870c557e9",
      "abstract": null,
      "fieldsOfStudy": [
        "Computer Science"
      ]
    }
  },
  {
    "sim": 0.40483609975629675,
    "gen": {
      "title": "A Tutorial on the Optimization of Amplify-and-Forward MIMO Relay Systems",
      "url": "https://www.semanticscholar.org/paper/603cf92ee0b25086bc814269b915ca0c66915952",
      "abstract": "The remarkable promise of multiple-input multiple-output (MIMO) wireless channels has motivated an intense research activity to characterize the theoretical and practical issues associated with the design of transmit (source) and receive (destination) processing matrices under different operating conditions. This activity was primarily focused on point-to-point (single-hop) communications but more recently there has been an extensive work on two-hop or multi-hop settings in which single or multiple relays are used to deliver the information from the source to the destination. The aim of this tutorial is to provide an up-to-date overview of the fundamental results and practical implementation issues in designing amplify-and-forward MIMO relay systems.",
      "fieldsOfStudy": [
        "Computer Science",
        "Mathematics"
      ],
      "topics": [
        "different operating conditions",
        "practical implementation issues",
        "single or multiple relays",
        "(destination) processing matrices",
        "point",
        "transmit (source",
        "intense research activity",
        "MIMO",
        "amplify-and-forward MIMO relay systems",
        "theoretical and practical issues",
        "date",
        "destination",
        "two-hop or multi-hop settings",
        "source",
        "extensive work",
        "single-hop",
        "transmit (source) and receive (destination) processing matrices",
        "multiple-input multiple-output (MIMO) wireless channels"
      ]
    },
    "org": {
      "title": "Robust Mission Design Through Evidence Theory and Multiagent Collaborative Search",
      "url": "https://www.semanticscholar.org/paper/9155431f138dd142326e161fcb34f78302c08dd1",
      "abstract": "Abstract: In this paper, the preliminary design of a space mission is approached by introducing uncertainties on the design parameters and formulating the resulting reliable design problem as a multiobjective optimization problem. Uncertainties are modelled through evidence theory and the belief, or credibility, that the successful achievement of mission goals is maximized along with the reliability of constraint satisfaction. The multiobjective optimization problem is solved through a novel algorithm based on the collaboration of a population of agents in search for the set of highly reliable solutions. Two typical problems in mission analysis are used to illustrate the proposed methodology.",
      "fieldsOfStudy": [
        "Medicine",
        "Computer Science",
        "Mathematics"
      ],
      "topics": [
        "constraint satisfaction",
        "mission goals",
        "mission analysis",
        "resulting reliable design problem",
        "search",
        "agents",
        "multiobjective optimization problem",
        "evidence theory",
        "highly reliable solutions",
        "uncertainties",
        "credibility",
        "proposed methodology",
        "design parameters"
      ]
    }
  },
  {
    "sim": 0.280578090099736,
    "gen": {
      "title": "Global Sparse Momentum SGD for Pruning Very Deep Neural Networks",
      "url": "https://www.semanticscholar.org/paper/fe091b4b01d69753837fc0eb41301e350e95bac6",
      "abstract": "Deep Neural Network (DNN) is powerful but computationally expensive and memory intensive, thus impeding its practical usage on resource-constrained front-end devices. DNN pruning is an approach for deep model compression, which aims at eliminating some parameters with tolerable performance degradation. In this paper, we propose a novel momentum-SGD-based optimization method to reduce the network complexity by on-the-fly pruning. Concretely, given a global compression ratio, we categorize all the parameters into two parts at each training iteration which are updated using different rules. In this way, we gradually zero out the redundant parameters, as we update them using only the ordinary weight decay but no gradients derived from the objective function. As a departure from prior methods that require heavy human works to tune the layer-wise sparsity ratios, prune by solving complicated non-differentiable problems or finetune the model after pruning, our method is characterized by 1) global compression that automatically finds the appropriate per-layer sparsity ratios; 2) end-to-end training; 3) no need for a time-consuming re-training process after pruning; and 4) superior capability to find better winning tickets which have won the initialization lottery.",
      "fieldsOfStudy": [
        "Computer Science",
        "Mathematics"
      ],
      "topics": [
        "deep model compression",
        "complicated non-differentiable problems",
        "DNN pruning",
        "tolerable performance degradation",
        "pruning",
        "end",
        "different rules",
        "prior methods",
        "heavy human works",
        "layer",
        "global compression ratio",
        "time-consuming re-training process",
        "tickets",
        "resource-constrained front-end devices",
        "layer-wise sparsity ratios",
        "better winning tickets",
        "training iteration"
      ]
    },
    "org": {
      "title": "Avoiding communication in primal and dual block coordinate descent methods",
      "url": "https://www.semanticscholar.org/paper/851df4e0716b2d53742aa3b45248cfcbe9735089",
      "abstract": "Primal and dual block coordinate descent methods are iterative methods for solving regularized and unregularized optimization problems. Distributed-memory parallel implementations of these methods ...",
      "fieldsOfStudy": [
        "Computer Science"
      ],
      "topics": [
        "iterative methods",
        "regularized and unregularized optimization problems",
        "Primal and dual block coordinate descent methods",
        "methods",
        "Distributed-memory parallel implementations"
      ]
    }
  },
  {
    "sim": 0.5574939271550392,
    "gen": {
      "title": "Quantifying Human Mobility Perturbation and Resilience in Natural Disasters",
      "url": "https://www.semanticscholar.org/paper/2372d55567f6f5d2573abaa2a80920fdb059e93d",
      "abstract": "Natural disasters exert a profound impact on the world population. In 2012, natural disasters affected 106 million people, forcing over 31.7 million people to leave their homes. Climate change has intensified natural disasters, resulting in more catastrophic events and making extreme weather more difficult to predict. Understanding and predicting human movements plays a critical role in disaster evacuation, response and relief. Researchers have developed different methodologies and applied several models to study human mobility patterns, including random walks, L\u00e9vy flight, and Brownian walks. However, the extent to which these models may apply to perturbed human mobility patterns during disasters and the associated implications for improving disaster evacuation, response and relief efforts is lacking. My PhD research aims to address the limitation in human mobility research and gain a ground truth understanding of human mobility patterns under the influence of natural disasters. The research contains three interdependent projects. In the first project, I developed a novel data collecting system. The system can be used to collect large scale data of human mobility from large online social networking platforms. By analyzing both the general characteristics of the collected data and conducting a case study in NYC, I confirmed that the data collecting system is a viable venue to collect empirical data for human mobility research. My second project examined human mobility patterns in NYC under the influence of Hurricane Sandy. Using the data collecting system developed in the first project, I collected 12 days of human mobility data from NYC. The data set contains movements during and several days after the strike of Hurricane Sandy. The results showed that human mobility was strongly perturbed",
      "fieldsOfStudy": [
        "Computer Science",
        "Geography"
      ],
      "topics": [
        "Brownian walks",
        "human mobility patterns",
        "human mobility",
        "human mobility research",
        "random walks",
        "human movements",
        "L\u00e9vy flight",
        "disaster evacuation",
        "natural disasters",
        "Brownian",
        "Hurricane Sandy",
        "large scale data",
        "empirical data",
        "perturbed human mobility patterns",
        "disasters",
        "large online social networking platforms"
      ]
    },
    "org": {
      "title": "Uncovering Spatiotemporal and Semantic Aspects of Tourists Mobility Using Social Sensing",
      "url": "https://www.semanticscholar.org/paper/a0ed63d86edbfc5c6ecbeabc7c0515372c3e6aea",
      "abstract": null,
      "fieldsOfStudy": [
        "Computer Science",
        "Business"
      ]
    }
  },
  null,
  {
    "sim": 0.4805114897196019,
    "gen": {
      "title": "Accelerating block coordinate descent for nonnegative tensor factorization",
      "url": "https://www.semanticscholar.org/paper/91ca757c848812458c648226beea93cb50e46f61",
      "abstract": "This paper is concerned with improving the empirical convergence speed of block\u2010coordinate descent algorithms for approximate nonnegative tensor factorization (NTF). We propose an extrapolation strategy in\u2010between block updates, referred to as heuristic extrapolation with restarts (HER). HER significantly accelerates the empirical convergence speed of most existing block\u2010coordinate algorithms for NTF, in particular for challenging computational scenarios, while requiring a negligible additional computational budget.",
      "fieldsOfStudy": [
        "Computer Science",
        "Mathematics"
      ],
      "topics": [
        "computational scenarios",
        "negligible additional computational budget",
        "NTF",
        "approximate nonnegative tensor factorization",
        "heuristic extrapolation",
        "extrapolation strategy in\u2010between block updates",
        "restarts",
        "existing block\u2010coordinate algorithms",
        "empirical convergence speed",
        "This paper",
        "challenging computational scenarios",
        "block\u2010coordinate descent algorithms",
        "block updates",
        "extrapolation strategy"
      ]
    },
    "org": {
      "title": "Optimal mini-batch and step sizes for SAGA",
      "url": "https://www.semanticscholar.org/paper/40078cf8fd68cbf7237467e44fa52d49a5eb8fd1",
      "abstract": "Recently it has been shown that the step sizes of a family of variance reduced gradient methods called the JacSketch methods depend on the expected smoothness constant. In particular, if this expected smoothness constant could be calculated a priori, then one could safely set much larger step sizes which would result in a much faster convergence rate. We fill in this gap, and provide simple closed form expressions for the expected smoothness constant and careful numerical experiments verifying these bounds. Using these bounds, and since the SAGA algorithm is part of this JacSketch family, we suggest a new standard practice for setting the step sizes and mini-batch size for SAGA that are competitive with a numerical grid search. Furthermore, we can now show that the total complexity of the SAGA algorithm decreases linearly in the mini-batch size up to a pre-defined value: the optimal mini-batch size. This is a rare result in the stochastic variance reduced literature, only previously shown for the Katyusha algorithm. Finally we conjecture that this is the case for many other stochastic variance reduced methods and that our bounds and analysis of the expected smoothness constant is key to extending these results.",
      "fieldsOfStudy": [
        "Mathematics",
        "Computer Science"
      ],
      "topics": [
        "stochastic variance reduced methods",
        "mini-batch size",
        "gradient methods",
        "variance",
        "larger step sizes",
        "SAGA",
        "JacSketch",
        "pre-defined value",
        "expected smoothness constant and careful numerical experiments",
        "stochastic variance reduced literature",
        "step sizes",
        "simple closed form expressions",
        "numerical grid search",
        "variance reduced gradient methods",
        "careful numerical experiments",
        "expected smoothness constant"
      ]
    }
  },
  null,
  {
    "sim": 0.5824042974711683,
    "gen": {
      "title": "NeVAE: A Deep Generative Model for Molecular Graphs",
      "url": "https://www.semanticscholar.org/paper/0c9fc038583b7a6b27772e808c798fcdc10778bc",
      "abstract": "Deep generative models have been praised for their ability to learn smooth latent representation of images, text, and audio, which can then be used to generate new, plausible data. However, current generative models are unable to work with molecular graphs due to their unique characteristics\u2014their underlying structure is not Euclidean or grid-like, they remain isomorphic under permutation of the nodes labels, and they come with a different number of nodes and edges. In this paper, we propose NeVAE, a novel variational autoencoder for molecular graphs, whose encoder and decoder are specially designed to account for the above properties by means of several technical innovations. In addition, by using masking, the decoder is able to guarantee a set of valid properties in the generated molecules. Experiments reveal that our model can discover plausible, diverse and novel molecules more effectively than several state of the art methods. Moreover, by utilizing Bayesian optimization over the continuous latent representation of molecules our model finds, we can also find molecules that maximize certain desirable properties more effectively than alternatives.",
      "fieldsOfStudy": [
        "Computer Science",
        "Physics",
        "Mathematics"
      ],
      "topics": [
        "technical innovations",
        "certain desirable properties",
        "valid properties",
        "nodes",
        "molecules",
        "state",
        "smooth latent representation",
        "molecular graphs",
        "edges",
        "current generative models",
        "alternatives",
        "permutation",
        "decoder",
        "generated molecules",
        "means"
      ]
    },
    "org": {
      "title": "On Spectral Graph Embedding: A Non-Backtracking Perspective and Graph Approximation",
      "url": "https://www.semanticscholar.org/paper/108b9b55f4c839f2c1094d3afb3d85c6f687efd1",
      "abstract": "Graph embedding has been proven to be efficient and effective in facilitating graph analysis. In this paper, we present a novel spectral framework called NOn-Backtracking Embedding (NOBE), which offers a new perspective that organizes graph data at a deep level by tracking the flow traversing on the edges with backtracking prohibited. Further, by analyzing the non-backtracking process, a technique called graph approximation is devised, which provides a channel to transform the spectral decomposition on an edge-to-edge matrix to that on a node-to-node matrix. Theoretical guarantees are provided by bounding the difference between the corresponding eigenvalues of the original graph and its graph approximation. Extensive experiments conducted on various real-world networks demonstrate the efficacy of our methods on both macroscopic and microscopic levels, including clustering and structural hole spanner detection.",
      "fieldsOfStudy": [
        "Computer Science"
      ],
      "topics": [
        "node",
        "graph data",
        "graph analysis",
        "structural hole spanner detection",
        "edge",
        "technique called graph approximation",
        "graph approximation",
        "original graph",
        "backtracking",
        "NOBE",
        "deep level",
        "real-world networks",
        "NOn-Backtracking Embedding",
        "novel spectral framework",
        "spectral decomposition",
        "Graph embedding",
        "clustering"
      ]
    }
  },
  null,
  {
    "sim": 0.371977682985286,
    "gen": {
      "title": "Inferring Hidden IoT Devices and User Interactions via Spatial-Temporal Traffic Fingerprinting",
      "url": "https://www.semanticscholar.org/paper/4bb10d276a339be5aa396610fbfe83c12ca615c0",
      "abstract": "With the popularization of Internet of Things (IoT) devices in smart home and industry fields, a huge number of IoT devices are connected to the Internet. However, what devices are connected to a network may not be known by the Internet Service Provider (ISP), since many IoT devices are placed within small networks (e.g., home networks) and are hidden behind network address translation (NAT). Without pinpointing IoT devices in a network, it is unlikely for the ISP to appropriately configure security policies and effectively manage the network. Additionally, inferring fine-grained user interactions of IoT devices is also an interesting yet unresolved problem. In this paper, we design an efficient and scalable system via spatial-temporal traffic fingerprinting from an ISP\u2019s perspective in consideration of practical issues like learning-testing asymmetry. Our system can accurately identify typical IoT devices in a network, with the additional capability of identifying what devices are hidden behind NAT and the number of each type of device that share the same IP address. Our system can also detect user interactions and meanwhile identify their (concurrent) number through a multi-output regression model. Through extensive evaluation, we demonstrate that the system can generally identify IoT devices with an F1-Score above 0.999, and estimate the number of the same type of IoT device behind NAT with an average error below 5%. By studying 29 user interactions of 7 devices, we show that our system is promising in detecting user interactions.",
      "fieldsOfStudy": [
        "Computer Science"
      ],
      "topics": [
        "IoT devices",
        "typical IoT devices",
        "network address translation",
        "devices",
        "small networks",
        "IoT",
        "NAT",
        "user interactions",
        "practical issues",
        "security policies",
        "Things (IoT) devices",
        "ISP",
        "multi-output regression model",
        "learning-testing asymmetry",
        "IP address",
        "7 devices"
      ]
    },
    "org": {
      "title": "A Synopses Data Engine for Interactive Extreme-Scale Analytics",
      "url": "https://www.semanticscholar.org/paper/824c47d385590823447d71b346c1a0e2c87128bc",
      "abstract": "We detail the novel architecture of a Synopses Data Engine (SDE) which combines the virtues of parallel processing and stream summarization towards interactive analytics at scale. Our SDE, built on top of Apache Flink, has a unique design that supports a very wide variety of synopses, allows for dynamically adding new functionality to it at runtime, and introduces a synopsis-as-a-service paradigm to enable various types of scalability.",
      "fieldsOfStudy": [
        "Computer Science"
      ],
      "topics": [
        "types",
        "scalability",
        "new functionality",
        "synopses",
        "interactive analytics",
        "scale",
        "parallel processing",
        "runtime",
        "Apache Flink",
        "summarization",
        "SDE",
        "Synopses Data Engine",
        "unique design",
        "wide variety",
        "stream summarization"
      ]
    }
  },
  {
    "sim": 0.4932341959496822,
    "gen": {
      "title": "Model Selection for Simulator-based Statistical Models: A Kernel Approach",
      "url": "https://www.semanticscholar.org/paper/4e0138804913788a89fa40649fb14a5c46b74a09",
      "abstract": "We propose a novel approach to model selection for simulator-based statistical models. The proposed approach defines a mixture of candidate models, and then iteratively updates the weight coefficients for those models as well as the parameters in each model simultaneously; this is done by recursively applying Bayes' rule, using the recently proposed kernel recursive ABC algorithm. The practical advantage of the method is that it can be used even when a modeler lacks appropriate prior knowledge about the parameters in each model. We demonstrate the effectiveness of the proposed approach with a number of experiments, including model selection for dynamical systems in ecology and epidemiology.",
      "fieldsOfStudy": [
        "Mathematics",
        "Computer Science"
      ],
      "topics": [
        "model selection",
        "candidate models",
        "recursive ABC algorithm",
        "simulator-based statistical models",
        "epidemiology",
        "dynamical systems",
        "prior knowledge",
        "Bayes",
        "model",
        "models",
        "ecology",
        "ABC",
        "Bayes rule",
        "experiments",
        "The proposed approach",
        "appropriate prior knowledge",
        "recently proposed kernel recursive ABC algorithm",
        "proposed approach"
      ]
    },
    "org": {
      "title": "Anomaly detection based on indicators aggregation",
      "url": "https://www.semanticscholar.org/paper/2f8906351a083845d0731e05d108fe5077eea834",
      "abstract": "Automatic anomaly detection is a major issue in various areas. Beyond mere detection, the identification of the source of the problem that produced the anomaly is also essential. This is particularly the case in aircraft engine health monitoring where detecting early signs of failure (anomalies) and helping the engine owner to implement efficiently the adapted maintenance operations (fixing the source of the anomaly) are of crucial importance to reduce the costs attached to unscheduled maintenance. This paper introduces a general methodology that aims at classifying monitoring signals into normal ones and several classes of abnormal ones. The main idea is to leverage expert knowledge by generating a very large number of binary indicators. Each indicator corresponds to a fully parametrized anomaly detector built from parametric anomaly scores designed by experts. A feature selection method is used to keep only the most discriminant indicators which are used at inputs of a Naive Bayes classifier. This give an interpretable classifier based on interpretable anomaly detectors whose parameters have been optimized indirectly by the selection process. The proposed methodology is evaluated on simulated data designed to reproduce some of the anomaly types observed in real world engines.",
      "fieldsOfStudy": [
        "Mathematics",
        "Computer Science"
      ],
      "topics": [
        "interpretable anomaly detectors",
        "parametric anomaly scores",
        "normal ones",
        "Automatic anomaly detection",
        "unscheduled maintenance",
        "classes",
        "real world engines",
        "binary indicators",
        "aircraft engine health monitoring",
        "crucial importance",
        "Naive Bayes",
        "expert knowledge",
        "areas",
        "anomalies",
        "anomaly types"
      ]
    }
  },
  {
    "sim": 0.45304015883474236,
    "gen": {
      "title": "The Power of Asymmetry in Binary Hashing",
      "url": "https://www.semanticscholar.org/paper/3904edd8da84d2cdb0bde76a16a8cc7dba81c495",
      "abstract": "When approximating binary similarity using the hamming distance between short binary hashes, we show that even if the similarity is symmetric, we can have shorter and more accurate hashes by using two distinct code maps. I.e. by approximating the similarity between x and x\u2032 as the hamming distance between f (x) and g(x\u2032), for two distinct binary codes f, g, rather than as the hamming distance between f (x) and f (x\u2032).",
      "fieldsOfStudy": [
        "Mathematics",
        "Computer Science"
      ],
      "topics": [
        "short binary hashes",
        "binary similarity",
        "distinct binary codes",
        "hamming distance",
        "distinct code maps",
        "shorter and more accurate hashes",
        "(x\u2032",
        "I.e.",
        "similarity",
        "f, g",
        "g(x\u2032"
      ]
    },
    "org": {
      "title": "Black-box Smoothing: A Provable Defense for Pretrained Classifiers",
      "url": "https://www.semanticscholar.org/paper/b1ef79a7bbe51a7f1f28e403c8bde35a3b54d985",
      "abstract": "We present a method for provably defending any pretrained image classifier against $\\ell_p$ adversarial attacks. By prepending a custom-trained denoiser to any off-the-shelf image classifier and using randomized smoothing, we effectively create a new classifier that is guaranteed to be $\\ell_p$-robust to adversarial examples, without modifying the pretrained classifier. The approach applies both to the case where we have full access to the pretrained classifier as well as the case where we only have query access. We refer to this defense as black-box smoothing, and we demonstrate its effectiveness through extensive experimentation on ImageNet and CIFAR-10. Finally, we use our method to provably defend the Azure, Google, AWS, and ClarifAI image classification APIs. Our code replicating all the experiments in the paper can be found at this https URL .",
      "fieldsOfStudy": [
        "Computer Science",
        "Mathematics"
      ],
      "topics": [
        "query access",
        "adversarial examples",
        "access",
        "ClarifAI image classification APIs",
        "pretrained image classifier",
        "pretrained classifier",
        "randomized smoothing",
        "CIFAR-10",
        "extensive experimentation",
        "new classifier",
        "$\\ell_p$ adversarial attacks",
        "ImageNet",
        "AWS",
        "black-box smoothing",
        "Google",
        "image classification APIs",
        "ClarifAI"
      ]
    }
  },
  {
    "sim": 0.4972999856543908,
    "gen": {
      "title": "An Efficient Algorithm for Object Detection in Thermal Images using Convolutional Neural Networks and Thermal Signature of the Objects",
      "url": "https://www.semanticscholar.org/paper/3a6569efb5e320a06283791943f5c7beb830e0c8",
      "abstract": "In recent years, thermal imagery techniques have gained even more relevance because of its application and uses in various domains starting from agriculture to electrical engineering and going all the way to military and surveillance to name a few. In this paper, the use of thermal imagery and deep learning techniques for object detection has been explored. The objects chosen are commonly available entities such as: cat, dog, man and a car. The dataset consists of thermal images and objects are detected on the basis of different heat signatures and further these images are used to train a Convolutional Neural Network (CNN) based model. Also, model training is done using idea of transfer learning and pre-trained models to evaluate and compare performance metrics against the exiting Keras based transfer learning algorithms which are utilized here. The best model achieved an average accuracy of 91.94%. The results were also verified against a test dataset.",
      "fieldsOfStudy": null,
      "topics": [
        "based model",
        "pre-trained models",
        "transfer learning",
        "deep learning techniques",
        "model training",
        "exiting Keras based transfer learning algorithms",
        "Convolutional Neural Network",
        "thermal imagery techniques",
        "different heat signatures",
        "performance metrics",
        "electrical engineering",
        "object detection",
        "thermal imagery",
        "domains",
        "objects",
        "surveillance"
      ]
    },
    "org": {
      "title": "Unsupervised Dictionary Learning for Anomaly Detection",
      "url": "https://www.semanticscholar.org/paper/16f6d3ee20a1422aee5871edc502be5449b1ce93",
      "abstract": "We investigate the possibilities of employing dictionary learning to address the requirements of most anomaly detection applications, such as absence of supervision, online formulations, low false positive rates. We present new results of our recent semi-supervised online algorithm, TODDLeR, on a anti-money laundering application. We also introduce a novel unsupervised method of using the performance of the learning algorithm as indication of the nature of the samples.",
      "fieldsOfStudy": [
        "Computer Science",
        "Mathematics"
      ],
      "topics": [
        "low false positive rates",
        "online formulations",
        "anomaly detection applications",
        "supervision",
        "anti-money laundering application",
        "recent semi-supervised online algorithm",
        "absence",
        "dictionary learning",
        "indication",
        "learning algorithm",
        "new results",
        "samples",
        "novel unsupervised method",
        "nature",
        "requirements",
        "TODDLeR"
      ]
    }
  },
  {
    "sim": 0.2534678688151022,
    "gen": {
      "title": "Collective Response of Human Populations to Large-Scale Emergencies",
      "url": "https://www.semanticscholar.org/paper/bfc23938b16cf2cf24a46357434d183b0303d096",
      "abstract": "Despite recent advances in uncovering the quantitative features of stationary human activity patterns, many applications, from pandemic prediction to emergency response, require an understanding of how these patterns change when the population encounters unfamiliar conditions. To explore societal response to external perturbations we identified real-time changes in communication and mobility patterns in the vicinity of eight emergencies, such as bomb attacks and earthquakes, comparing these with eight non-emergencies, like concerts and sporting events. We find that communication spikes accompanying emergencies are both spatially and temporally localized, but information about emergencies spreads globally, resulting in communication avalanches that engage in a significant manner the social network of eyewitnesses. These results offer a quantitative view of behavioral changes in human activity under extreme conditions, with potential long-term impact on emergency detection and response.",
      "fieldsOfStudy": [
        "Geography",
        "Computer Science",
        "Medicine",
        "Physics"
      ],
      "topics": [
        "emergency response",
        "emergency detection",
        "emergencies",
        "unfamiliar conditions",
        "stationary human activity patterns",
        "response",
        "extreme conditions",
        "societal response",
        "communication avalanches",
        "patterns",
        "events",
        "human activity",
        "eyewitnesses",
        "communication",
        "bomb attacks",
        "sporting events",
        "behavioral changes",
        "communication spikes",
        "pandemic prediction"
      ]
    },
    "org": {
      "title": "Peg-in-Hole Revisited: A Generic Force Model for Haptic Assembly",
      "url": "https://www.semanticscholar.org/paper/596e185143a842b67b0f424b717b5e704b11733e",
      "abstract": "The development of a generic and effective force model for semi-automatic or manual virtual assembly with haptic support is not a trivial task, esp. when the assembly constraints involve complex features of arbitrary shape. The primary challenge lies in a proper formulation of the guidance forces that effectively assist the user in his/her exploration of the virtual environment, from repulsing collisions to attracting proper contact. The secondary difficulty is that of efficient implementation that maintains the standard 1 kHz haptic refresh rate. We propose a purely geometric model for an artificial energy field that favors spatial relations leading to proper assembly, differentiated to obtain forces and torques in a 6 DOF motion. The energy function is expressed in terms of a convolution of shape-dependent affinity fields, precomputed offline separately for each object. We test the effectiveness of the method using familiar peg-in-hole examples. We show that the proposed technique unifies collision detection and precision assembly modes into a single interaction, and provides a generic and automatic constraint model for the so-called virtual fixtures, with no restrictive assumptions on the types of the involved assembly features.Copyright \u00a9 2014 by ASME",
      "fieldsOfStudy": [
        "Computer Science",
        "Physics"
      ],
      "topics": [
        "proper assembly",
        "arbitrary shape",
        "precision assembly",
        "proper contact",
        "complex features",
        "-automatic or manual virtual assembly",
        "haptic support",
        "forces",
        "shape-dependent affinity fields",
        "collision detection",
        "spatial relations",
        "involved assembly features",
        "collisions",
        "DOF",
        "assembly constraints",
        "precision assembly modes",
        "semi-automatic or manual virtual assembly",
        "torques"
      ]
    }
  },
  {
    "sim": 0.6371083623858926,
    "gen": {
      "title": "Efficient Processing of k Nearest Neighbor Joins using MapReduce",
      "url": "https://www.semanticscholar.org/paper/02837c4b66da1288e30e9f5012fbf7cb68e67ad5",
      "abstract": "k nearest neighbor join (kNN join), designed to find k nearest neighbors from a dataset S for every object in another dataset R, is a primitive operation widely adopted by many data mining applications. As a combination of the k nearest neighbor query and the join operation, kNN join is an expensive operation. Given the increasing volume of data, it is difficult to perform a kNN join on a centralized machine efficiently. In this paper, we investigate how to perform kNN join using MapReduce which is a well-accepted framework for data-intensive applications over clusters of computers. In brief, the mappers cluster objects into groups; the reducers perform the kNN join on each group of objects separately. We design an effective mapping mechanism that exploits pruning rules for distance filtering, and hence reduces both the shuffling and computational costs. To reduce the shuffling cost, we propose two approximate algorithms to minimize the number of replicas. Extensive experiments on our in-house cluster demonstrate that our proposed methods are efficient, robust and scalable.",
      "fieldsOfStudy": [
        "Computer Science"
      ],
      "topics": [
        "data mining applications",
        "clusters",
        "replicas",
        "objects",
        "computers",
        "k nearest neighbors",
        "groups",
        "data",
        "distance filtering",
        "pruning rules",
        "data-intensive applications",
        "mappers cluster objects",
        "join",
        "kNN",
        "primitive operation",
        "join operation",
        "kNN join"
      ]
    },
    "org": {
      "title": "Semantic and Influence aware k-Representative Queries over Social Streams",
      "url": "https://www.semanticscholar.org/paper/7e89c77c2e4f393cd71edd24e5238f451d9f522e",
      "abstract": "Massive volumes of data continuously generated on social platforms have become an important information source for users. A primary method to obtain fresh and valuable information from social streams is \\emph{social search}. Although there have been extensive studies on social search, existing methods only focus on the \\emph{relevance} of query results but ignore the \\emph{representativeness}. In this paper, we propose a novel Semantic and Influence aware $k$-Representative ($k$-SIR) query for social streams based on topic modeling. Specifically, we consider that both user queries and elements are represented as vectors in the topic space. A $k$-SIR query retrieves a set of $k$ elements with the maximum \\emph{representativeness} over the sliding window at query time w.r.t. the query vector. The representativeness of an element set comprises both semantic and influence scores computed by the topic model. Subsequently, we design two approximation algorithms, namely \\textsc{Multi-Topic ThresholdStream} (MTTS) and \\textsc{Multi-Topic ThresholdDescend} (MTTD), to process $k$-SIR queries in real-time. Both algorithms leverage the ranked lists maintained on each topic for $k$-SIR processing with theoretical guarantees. Extensive experiments on real-world datasets demonstrate the effectiveness of $k$-SIR query compared with existing methods as well as the efficiency and scalability of our proposed algorithms for $k$-SIR processing.",
      "fieldsOfStudy": [
        "Computer Science"
      ],
      "topics": [
        "query time w.r.t",
        "query results",
        "queries",
        "topic modeling",
        "$k$-SIR processing",
        "existing methods",
        "social streams",
        "theoretical guarantees",
        "social platforms",
        "\\emph{social search",
        "$k$-SIR query",
        "elements",
        "users",
        "query time",
        "social search",
        "-Topic ThresholdDescend",
        "vectors",
        "user queries",
        "topic space"
      ]
    }
  },
  null,
  {
    "sim": 0.3817169955980222,
    "gen": {
      "title": "On the parameterization of positive real sequences and MA parameter estimation",
      "url": "https://www.semanticscholar.org/paper/5c9d9572f44fc39f66e9ab405f0ee5789595c3d5",
      "abstract": "An algorithm for moving average (MA) parameter estimation was proposed by Stoica et al. (see ibid. vol.48, p.1999-2012, 2000). Its key step (covariance fitting) is a semidefinite programming (SDP) problem with two convex constraints: one reflecting the real positiveness of the desired covariance sequence and the other having a second-order cone form. We analyze two parameterizations of a positive real sequence and show that there is a one-to-one correspondence between them. We also show that the dual of the covariance fitting problem has a significantly smaller number of variables and, thus, a much reduced computational complexity. We discuss in detail the formulations that are best suited for the currently available semidefinite quadratic programming packages. Experimental results show that the execution times of the newly proposed algorithms scale well with the MA order, which are therefore convenient for large-order MA signals.",
      "fieldsOfStudy": [
        "Mathematics",
        "Computer Science"
      ],
      "topics": [
        "Stoica et al",
        "fitting problem",
        "large-order MA signals",
        "covariance",
        "parameter estimation",
        "MA",
        "desired covariance sequence",
        "variables",
        "second",
        "SDP",
        "second-order cone form",
        "positive real sequence",
        "Stoica",
        "MA order",
        "currently available semidefinite quadratic programming packages",
        "covariance fitting",
        "vol.48",
        "ibid",
        "covariance fitting problem",
        "al",
        "real positiveness"
      ]
    },
    "org": {
      "title": "Optical phase extraction algorithm based on the continuous wavelet and the Hilbert transforms",
      "url": "https://www.semanticscholar.org/paper/717d5eb0880469e70eba0960a062a095312de304",
      "abstract": "In this paper we present an algorithm for optical phase evaluation based on the wavelet transform technique. The main advantage of this method is that it requires only one fringe pattern. This algorithm is based on the use of a \uf070/2 phase shifted fringe pattern where it is calculated via the Hilbert transform. To test its validity, the algorithm was used to demodulate a simulated fringe pattern giving the phase distribution with a good accuracy.",
      "fieldsOfStudy": [
        "Computer Science",
        "Mathematics"
      ],
      "topics": [
        "fringe pattern",
        "optical phase evaluation",
        "wavelet transform technique",
        "Hilbert",
        "simulated fringe pattern",
        "Hilbert transform",
        "good accuracy",
        "phase distribution",
        "\uf070/2 phase",
        "\uf070/2",
        "fringe pattern",
        "The main advantage",
        "algorithm",
        "\uf070/2 phase shifted fringe pattern",
        "method",
        "use",
        "validity"
      ]
    }
  },
  {
    "sim": 0.3078192609093342,
    "gen": {
      "title": "Robust Attitude Tracking for Aerobatic Helicopters: A Geometric Approach",
      "url": "https://www.semanticscholar.org/paper/26dfd760d322a45e0ffa66e5639d9ce85f1363e9",
      "abstract": "This article highlights the significance of the rotor dynamics in control design for small-scale aerobatic helicopters and proposes two singularity-free robust attitude-tracking controllers based on the available states for feedback. The first employs the angular velocity and the flap angle states (a variable that is not easy to measure) and uses a backstepping technique to design a robust compensator (BRC) to actively suppress the disturbance induced tracking error. The second exploits the inherent damping present in the helicopter dynamics leading to a structure-preserving, passively robust controller (SPR), which is free of the flap angle feedback. The BRC controller is designed to be robust in the presence of two types of disturbance: structured and unstructured. The structured disturbance is due to the uncertainty in the rotor parameters, and the unstructured disturbance is modeled as exogenous torque acting on the fuselage. The performance of the controller is demonstrated in the presence of both types of disturbances through numerical simulations. In contrast, the SPR tracking controller is derived such that the tracking error dynamics inherits the natural damping characteristic of the helicopter. The SPR controller is shown to be almost globally asymptotically stable, and its performance is evaluated experimentally by performing aggressive flip maneuvers. Throughout this study, a nonlinear coupled rotor-fuselage helicopter model with first-order flap dynamics is used.",
      "fieldsOfStudy": [
        "Computer Science"
      ],
      "topics": [
        "disturbances",
        "tracking error",
        "feedback",
        "aggressive flip maneuvers",
        "first-order flap dynamics",
        "exogenous torque",
        "numerical simulations",
        "small-scale aerobatic helicopters",
        "helicopter dynamics",
        "tracking error dynamics",
        "flap angle feedback",
        "nonlinear coupled rotor-fuselage helicopter model",
        "SPR tracking controller",
        "disturbance induced tracking error"
      ]
    },
    "org": {
      "title": "Using Mimicry to Learn about Mental Representations",
      "url": "https://www.semanticscholar.org/paper/9c3725b72fde095b4d3784b584645038db474be4",
      "abstract": "Phonology typically describes speech in terms of discrete signs like features. The field of intonational phonology uses discrete accents to describe intonation and prosody. But, are such representations useful? The results of mimicry experiments indicate that discrete signs are not a useful representation of the shape of intonation contours. Human behaviour seems to be better represented by a attractors where memory retains substantial fine detail about an utterance. There is no evidence that discrete abstract representations that might be formed that have an effect on the speech that is subsequently produced. This paper also discusses conditions under which a discrete phonology can arise from an attractor model and why - for intonation - attractors can be inferred without the implying a discrete phonology.",
      "fieldsOfStudy": [
        "Computer Science",
        "Biology"
      ],
      "topics": [
        "discrete accents",
        "intonation contours",
        "intonation",
        "substantial fine detail",
        "attractors",
        "intonational phonology",
        "representations",
        "speech",
        "discrete phonology",
        "discrete signs",
        "prosody",
        "features",
        "implying a discrete phonology",
        "attractor model",
        "Phonology",
        "terms",
        "memory"
      ]
    }
  },
  null,
  null,
  {
    "sim": 0.41174983791686603,
    "gen": {
      "title": "A Unified Framework for Marketing Budget Allocation",
      "url": "https://www.semanticscholar.org/paper/bbc750f3da77af4c746f97ac540f0821aa8042d7",
      "abstract": "While marketing budget allocation has been studied for decades in traditional business, nowadays online business brings much more challenges due to the dynamic environment and complex decision-making process. In this paper, we present a novel unified framework for marketing budget allocation. By leveraging abundant data, the proposed data-driven approach can help us to overcome the challenges and make more informed decisions. In our approach, a semi-black-box model is built to forecast the dynamic market response and an efficient optimization method is proposed to solve the complex allocation task. First, the response in each market-segment is forecasted by exploring historical data through a semi-black-box model, where the capability of logit demand curve is enhanced by neural networks. The response model reveals relationship between sales and marketing cost. Based on the learned model, budget allocation is then formulated as an optimization problem, and we design efficient algorithms to solve it in both continuous and discrete settings. Several kinds of business constraints are supported in one unified optimization paradigm, including cost upper bound, profit lower bound, or ROI lower bound. The proposed framework is easy to implement and readily to handle large-scale problems. It has been successfully applied to many scenarios in Alibaba Group. The results of both offline experiments and online A/B testing demonstrate its effectiveness.",
      "fieldsOfStudy": [
        "Computer Science",
        "Mathematics"
      ],
      "topics": [
        "ROI",
        "cost upper bound",
        "efficient algorithms",
        "neural networks",
        "complex decision-making process",
        "online business",
        "marketing cost",
        "budget allocation",
        "traditional business",
        "business constraints",
        "logit demand curve",
        "profit",
        "historical data",
        "Alibaba Group",
        "abundant data",
        "profit lower bound",
        "marketing budget allocation"
      ]
    },
    "org": {
      "title": "Reuse, temporal dynamics, interest sharing, and collaboration in social tagging systems",
      "url": "https://www.semanticscholar.org/paper/2ba7b5a8f627d204dd56a5317618a0647180c17f",
      "abstract": "User\u2013generated content shapes the dynamics of the World Wide Web. In particular, collaborative tagging represents a simple, yet powerful, feature that enables users to share and collaboratively annotate content such as photos and URLs. This collaborative behavior and the pool of user\u2013generated metadata create opportunities to improve existing systems and to design new mechanisms. However, to realize this potential, it is necessary to first understand the usage characteristics of current systems. This work addresses this issue by characterizing three tagging systems ( CiteULike, Connotea and del.icio.us ) while focusing on three aspects: i) the patterns of information (tags and items) production; ii) the temporal dynamics of users\u2019 tag vocabularies; and, iii) the social aspects of tagging systems. The analysis of the patterns of information production shows that users publish new content more often than they annotate already existing content in the system. The opposite, however, occurs for tags; the level of tag reuse is much higher. The study of the temporal dynamics of user vocabularies shows that the growth rate of tag vocabularies across the user population over time decreases at early ages, stabilizes, and returns to increase for older users. Moreover, a closer look into the change of vocabulary contents over time shows that despite the fact that tag vocabularies are slowly growing in size with user age, the relative frequency in which each tag is used converges relatively quickly in a users lifetime. Finally, the characterization of social aspects of tagging unveils the relationship between the implicit user ties, as inferred from the similarity between users\u2019 activity, and their explicit social ties, as represented by co\u2013membership in discussion groups or semantic similarity between tag vocabularies.",
      "fieldsOfStudy": [
        "Computer Science",
        "Physics"
      ],
      "topics": [
        "user vocabularies",
        "user age",
        "older users",
        "users",
        "tag vocabularies",
        "existing systems",
        "new content",
        "tag reuse",
        "tags",
        "tagging systems",
        "current systems",
        "new mechanisms",
        "generated content",
        "social aspects",
        "vocabulary contents",
        "content"
      ]
    }
  },
  {
    "sim": 0.293357032700422,
    "gen": {
      "title": "Simple Recurrent Units for Highly Parallelizable Recurrence",
      "url": "https://www.semanticscholar.org/paper/7ba9b6266569bd7b6a3c2ec64348c5b969a5ceb7",
      "abstract": "Common recurrent neural architectures scale poorly due to the intrinsic difficulty in parallelizing their state computations. In this work, we propose the Simple Recurrent Unit (SRU), a light recurrent unit that balances model capacity and scalability. SRU is designed to provide expressive recurrence, enable highly parallelized implementation, and comes with careful initialization to facilitate training of deep models. We demonstrate the effectiveness of SRU on multiple NLP tasks. SRU achieves 5\u20149x speed-up over cuDNN-optimized LSTM on classification and question answering datasets, and delivers stronger results than LSTM and convolutional models. We also obtain an average of 0.7 BLEU improvement over the Transformer model (Vaswani et al., 2017) on translation by incorporating SRU into the architecture.",
      "fieldsOfStudy": [
        "Computer Science"
      ],
      "topics": [
        "deep models",
        "model capacity",
        "convolutional models",
        "scalability",
        "careful initialization",
        "stronger results",
        "SRU",
        "answering datasets",
        "translation",
        "LSTM",
        "multiple NLP tasks",
        "Vaswani",
        "Transformer model",
        "training",
        "state computations",
        "Common recurrent neural architectures",
        "al"
      ]
    },
    "org": {
      "title": "Open Cores for Digital Signal Processing",
      "url": "https://www.semanticscholar.org/paper/5b9271c27acab2a9dcc9feada884f3df380a9286",
      "abstract": "This paper presents the design and implementation of three System on Chip (SoC) cores, which implement the Digital Signal Processing (DSP) functions: Finite Impulse Response (FIR) filter, Infinite Impulse Response (IIR) filter and Fast Fourier Transform (FFT). The FIR filter core is based on the symmetrical realization form, the IIR filter core is based on the Second Order Sections (SOS) architecture and the FFT core is based on the Radix $2^2$ Single Delay Feedback (R$2^2$SDF) architecture. The three cores are compatible with the Wishbone SoC bus and they were described using generic and structural VHDL. In system hardware verification was performed by using an OpenRisc-based SoC synthesized on an Altera FPGA, the tests showed that the designed DSP cores are suitable for building SoC based on the OpenRisc processor and the Wishbone bus.",
      "fieldsOfStudy": [
        "Computer Science"
      ],
      "topics": [
        "cores",
        "FFT",
        "Finite Impulse Response",
        "Infinite Impulse Response",
        "SoC",
        "The FIR filter core",
        "Fast Fourier Transform",
        "designed DSP cores",
        "FFT core",
        "DSP",
        "Altera FPGA",
        "IIR",
        "Wishbone SoC bus",
        "OpenRisc",
        "Radix",
        "Wishbone",
        "OpenRisc-based SoC",
        "generic and structural VHDL",
        "SOS",
        "Altera",
        "Wishbone bus"
      ]
    }
  },
  {
    "sim": 0.22699672776817437,
    "gen": {
      "title": "R\\'egularisation et optimisation pour l'imagerie sismique des fondations de pyl\\^ones",
      "url": "https://www.semanticscholar.org/paper/506a48b2e4ed3507ead6732ec520b29100a8d621",
      "abstract": "Ce rapport de recherche r\u00e9sume l'avancement de travaux men\u00e9s conjointement par l'IRCCyN et l'Ecole Polytechnique de Montr\u00e9al concernant la r\u00e9solution du probl\u00e8me inverse pour l'imagerie sismique des fondations de pyl\u00f4nes \u00e9lectriques. Nous abordons plusieurs m\u00e9thodes de type \u00ab carto-graphie \u00bb. Nous nous int\u00e9ressons plus particuli\u00e8rement \u00e0 des m\u00e9thodes bas\u00e9es sur une formulation bilin\u00e9aire du probl\u00e8me direct d'une part (CSI, gradient modifi\u00e9, etc.) et \u00e0 des m\u00e9thodes bas\u00e9es sur une formulation dite \u00ab primale \u00bb d'autre part. Les performances de ces m\u00e9thodes sont \u00e9valu\u00e9es sur des donn\u00e9es synth\u00e9tiques. Ces travaux ont \u00e9t\u00e9 partiellement financ\u00e9s par RTE \u2013 CNER, qui est \u00e0 l'initiative du projet, et ont \u00e9t\u00e9 effectu\u00e9s avec la collaboration de EDF R&D.",
      "fieldsOfStudy": [
        "Computer Science"
      ],
      "topics": [
        "du probl\u00e8me direct dune part",
        "particuli\u00e8rement \u00e0 des m\u00e9thodes bas\u00e9es sur une formulation bilin\u00e9aire",
        "sur des donn\u00e9es synth\u00e9tiques",
        "et \u00e0 des m\u00e9thodes bas\u00e9es sur une formulation dite \u00ab primale \u00bb dautre part",
        "de ces m\u00e9thodes",
        "la collaboration",
        "Ces travaux ont",
        "CSI, gradient modifi\u00e9",
        "CSI",
        "r\u00e9sume lavancement de travaux men\u00e9s conjointement par lIRCCyN",
        "\u00e0 linitiative du projet",
        "et",
        "qui",
        "de EDF R&D.",
        "m\u00e9thodes de type \u00ab carto-graphie \u00bb",
        "gradient modifi\u00e9",
        "EDF R&D.",
        "probl\u00e8me",
        "graphie",
        "probl\u00e8me direct dune part",
        "projet",
        "Montr\u00e9al",
        "linitiative",
        "lEcole",
        "type",
        "lEcole Polytechnique de Montr\u00e9al",
        "lIRCCyN",
        "des m\u00e9thodes"
      ]
    },
    "org": {
      "title": "On Leader Green Election",
      "url": "https://www.semanticscholar.org/paper/131d60af5e23ea4f8cc58c57a215e0e1bfaa739d",
      "abstract": "We investigate the number of survivors in the Leader Green Election (LGE) algorithm from [5]. Our method is based on the Rice method and gives more precise formulas. We derive upper bounds on the number of survivors in this algorithm and we propose a proper use of LGE. Finally, we discuss one property of a general urns and balls problem and show a lower bound for a number of rounds for a large class of leader election protocols.",
      "fieldsOfStudy": [
        "Computer Science",
        "Mathematics"
      ],
      "topics": [
        "LGE",
        "leader election protocols",
        "precise formulas",
        "proper use",
        "algorithm",
        "rounds",
        "survivors",
        "upper bounds",
        "balls",
        "Leader Green Election",
        "large class",
        "Rice",
        "number",
        "number",
        "Rice method",
        "(LGE",
        "lower bound",
        "general urns and balls problem"
      ]
    }
  },
  null,
  {
    "sim": 0.3540994329656034,
    "gen": {
      "title": "Dynamic replication factor model for Linux containers-based cloud systems",
      "url": "https://www.semanticscholar.org/paper/118110b70128d9a83f3336eff6b6de7e65d7f547",
      "abstract": null,
      "fieldsOfStudy": [
        "Computer Science"
      ]
    },
    "org": {
      "title": "Online Learning for Ground Trajectory Prediction",
      "url": "https://www.semanticscholar.org/paper/ed237cefc869c7f4bfedeef19bd7d87f338e244c",
      "abstract": "This paper presents a model based on an hybrid system to numerically simulate the climbing phase of an aircraft. This model is then used within a trajectory prediction tool. Finally, the Covariance Matrix Adaptation Evolution Strategy (CMA-ES) optimization algorithm is used to tune five selected parameters, and thus improve the accuracy of the model. Incorporated within a trajectory prediction tool, this model can be used to derive the order of magnitude of the prediction error over time, and thus the domain of validity of the trajectory prediction. A first validation experiment of the proposed model is based on the errors along time for a one-time trajectory prediction at the take off of the flight with respect to the default values of the theoretical BADA model. This experiment, assuming complete information, also shows the limit of the model. A second experiment part presents an on-line trajectory prediction, in which the prediction is continuously updated based on the current aircraft position. This approach raises several issues, for which improvements of the basic model are proposed, and the resulting trajectory prediction tool shows statistically significantly more accurate results than those of the default model.",
      "fieldsOfStudy": [
        "Computer Science"
      ],
      "topics": [
        "resulting trajectory prediction tool",
        "theoretical BADA model",
        "time",
        "default model",
        "trajectory prediction",
        "proposed model",
        "basic model",
        "prediction error",
        "current aircraft position",
        "model",
        "model",
        "BADA"
      ]
    }
  },
  {
    "sim": 0.3473665934373795,
    "gen": {
      "title": "An Optimized Control Strategy for Parallel Hybrid Electric Vehicle",
      "url": "https://www.semanticscholar.org/paper/ed60e1e0cffb29a32d02098ebfe1afd9532a8186",
      "abstract": "A systematic process of optimization is suggested to obtain the best control maps for a parallel type hybrid electric vehicle. Taking the fuel consumption as the cost function and driving cycle as part of the constraints,an optimization problem for CVT pulley ratio control and motor torque control can be formulated. The change of the battery charge state between the start and end point of the given driving cycle also works as a constraint. In order to see the effect of various control strategies on system behavior and overall fuel consumption, a simulation model was built to accommodate the functional blocks representing hybrid powertrain subsystem components and corresponding control units. The aforementioned control strategies were tested and validated on this simulation model and finally tested on a prototype vehicle powered by 1.6 liter gasoline engine and 12kW engine mounted permanent magnet motor to demonstrate the effectiveness of the hybrid system on fuel economy improvement and performance.",
      "fieldsOfStudy": [
        "Engineering"
      ],
      "topics": [
        "motor torque control",
        "corresponding control units",
        "permanent magnet motor",
        "hybrid powertrain subsystem components",
        "12kW engine",
        "control strategies",
        "CVT pulley ratio control",
        "fuel economy improvement",
        "overall fuel consumption",
        "system behavior",
        "parallel type hybrid electric vehicle",
        "performance",
        "1.6 liter gasoline engine",
        "12kW",
        "optimization",
        "driving cycle",
        "12kW engine mounted permanent magnet motor",
        "best control maps",
        "The aforementioned control strategies"
      ]
    },
    "org": {
      "title": "PTP: Path-specified Transport Protocol for Concurrent Multipath Transmission in Named Data Networks",
      "url": "https://www.semanticscholar.org/paper/a47a2e99cd0435fdfab7389c69f157e2389fe258",
      "abstract": null,
      "fieldsOfStudy": [
        "Computer Science"
      ]
    }
  },
  {
    "sim": 0.5985984947125579,
    "gen": {
      "title": "On the Iteration Complexity Analysis of Stochastic Primal-Dual Hybrid Gradient Approach with High Probability",
      "url": "https://www.semanticscholar.org/paper/c71f3a5fe7b388d14df03327b913e73f3df54a5a",
      "abstract": null,
      "fieldsOfStudy": [
        "Mathematics",
        "Computer Science"
      ]
    },
    "org": {
      "title": "On Learning Invariant Representation for Domain Adaptation",
      "url": "https://www.semanticscholar.org/paper/3645848a932a8182533e5b054ca67d1072f360f9",
      "abstract": "Due to the ability of deep neural nets to learn rich representations, recent advances in unsupervised domain adaptation have focused on learning domain-invariant features that achieve a small error on the source domain. The hope is that the learnt representation, together with the hypothesis learnt from the source domain, can generalize to the target domain. In this paper, we first construct a simple counterexample showing that, contrary to common belief, the above conditions are not sufficient to guarantee successful domain adaptation. In particular, the counterexample exhibits \\emph{conditional shift}: the class-conditional distributions of input features change between source and target domains. To give a sufficient condition for domain adaptation, we propose a natural and interpretable generalization upper bound that explicitly takes into account the aforementioned shift. Moreover, we shed new light on the problem by proving an information-theoretic lower bound on the joint error of \\emph{any} domain adaptation method that attempts to learn invariant representations. Our result characterizes a fundamental tradeoff between learning invariant representations and achieving small joint error on both domains when the marginal label distributions differ from source to target. Finally, we conduct experiments on real-world datasets that corroborate our theoretical findings. We believe these insights are helpful in guiding the future design of domain adaptation and representation learning algorithms.",
      "fieldsOfStudy": [
        "Computer Science",
        "Mathematics"
      ],
      "topics": [
        "domain adaptation",
        "unsupervised domain adaptation",
        "successful domain adaptation",
        "small joint error",
        "representation learning algorithms",
        "source and target domains",
        "\\emph{any} domain adaptation method",
        "invariant representations",
        "source",
        "domain-invariant features",
        "source domain",
        "rich representations",
        "target domain",
        "input features",
        "\\emph{conditional shift",
        "target",
        "domains"
      ]
    }
  },
  {
    "sim": 0.5508268750427193,
    "gen": {
      "title": "Statistical Decision Making for Optimal Budget Allocation in Crowd Labeling",
      "url": "https://www.semanticscholar.org/paper/2b4bcce21acfba3309df59d3e7c4d13c4d0c2147",
      "abstract": "It has become increasingly popular to obtain machine learning labels through commercial crowdsourcing services. The crowdsourcing workers or annotators are paid for each label they provide, but the task requester usually has only a limited amount of the budget. Since the data instances have different levels of labeling difficulty and the workers have different reliability for the labeling task, it is desirable to wisely allocate the budget among all the instances and workers such that the overall labeling quality is maximized. In this paper, we formulate the budget allocation problem as a Bayesian Markov decision process (MDP), which simultaneously conducts learning and decision making. The optimal allocation policy can be obtained by using the dynamic programming (DP) recurrence. However, DP quickly becomes computationally intractable when the size of the problem increases. To solve this challenge, we propose a computationally eficient approximate policy which is called optimistic knowledge gradient. Our method applies to both pull crowdsourcing marketplaces with homogeneous workers and push marketplaces with heterogeneous workers. It can also incorporate the contextual information of instances when they are available. The experiments on both simulated and real data show that our policy achieves a higher labeling quality than other existing policies at the same budget level.",
      "fieldsOfStudy": [
        "Computer Science",
        "Mathematics"
      ],
      "topics": [
        "homogeneous workers",
        "workers",
        "labeling difficulty",
        "existing policies",
        "decision making",
        "different levels",
        "machine learning labels",
        "crowdsourcing marketplaces",
        "commercial crowdsourcing services",
        "optimistic knowledge gradient",
        "different reliability",
        "Bayesian Markov",
        "budget level",
        "overall labeling quality",
        "learning"
      ]
    },
    "org": {
      "title": "Semantic Channel and Shannon's Channel Mutually Match for Multi-Label Classification",
      "url": "https://www.semanticscholar.org/paper/26e65c4d3578f8827b545a42fb9d088f2e86b5b8",
      "abstract": null,
      "fieldsOfStudy": [
        "Mathematics",
        "Computer Science"
      ]
    }
  },
  {
    "sim": 0.49014941488993924,
    "gen": {
      "title": "A new family of weighted one-parameter flux reconstruction schemes",
      "url": "https://www.semanticscholar.org/paper/5a6d29b15ba2a02a8ad4599f7d795a88877f0613",
      "abstract": null,
      "fieldsOfStudy": [
        "Mathematics"
      ]
    },
    "org": {
      "title": "Unifying mirror descent and dual averaging",
      "url": "https://www.semanticscholar.org/paper/72b7b7ae04bc0245ba6bfbf29c0de454924a9c4f",
      "abstract": null,
      "fieldsOfStudy": [
        "Mathematics",
        "Computer Science"
      ]
    }
  },
  null,
  {
    "sim": 0.7399570323899364,
    "gen": {
      "title": "A Novel Approach for Handling Misbehaving Nodes in Behavior-Aware Mobile Networking",
      "url": "https://www.semanticscholar.org/paper/49d836b2928ee7202127d4d20757a84b775f184f",
      "abstract": "Profile-cast is a service paradigm within the communication framework of delay tolerant networks (DTN). Instead of using destination addresses to determine the final destination it uses similarity-based forwarding protocol. With the rise in popularity of various wireless networks, the need to make wireless technologies robust, resilient to attacks and failure becomes mandatory. One issue that remains to be addressed in behavioral networks is node co-operation in forwarding packets. Nodes might behave selfishly (due to bandwidth preservation, energy /power constraints) or maliciously by dropping packets or not forwarding them to other nodes based on profile similarity. In both cases the net result is degradation in the performance of the network. It is our goal to show that the performance of the behavioral network can be improved by employing self-policing scheme that would detect node misbehavior and then decide how to tackle them in order to ensure node cooperation or so that the overall performance does not fall below a certain threshold. For this various existing self-policing techniques which are in use in ad-hoc networks will be first tried on this behavioral scenario.At various stages simulation would be used to measure performances of the network under different constraints, and after subjected to different techniques",
      "fieldsOfStudy": [
        "Computer Science"
      ],
      "topics": [
        "behavioral networks",
        "tolerant networks",
        "wireless networks",
        "forwarding packets",
        "different techniques",
        "profile similarity",
        "different constraints",
        "nodes",
        "similarity-based forwarding protocol",
        "performances",
        "node co",
        "node misbehavior",
        "energy /power constraints",
        "packets",
        "node cooperation",
        "delay tolerant networks",
        "Nodes"
      ]
    },
    "org": {
      "title": "Improving Performance of Routing Protocols Using MRP Framework",
      "url": "https://www.semanticscholar.org/paper/4dec6ba19da3d82f4f70cfa7fa0e3a71009ec5b5",
      "abstract": "These days MANET is an amazing remarkably altering or rising technology, for the reason that of its elite nature of scattered mobile devices and self motivated network topology. The mobile adhoc routing protocol follows several principles in wireless MANETs. The up to date and novel applications based on wireless technology are being produced in the private as well as commercial sectors. A lot of challenges which are facing wireless MANETs like network stability, security, energy efficiency and performance analysis etc. At present wireless adhoc network get much more attention because of its accessibility everywhere. As a result researchers produce several routing protocols. In this paper first of all we analyzed the performance investigation of wireless routing protocols on the basis of ROH, throughput, end to end delay and PDR. After that we proposed an Mixed Routing Protocol framework which improve performance.",
      "fieldsOfStudy": [
        "Computer Science"
      ],
      "topics": [
        "wireless routing protocols",
        "performance analysis",
        "routing protocols",
        "wireless MANETs",
        "present wireless adhoc network",
        "wireless technology",
        "self motivated network topology",
        "network stability",
        "performance",
        "PDR",
        "energy efficiency",
        "scattered mobile devices",
        "principles",
        "delay",
        "security",
        "wireless adhoc network",
        "end"
      ]
    }
  },
  null,
  null,
  {
    "sim": 0.39905041982544076,
    "gen": {
      "title": "Fuzzy Reasoning and Fuzzy Petri Nets in Manufacturing Systems Modeling",
      "url": "https://www.semanticscholar.org/paper/77bdca6d457ef4de937bc16e6d4723df4d22d952",
      "abstract": "This article presents a net-based structure to model approximate reasoning using fuzzy logic, the fuzzy Petri net model. The knowledge bases to be considered here are assumed to be fuzzy production rules. After a brief introduction of knowledge representation and fuzzy reasoning, we give a new definition of the Fuzzy Petri net model. Next, the basic net structures for complex forms of rules such as rules with conjunction in the antecedent, rules with linguistic quantifiers, and rules with certainty factors are presented. Typical rules sets like parallel rules and conflicting rules are addressed as well. Design techniques to be used when the basic structures are mixed in the context of a large knowledge base are also included. A fuzzy reasoning algorithm is provided. Finally, an application example concerning manufacturing cells modeling is introduced to illustrate the usefulness of the approach proposed.",
      "fieldsOfStudy": [
        "Computer Science"
      ],
      "topics": [
        "fuzzy production rules",
        "conflicting rules",
        "rules",
        "parallel rules",
        "Typical rules",
        "fuzzy reasoning",
        "fuzzy logic",
        "certainty factors",
        "approximate reasoning",
        "linguistic quantifiers",
        "Fuzzy Petri",
        "complex forms",
        "Fuzzy Petri net model",
        "knowledge representation",
        "Typical rules sets",
        "basic net structures",
        "manufacturing cells modeling"
      ]
    },
    "org": {
      "title": "An Appropriate Sensor Distribution Technique in Wireless Sensor Networks",
      "url": "https://www.semanticscholar.org/paper/f0a0768b4c714b33255391405c819f05983ddc42",
      "abstract": "Wireless Sensor Network (WSN) is pertinent to many applications with varied network parameters. Sensor node placement in the application region whether it is indoor or outdoor is a major task as well as plays very remarkable role in the network performance. Node placement is carried out according to the region where it is applied, either deterministic or non deterministic. Because of the need for different sensing probability or detection probability, same approach of sensor placement may not be suited for all the applications. Some of the applications are well formed and give better performance with the uniform distribution of sensors but few need intense distribution of nodes in particular sensitive places especially those applications meant for intrusion detection. An application which needs high level intrusion detection and a suitable sensor distribution methodology, known as Half-Normal Distribution (Half-Gaussian) based deployment, have been set forth in this paper. We have also discussed the theoretical comparison for detection probability with the uniform distribution in terms of number of sensor nodes.",
      "fieldsOfStudy": [
        "Computer Science"
      ],
      "topics": [
        "sensor nodes",
        "intense distribution",
        "sensor placement",
        "intrusion detection",
        "high level intrusion detection",
        "sensors",
        "detection probability",
        "applications",
        "particular sensitive places",
        "varied network parameters",
        "suitable sensor distribution methodology",
        "better performance",
        "nodes",
        "different sensing probability",
        "uniform distribution",
        "Node placement"
      ]
    }
  },
  {
    "sim": 0.7037218877209293,
    "gen": {
      "title": "Approximate Policy Iteration Schemes: A Comparison",
      "url": "https://www.semanticscholar.org/paper/d746a1f64daae2d3fb91de8ffe08e9e5668cdc38",
      "abstract": "We consider the infinite-horizon discounted optimal control problem formalized by Markov Decision Processes. We focus on several approximate variations of the Policy Iteration algorithm: Approximate Policy Iteration (API) (Bertsekas & Tsitsiklis, 1996), Conservative Policy Iteration (CPI) (Kakade & Langford, 2002), a natural adaptation of the Policy Search by Dynamic Programming algorithm (Bagnell et al., 2003) to the infinite-horizon case (PSDP\u221e), and the recently proposed Non-Stationary Policy Iteration (NSPI(m)) (Scherrer & Lesner, 2012). For all algorithms, we describe performance bounds with respect the per-iteration error e, and make a comparison by paying a particular attention to the concentrability constants involved, the number of iterations and the memory required. Our analysis highlights the following points: 1) The performance guarantee of CPI can be arbitrarily better than that of API, but this comes at the cost of a relative--exponential in 1/e increase of the number of iterations. 2) PSDP1 enjoys the best of both worlds: its performance guarantee is similar to that of CPI, but within a number of iterations similar to that of API. 3) Contrary to API that requires a constant memory, the memory needed by CPI and PSDP\u221e is proportional to their number of iterations, which may be problematic when the discount factor \u03b3 is close to 1 or the approximation error e is close to 0; we show that the NSPI(m) algorithm allows to make an overall trade-off between memory and performance. Simulations with these schemes confirm our analysis.",
      "fieldsOfStudy": [
        "Computer Science",
        "Mathematics"
      ],
      "topics": [
        "iterations",
        "memory",
        "Conservative Policy Iteration",
        "Non-Stationary Policy Iteration",
        "Approximate Policy Iteration",
        "CPI",
        "performance bounds",
        "Markov Decision Processes",
        "performance",
        "API",
        "Dynamic Programming algorithm",
        "PSDP\u221e",
        "NSPI(m",
        "approximation error e",
        "Policy Iteration algorithm",
        "Dynamic Programming",
        "Lesner",
        "recently proposed Non-Stationary Policy Iteration"
      ]
    },
    "org": {
      "title": "MCTS Based on Simple Regret",
      "url": "https://www.semanticscholar.org/paper/0d5f8b8a2956aca4ac3f8b8ed5088a4d089da687",
      "abstract": "\n \n UCT, a state-of-the art algorithm for Monte Carlo tree search (MCTS) in games and Markov decision processes, is based on UCB, a sampling policy for the Multi-armed Bandit problem (MAB) that minimizes the cumulative regret. However, search differs from MAB in that in MCTS it is usually only the final ``arm pull'' (the actual move selection) that collects a reward, rather than all ``arm pulls''. Therefore, it makes more sense to minimize the simple regret, as opposed to the cumulative regret. We begin by introducing policies for multi-armed bandits with lower finite-time and asymptotic simple regret than UCB, using it to develop a two-stage scheme (SR+CR) for MCTS which outperforms UCT empirically. Optimizing the sampling process is itself a metareasoning problem, a solution of which can use value of information (VOI) techniques. Although the theory of VOI for search exists, applying it to MCTS is non-trivial, as typical myopic assumptions fail. Lacking a complete working VOI theory for MCTS, we nevertheless propose a sampling scheme that is ``aware'' of VOI, achieving an algorithm that in empirical evaluation outperforms both UCT and the other proposed algorithms.\n \n",
      "fieldsOfStudy": [
        "Computer Science"
      ],
      "topics": [
        "asymptotic simple regret",
        "MCTS",
        "empirical evaluation outperforms",
        "Monte Carlo tree search",
        "multi-armed bandits",
        "VOI",
        "Markov decision processes",
        "typical myopic assumptions",
        "UCB",
        "cumulative regret",
        "search exists",
        "simple regret",
        "Monte Carlo",
        "search differs",
        "policies",
        "UCT",
        "empirical evaluation",
        "actual move selection",
        "proposed algorithms",
        "search"
      ]
    }
  },
  null,
  {
    "sim": 0.445622517707866,
    "gen": {
      "title": "Understanding Deep Convolutional Networks through Gestalt Theory",
      "url": "https://www.semanticscholar.org/paper/f38cedfd93f421058682c30635a64fc0a2498192",
      "abstract": "The superior performance of deep convolutional networks over high-dimensional problems have made them very popular for several applications. Despite their wide adoption, their underlying mechanisms still remain unclear with their improvement procedures still relying mainly on a trial and error process. We introduce a novel sensitivity analysis based on the Gestalt theory for giving insights into the classifier function and intermediate layers. Since Gestalt psychology stipulates that perception can be a product of complex interactions among several elements, we perform an ablation study based on this concept to discover which principles and image context significantly contribute in the network classification. Our results reveal that convnets follow most of the visual cortical perceptual mechanisms defined by the Gestalt principles at several levels. The proposed framework stimulates specific feature maps in classification problems and reveal important network attributes that can produce more explainable network models.",
      "fieldsOfStudy": [
        "Computer Science"
      ],
      "topics": [
        "explainable network models",
        "important network attributes",
        "classification problems",
        "deep convolutional networks",
        "applications",
        "levels",
        "intermediate layers",
        "image context",
        "Gestalt psychology",
        "network classification",
        "Gestalt",
        "complex interactions",
        "high-dimensional problems",
        "visual cortical perceptual mechanisms",
        "specific feature maps"
      ]
    },
    "org": {
      "title": "Combinaison d'information visuelle, conceptuelle, et contextuelle pour la construction automatique de hierarchies semantiques adaptees a l'annotation d'images",
      "url": "https://www.semanticscholar.org/paper/b9ecaa88aab496443b90dcdfef850575c0d0a1fb",
      "abstract": "This paper proposes a new methodology to automatically build semantic hierarchies suitable for image annotation and classification. The building of the hierarchy is based on a new measure of semantic similarity. The proposed measure incorporates several sources of information: visual, conceptual and contextual as we defined in this paper. The aim is to provide a measure that best represents image semantics. We then propose rules based on this measure, for the building of the final hierarchy, and which explicitly encode hierarchical relationships between different concepts. Therefore, the built hierarchy is used in a semantic hierarchical classification framework for image annotation. Our experiments and results show that the hierarchy built improves classification results. \nCe papier propose une nouvelle methode pour la construction automatique de hierarchies semantiques adaptees a la classification et a l'annotation d'images. La construction de la hierarchie est basee sur une nouvelle mesure de similarite semantique qui integre plusieurs sources d'informations: visuelle, conceptuelle et contextuelle que nous definissons dans ce papier. L'objectif est de fournir une mesure qui est plus proche de la semantique des images. Nous proposons ensuite des regles, basees sur cette mesure, pour la construction de la hierarchie finale qui encode explicitement les relations hierarchiques entre les differents concepts. La hierarchie construite est ensuite utilisee dans un cadre de classification semantique hierarchique d'images en concepts visuels. Nos experiences et resultats montrent que la hierarchie construite permet d'ameliorer les resultats de la classification.",
      "fieldsOfStudy": [
        "Computer Science"
      ],
      "topics": [
        "la construction de la hierarchie finale",
        "image annotation",
        "pour la construction de la hierarchie finale qui encode explicitement les relations",
        "la construction automatique de hierarchies semantiques",
        "entre les differents concepts",
        "la hierarchie",
        "image semantics",
        "different concepts",
        "semantic hierarchies",
        "La hierarchie construite est ensuite utilisee dans",
        "semantic hierarchical classification framework",
        "de la classification",
        "semantic similarity",
        "plusieurs sources",
        "hierarchical relationships",
        "results",
        "sources",
        "les differents concepts",
        "la classification",
        "nous"
      ]
    }
  },
  {
    "sim": 0.6325647077515453,
    "gen": {
      "title": "The Decentralized Financial Crisis",
      "url": "https://www.semanticscholar.org/paper/c876d76a22b23ae28fe6d9c5589f857768d543fc",
      "abstract": "The Global Financial Crisis of 2008, caused by the accumulation of excessive financial risk, inspired Satoshi Nakamoto to create Bitcoin. Now, more than ten years later, Decentralized Finance (DeFi), a peer-to-peer financial paradigm which leverages blockchain-based smart contracts to ensure its integrity and security, contains over 702m USD of capital as of April 15th, 2020. As this ecosystem develops, it is at risk of the very sort of financial meltdown it is supposed to be preventing. In this paper we explore how design weaknesses and price fluctuations in DeFi protocols could lead to a DeFi crisis. We focus on DeFi lending protocols as they currently constitute most of the DeFi ecosystem with a 76% market share by capital as of April 15th, 2020. First, we demonstrate the feasibility of attacking Maker's governance design to take full control of the protocol, the largest DeFi protocol by market share, which would have allowed the theft of 0.5bn USD of collateral and the minting of an unlimited supply of DAI tokens. In doing so, we present a novel strategy utilizing so-called flash loans that would have in principle allowed the execution of the governance attack in just two transactions and without the need to lock any assets. Approximately two weeks after we disclosed the attack details, Maker modified the governance parameters mitigating the attack vectors. Second, we turn to a central component of financial risk in DeFi lending protocols. Inspired by stress-testing as performed by central banks, we develop a stress-testing framework for a stylized DeFi lending protocol, focusing our attention on the impact of a drying-up of liquidity on protocol solvency. Based on our parameters, we find that with sufficiently illiquidity a lending protocol with a total debt of 400m USD could become undercollateralized within 19 days.",
      "fieldsOfStudy": [
        "Computer Science",
        "Business"
      ],
      "topics": [
        "DeFi protocols",
        "protocol solvency",
        "April 15th",
        "DeFi",
        "stylized DeFi lending protocol",
        "excessive financial risk",
        "financial risk",
        "DAI tokens",
        "largest DeFi protocol",
        "market share",
        "financial meltdown",
        "USD",
        "capital",
        "Bitcoin",
        "lending protocol",
        "risk"
      ]
    },
    "org": {
      "title": "Diversification Across Mining Pools: Optimal Mining Strategies under PoW",
      "url": "https://www.semanticscholar.org/paper/cf661c9c9957681518b908656aa059a5e6800fbf",
      "abstract": "\n Mining is a central operation of all proof-of-work (PoW)-based cryptocurrencies. The vast majority of miners today participate in \u201cmining pools\u201d instead of \u201csolo mining\u201d in order to lower risk and achieve a more steady income. However, this rise of participation in mining pools negatively affects the decentralization levels of most cryptocurrencies. In this work, we look into mining pools from the point of view of a miner: We present an analytical model and implement a computational tool that allows miners to optimally distribute their computational power over multiple pools and PoW cryptocurrencies (i.e. build a mining portfolio), taking into account their risk aversion levels. Our tool allows miners to maximize their risk-adjusted earnings by diversifying across multiple mining pools. Our underlying techniques are drawn from both the areas of financial economy and computer science since we use computer science-based approaches (i.e. optimization techniques) to experimentally prove how parties (and in particular miners) interact with cryptocurrencies in a way of increasing their Sharpe ratio. To showcase our model, we run an experiment in Bitcoin historical data and demonstrate that a miner diversifying over multiple pools, as instructed by our model/tool, receives a higher overall Sharpe ratio (i.e. average excess reward over its standard deviation/volatility).",
      "fieldsOfStudy": [
        "Economics",
        "Computer Science"
      ],
      "topics": [
        "cryptocurrencies",
        "multiple pools",
        "solo mining",
        "Mining",
        "particular miners",
        "miners",
        "Sharpe",
        "computer science",
        "risk",
        "higher overall Sharpe ratio",
        "\u201cmining pools",
        "Bitcoin historical data",
        "risk aversion levels",
        "i.e. average excess reward",
        "Sharpe ratio"
      ]
    }
  },
  {
    "sim": 0.4677301656224818,
    "gen": {
      "title": "Detecting Cyberbullying and Cyberaggression in Social Media",
      "url": "https://www.semanticscholar.org/paper/60504821aaf462637c3702a5f699e2fad589e5ff",
      "abstract": "Cyberbullying and cyberaggression are increasingly worrisome phenomena affecting people across all demographics. More than half of young social media users worldwide have been exposed to such prolonged and/or coordinated digital harassment. Victims can experience a wide range of emotions, with negative consequences such as embarrassment, depression, isolation from other community members, which embed the risk to lead to even more critical consequences, such as suicide attempts. In this work, we take the first concrete steps to understand the characteristics of abusive behavior in Twitter, one of today\u2019s largest social media platforms. We analyze 1.2 million users and 2.1 million tweets, comparing users participating in discussions around seemingly normal topics like the NBA, to those more likely to be hate-related, such as the Gamergate controversy, or the gender pay inequality at the BBC station. We also explore specific manifestations of abusive behavior, i.e., cyberbullying and cyberaggression, in one of the hate-related communities (Gamergate). We present a robust methodology to distinguish bullies and aggressors from normal Twitter users by considering text, user, and network-based attributes. Using various state-of-the-art machine-learning algorithms, we classify these accounts with over 90% accuracy and AUC. Finally, we discuss the current status of Twitter user accounts marked as abusive by our methodology and study the performance of potential mechanisms that can be used by Twitter to suspend users in the future.",
      "fieldsOfStudy": [
        "Computer Science",
        "Psychology"
      ],
      "topics": [
        "Twitter user accounts",
        "normal Twitter users",
        "young social media users",
        "users",
        "community members",
        "suicide attempts",
        "abusive behavior",
        "negative consequences",
        "Twitter",
        "prolonged and/or coordinated digital harassment",
        "AUC",
        "potential mechanisms",
        "BBC",
        "today\u2019s largest social media platforms",
        "Gamergate"
      ]
    },
    "org": {
      "title": "Forecasting elections results via the voter model with stubborn nodes",
      "url": "https://www.semanticscholar.org/paper/a8b1980bd73ee5c251fc484ee4840efeebdced0b",
      "abstract": null,
      "fieldsOfStudy": [
        "Computer Science",
        "Economics"
      ]
    }
  },
  {
    "sim": 0.5204341714115713,
    "gen": {
      "title": "Prestopping: How Does Early Stopping Help Generalization against Label Noise?",
      "url": "https://www.semanticscholar.org/paper/2700e81e00e241eba83ed9f73866e4ad7b0a60df",
      "abstract": "Noisy labels are very common in real-world training data, which lead to poor generalization on test data because of overfitting to the noisy labels. In this paper, we claim that such overfitting can be avoided by \"early stopping\" training a deep neural network before the noisy labels are severely memorized. Then, we resume training the early stopped network using a \"maximal safe set,\" which maintains a collection of almost certainly true-labeled samples at each epoch since the early stop point. Putting them all together, our novel two-phase training method, called Prestopping, realizes noise-free training under any type of label noise for practical use. Extensive experiments using four image benchmark data sets verify that our method significantly outperforms four state-of-the-art methods in test error by 0.4-8.2 percent points under existence of real-world noise.",
      "fieldsOfStudy": [
        "Computer Science",
        "Mathematics"
      ],
      "topics": [
        "test data",
        "practical use",
        "real-world training data",
        "test error",
        "real-world noise",
        "poor generalization",
        "noise-free training",
        "Noisy",
        "existence",
        "image benchmark data sets",
        "early stop point",
        "noisy labels",
        "early stopped network",
        "overfitting",
        "Prestopping"
      ]
    },
    "org": {
      "title": "Randomized Spectral Clustering in Large-Scale Stochastic Block Models",
      "url": "https://www.semanticscholar.org/paper/662886d79fffe8dedefe4af316ee223fec37df7f",
      "abstract": "Abstract Spectral clustering has been one of the widely used methods for community detection in networks. However, large-scale networks bring computational challenges to the eigenvalue decomposition therein. In this paper, we study the spectral clustering using randomized sketching algorithms from a statistical perspective, where we typically assume the network data are generated from a stochastic block model that is not necessarily of full rank. To do this, we first use the recently developed sketching algorithms to obtain two randomized spectral clustering algorithms, namely, the random projection-based and the random sampling-based spectral clustering. Then we study the theoretical bounds of the resulting algorithms in terms of the approximation error for the population adjacency matrix, the misclassification error, and the estimation error for the link probability matrix. It turns out that, under mild conditions, the randomized spectral clustering algorithms lead to the same theoretical bounds as those of the original spectral clustering algorithm. We also extend the results to degree-corrected stochastic block models. Numerical experiments support our theoretical findings and show the efficiency of randomized methods. A new R package called Rclust is developed and made available to the public. Supplementary materials for this article are available online.",
      "fieldsOfStudy": [
        "Computer Science",
        "Mathematics"
      ],
      "topics": [
        "randomized sketching algorithms",
        "rank",
        "randomized methods",
        "networks",
        "original spectral clustering algorithm",
        "computational challenges",
        "community detection",
        "degree-corrected stochastic block models",
        "Abstract Spectral clustering",
        "stochastic block model",
        "link probability matrix",
        "resulting algorithms",
        "spectral clustering",
        "Spectral clustering",
        "estimation error",
        "population adjacency matrix",
        "misclassification error"
      ]
    }
  },
  null,
  {
    "sim": 0.22127697097992238,
    "gen": {
      "title": "On the Discretization of Robust Exact Filtering Differentiators",
      "url": "https://www.semanticscholar.org/paper/c4d24121bd07e01dcb8b84cfee352b8085ec5f5d",
      "abstract": null,
      "fieldsOfStudy": [
        "Computer Science",
        "Engineering",
        "Mathematics"
      ]
    },
    "org": {
      "title": "Threshold-based obfuscated keys with quantifiable security against invasive readout",
      "url": "https://www.semanticscholar.org/paper/807e572b01728ca6255c8b79706e12acf8de5b97",
      "abstract": "Advances in reverse engineering make it challenging to deploy any on-chip information in a way that is hidden from a determined attacker. A variety of techniques have been proposed for design obfuscation including look-alike cells in which functionality is determined by hard to observe mechanisms including dummy vias or transistor threshold voltages. Threshold-based obfuscation is especially promising because threshold voltages cannot be observed optically and require more sophisticated measurements by the attacker. In this work, we demonstrate the effectiveness of a methodology that applies threshold-defined behavior to memory cells, in combination with error correcting codes to achieve a high degree of protection against invasive reverse engineering. The combination of error correction and small threshold manipulations is significant because it makes the attacker's job harder without compromising the reliability of the obfuscated key. We present analysis to quantify key reliability of our approach, and its resistance to reverse engineering attacks that seek to extract the key through imperfect measurement of transistor threshold voltages. The security analysis and cost metrics we provide allow designers to make a quantifiable tradeoff between cost and security. We find that the combination of small threshold offsets and stronger error correcting codes are advantageous when security is the primary objective.",
      "fieldsOfStudy": [
        "Computer Science"
      ],
      "topics": [
        "transistor threshold voltages",
        "threshold voltages",
        "imperfect measurement",
        "small threshold manipulations",
        "small threshold offsets",
        "key reliability",
        "engineering attacks",
        "reverse engineering",
        "error correcting codes",
        "memory cells",
        "dummy vias",
        "security",
        "threshold-defined behavior",
        "cost metrics",
        "hard to observe mechanisms"
      ]
    }
  },
  {
    "sim": 0.7534550605758313,
    "gen": {
      "title": "Finite-SNR Diversity-Multiplexing Tradeoff via Asymptotic Analysis of Large MIMO Systems",
      "url": "https://www.semanticscholar.org/paper/cd6fa5e279ef3866833bccfb5a329dc4d923caec",
      "abstract": "Diversity-multiplexing tradeoff (DMT) was characterized asymptotically (SNR- > infinity) for i.i.d. Rayleigh fading channel by Zheng and Tse . The SNR-asymptotic DMT overestimates the finite-SNR one . This paper outlines a number of additional limitations and difficulties of the DMT framework and discusses their implications. Using the recent results on the size-asymptotic (in the number of antennas) outage capacity distribution, the finite-SNR, size-asymptotic DMT is derived for a broad class of fading distributions. The SNR range over which the finite-SNR DMT is accurately approximated by the SNR-asymptotic one is characterized. The multiplexing gain definition is shown to affect critically this range and thus should be carefully selected, so that the SNR-asymptotic DMT is an accurate approximation at realistic SNR values and thus has operational significance to be used as a design criterion. The finite-SNR diversity gain is shown to decrease with correlation and power imbalance in a broad class of fading channels, and such an effect is described in a compact, closed form. Complete characterization of the outage probability (or outage capacity) requires not only the finite-SNR DMT, but also the SNR offset, which is introduced and investigated as well. This offset, which is not accounted for in the DMT framework, is shown to have a significant impact on the outage probability for a broad class of fading channels, especially when the multiplexing gain is small. The analytical results and conclusions are validated via extensive Monte Carlo simulations. Overall, the size-asymptotic DMT represents a valuable alternative to the SNR-asymptotic one.",
      "fieldsOfStudy": [
        "Mathematics",
        "Computer Science"
      ],
      "topics": [
        "realistic SNR values",
        "SNR",
        "fading channels",
        "fading distributions",
        "outage capacity",
        "i.i.d",
        "extensive Monte Carlo simulations",
        "Rayleigh fading channel",
        "operational significance",
        "power imbalance",
        "Monte Carlo",
        "SNR-asymptotic one",
        "SNR offset",
        "i.i.d. Rayleigh fading channel",
        "DMT",
        "Tse",
        "SNR-asymptotic DMT",
        "finite-SNR DMT",
        "The finite-SNR diversity gain",
        "DMT framework"
      ]
    },
    "org": {
      "title": "Multiple-symbol differential detection for distributed space-time coding",
      "url": "https://www.semanticscholar.org/paper/7ac56070b4b1f2f824055109742b636fbfe5678c",
      "abstract": "Differential distributed space-time coding (DDSTC) technique has been considered for relay networks to provide both diversity gain and high throughput in the absence of channel state information. Conventional differential detection (CDD) or two-symbol non-coherent detection over slow-fading channels has been examined and shown to suffer 3-4 dB loss when compared to coherent detections. Moreover, it has also been shown that the performance of CDD severely degrades in fast-fading channels and an irreducible error floor exists at high signal-to-noise ratio region. To overcome the error floor experienced with fast-fading, a nearly optimal \u201cmultiple-symbol\u201d differential detection (MSDD) is developed in this paper. The MSDD algorithm jointly processes a larger window of received signals for detection and significantly improves the performance of D-DSTC in fast-fading channels. The error performance of the MSDD algorithm is illustrated with simulation results under different fading scenarios.",
      "fieldsOfStudy": [
        "Computer Science",
        "Mathematics"
      ],
      "topics": [
        "coherent detections",
        "channel state information",
        "Conventional differential detection",
        "detection",
        "different fading scenarios",
        "high throughput",
        "received signals",
        "relay networks",
        "two-symbol non-coherent detection",
        "simulation results",
        "MSDD",
        "CDD",
        "noise",
        "irreducible error floor",
        "fast-fading channels",
        "slow-fading channels"
      ]
    }
  },
  {
    "sim": 0.44743636473334836,
    "gen": {
      "title": "Robust Sparse Analysis Regularization",
      "url": "https://www.semanticscholar.org/paper/7e7b3a0d014fe5686c2ac1dd85e68efe6fd96fb4",
      "abstract": "This paper investigates the theoretical guarantees of l1-analysis regularization when solving linear inverse problems. Most of previous works in the literature have mainly focused on the sparse synthesis prior where the sparsity is measured as the l1 norm of the coefficients that synthesize the signal from a given dictionary. In contrast, the more general analysis regularization minimizes the l1 norm of the correlations between the signal and the atoms in the dictionary, where these correlations define the analysis support. The corresponding variational problem encompasses several well-known regularizations such as the discrete total variation and the fused Lasso. Our main contributions consist in deriving sufficient conditions that guarantee exact or partial analysis support recovery of the true signal in presence of noise. More precisely, we give a sufficient condition to ensure that a signal is the unique solution of the l1 -analysis regularization in the noiseless case. The same condition also guarantees exact analysis support recovery and l2-robustness of the l1-analysis minimizer vis-a\u0300-vis an enough small noise in the measurements. This condition turns to be sharp for the robustness of the sign pattern. To show partial support recovery and l2 -robustness to an arbitrary bounded noise, we introduce a stronger sufficient condition. When specialized to the l1-synthesis regularization, our results recover some corresponding recovery and robustness guarantees previously known in the literature. From this perspective, our work is a generalization of these results. We finally illustrate these theoretical findings on several examples to study the robustness of the 1-D total variation, shift-invariant Haar dictionary, and fused Lasso regularizations.",
      "fieldsOfStudy": [
        "Computer Science",
        "Mathematics"
      ],
      "topics": [
        "Lasso regularizations",
        "partial support recovery",
        "linear inverse problems",
        "noise",
        "l1 -analysis regularization",
        "Lasso",
        "exact or partial analysis support recovery",
        "l2 -robustness",
        "examples",
        "stronger sufficient condition",
        "l1-synthesis regularization",
        "shift-invariant Haar dictionary",
        "fused Lasso regularizations",
        "general analysis regularization"
      ]
    },
    "org": {
      "title": "The $\\mathbb {Z}_2$-Genus of Kuratowski Minors",
      "url": "https://www.semanticscholar.org/paper/989a5bdd2740f4be6f6610d35efe870bf7951870",
      "abstract": null,
      "fieldsOfStudy": [
        "Mathematics",
        "Computer Science"
      ]
    }
  },
  null,
  {
    "sim": 0.553547452782316,
    "gen": {
      "title": "Using Spectral Radius Ratio for Node Degree to Analyze the Evolution of Complex Networks",
      "url": "https://www.semanticscholar.org/paper/01a61b209b52b0a0f49ef426bfa17ddde28ccc8c",
      "abstract": "In this paper, we show that the spectral radius ratio for node degree could be used to analyze the variation of node degree during the evolution of complex networks. We focus on three commonly studied models of complex networks: random networks, scale-free networks and small-world networks. The spectral radius ratio for node degree is defined as the ratio of the principal (largest) eigenvalue of the adjacency matrix of a network graph to that of the average node degree. During the evolution of each of the above three categories of networks (using the appropriate evolution model for each category), we observe the spectral radius ratio for node degree to exhibit high-very high positive correlation (0.75 or above) to that of the coefficient of variation of node degree (ratio of the standard deviation of node degree and average node degree). We show that the spectral radius ratio for node degree could be used as the basis to tune the operating parameters of the evolution models for each of the three categories of complex networks as well as analyze the impact of specific operating parameters for each model.",
      "fieldsOfStudy": null,
      "topics": [
        "node degree",
        "complex networks",
        "random networks",
        "networks",
        "specific operating parameters",
        "small-world networks",
        "average node degree",
        "scale-free networks",
        "ratio",
        "network graph",
        "appropriate evolution model",
        "variation",
        "evolution models",
        "The spectral radius ratio"
      ]
    },
    "org": {
      "title": "Distributed Asynchronous Algorithms for Solving Positive Definite Linear Equations over Dynamic Networks",
      "url": "https://www.semanticscholar.org/paper/4fdc2e1a08209fbac84fcf97767a7eced95b09b8",
      "abstract": "This paper develops Subset Equalizing (SE), a distributed algorithm for solving a symmetric positive definite system of linear equations over a network of agents with arbitrary asynchronous interactions and membership dynamics, where each agent may join and leave the network at any time, for infinitely many times, and may lose all its memory upon leaving. To design and analyze SE, we introduce a time-varying Lyapunov-like function, defined on a state space with changing dimension, and a generalized concept of network connectivity, capable of handling such interactions and membership dynamics. Based on them, we establish the boundedness, asymptotic convergence, and exponential convergence of SE, along with a bound on its convergence rate. Finally, through extensive simulation, we demonstrate the effectiveness of SE in a volatile agent network and show that a special case of SE, termed Groupwise Equalizing, is significantly more bandwidth/energy efficient than two existing algorithms in multi-hop wireless networks.",
      "fieldsOfStudy": [
        "Computer Science",
        "Mathematics"
      ],
      "topics": [
        "network connectivity",
        "multi-hop wireless networks",
        "membership dynamics",
        "arbitrary asynchronous interactions",
        "agents",
        "interactions",
        "volatile agent network",
        "SE",
        "changing dimension",
        "exponential convergence",
        "Groupwise Equalizing",
        "asymptotic convergence",
        "infinitely many times",
        "linear equations",
        "network"
      ]
    }
  },
  {
    "sim": 0.4134561288062486,
    "gen": {
      "title": "AI 2004: Advances in Artificial Intelligence, 17th Australian Joint Conference on Artificial Intelligence, Cairns, Australia, December 4-6, 2004, Proceedings",
      "url": "https://www.semanticscholar.org/paper/612b052ac49aefad99cb3153de691ba3a6c41057",
      "abstract": null,
      "fieldsOfStudy": [
        "Computer Science",
        "Engineering"
      ]
    },
    "org": {
      "title": "Fast and Tiny Structural Self-Indexes for XML",
      "url": "https://www.semanticscholar.org/paper/90ba9a3d68fe7af8db3ae40a4459c7c25ff02285",
      "abstract": "XML document markup is highly repetitive and therefore well compressible using dictionary-based methods such as DAGs or grammars. In the context of selectivity estimation, grammar-compressed trees were used before as synopsis for structural XPath queries. Here a fully-fledged index over such grammars is presented. The index allows to execute arbitrary tree algorithms with a slow-down that is comparable to the space improvement. More interestingly, certain algorithms execute much faster over the index (because no decompression occurs). E.g., for structural XPath count queries, evaluating over the index is faster than previous XPath implementations, often by two orders of magnitude. The index also allows to serialize XML results (including texts) faster than previous systems, by a factor of ca. 2-3. This is due to efficient copy handling of grammar repetitions, and because materialization is totally avoided. In order to compare with twig join implementations, we implemented a materializer which writes out pre-order numbers of result nodes, and show its competitiveness.",
      "fieldsOfStudy": [
        "Computer Science"
      ],
      "topics": [
        "previous XPath implementations",
        "grammars",
        "pre-order numbers",
        "grammar repetitions",
        "structural XPath count queries",
        "grammars",
        "previous systems",
        "result nodes",
        "order",
        "arbitrary tree algorithms",
        "XPath",
        "XML results",
        "implementations",
        "twig join implementations",
        "grammar-compressed trees"
      ]
    }
  },
  {
    "sim": 0.41893124029850415,
    "gen": {
      "title": "A Multi-View Embedding Space for Modeling Internet Images, Tags, and Their Semantics",
      "url": "https://www.semanticscholar.org/paper/7fceccc7a1046caa4936b14eeacb71ccf4d6be10",
      "abstract": null,
      "fieldsOfStudy": [
        "Mathematics",
        "Computer Science"
      ]
    },
    "org": {
      "title": "Understanding and Stabilizing GANs' Training Dynamics with Control Theory",
      "url": "https://www.semanticscholar.org/paper/466f2700541252556dea82ec3ba625c6e7a61c29",
      "abstract": "Generative adversarial networks (GANs) are effective in generating realistic images but the training is often unstable. There are existing efforts that model the training dynamics of GANs in the parameter space but the analysis cannot directly motivate practically effective stabilizing methods. To this end, we present a conceptually novel perspective from control theory to directly model the dynamics of GANs in the function space and provide simple yet effective methods to stabilize GANs' training. We first analyze the training dynamic of a prototypical Dirac GAN and adopt the widely-used closed-loop control (CLC) to improve its stability. We then extend CLC to stabilize the training dynamic of normal GANs, where CLC is implemented as a squared $L2$ regularizer on the output of the discriminator. Empirical results show that our method can effectively stabilize the training and obtain state-of-the-art performance on data generation tasks.",
      "fieldsOfStudy": [
        "Computer Science",
        "Mathematics"
      ],
      "topics": [
        "data generation tasks",
        "normal GANs",
        "GANs",
        "control theory",
        "CLC",
        "practically effective stabilizing methods",
        "realistic images",
        "GANs training",
        "Dirac GAN",
        "training dynamics",
        "simple yet effective methods",
        "Generative adversarial networks",
        "training",
        "existing efforts",
        "prototypical Dirac GAN"
      ]
    }
  },
  {
    "sim": 0.37946321795132765,
    "gen": {
      "title": "Task Agnostic Robust Learning on Corrupt Outputs by Correlation-Guided Mixture Density Networks",
      "url": "https://www.semanticscholar.org/paper/721bfac34c6b1e0a8f5620b33df96da0c8e5dd40",
      "abstract": "In this paper, we focus on weakly supervised learning with noisy training data for both classification and regression problems. We assume that the training outputs are collected from a mixture of a target and correlated noise distributions. Our proposed method simultaneously estimates the target distribution and the quality of each data which is defined as the correlation between the target and data generating distributions. The cornerstone of the proposed method is a Cholesky Block that enables modeling dependencies among mixture distributions in a differentiable manner where we maintain the distribution over the network weights. We first provide illustrative examples in both regression and classification tasks to show the effectiveness of the proposed method. Then, the proposed method is extensively evaluated in a number of experiments where we show that it constantly shows comparable or superior performances compared to existing baseline methods in the handling of noisy data.",
      "fieldsOfStudy": [
        "Computer Science",
        "Mathematics"
      ],
      "topics": [
        "correlated noise distributions",
        "noisy data",
        "noisy training data",
        "existing baseline methods",
        "classification tasks",
        "target and data generating distributions",
        "distribution",
        "network weights",
        "weakly supervised learning",
        "data",
        "dependencies",
        "Our proposed method",
        "proposed method",
        "experiments",
        "comparable or superior performances"
      ]
    },
    "org": {
      "title": "Lowest Unique Bid Auctions with Resubmission Opportunities",
      "url": "https://www.semanticscholar.org/paper/ff5b84092a2ea111b791fda4a1ecc447f9ef9be7",
      "abstract": "The recent online platforms propose multiple items for bidding. The state of the art, however, is limited to the analysis of one item auction without resubmission. In this paper we study multi-item lowest unique bid auctions (LUBA) with resubmission in discrete bid spaces under budget constraints. We show that the game does not have pure Bayes-Nash equilibria (except in very special cases). However, at least one mixed Bayes-Nash equilibria exists for arbitrary number of bidders and items. The equilibrium is explicitly computed for two-bidder setup with resubmission possibilities. In the general setting we propose a distributed strategic learning algorithm to approximate equilibria. Computer simulations indicate that the error quickly decays in few number of steps. When the number of bidders per item follows a Poisson distribution, it is shown that the seller can get a non-negligible revenue on several items, and hence making a partial revelation of the true value of the items. Finally, the attitude of the bidders towards the risk is considered. In contrast to risk-neutral agents who bids very small values, the cumulative distribution and the bidding support of risk-sensitive agents are more distributed.",
      "fieldsOfStudy": [
        "Computer Science",
        "Mathematics",
        "Economics"
      ],
      "topics": [
        "items",
        "items",
        "multiple items",
        "discrete bid spaces",
        "approximate equilibria",
        "budget constraints",
        "resubmission possibilities",
        "multi-item lowest unique bid auctions",
        "resubmission",
        "bidding",
        "arbitrary number",
        "number",
        "bidders",
        "item auction",
        "equilibria",
        "risk-sensitive agents"
      ]
    }
  },
  null,
  {
    "sim": 0.7196581760966752,
    "gen": {
      "title": "Scheduling Status Update for Optimizing Age of Information in the Context of Industrial Cyber-Physical System",
      "url": "https://www.semanticscholar.org/paper/9220d4cc2adc1d87e3818d2adf1dad20966f5da4",
      "abstract": "Age of Information has been emerged as an interesting metric in real-time wireless networks that captures the freshness of information in the underlying applications. This topic is motivated by the problem in which the users of a network care about timely information defined as the age of the most recent status update a user has received. In the proposed work, we have studied this concept in the context of an industrial wireless sensor-actuator network for cyber-physical production systems. Such a network with ever-changing dynamics requires continuous updates of the system states and ongoing tasks by exchanging the time-critical, event-driven, and/or time-driven information to maintain the stability of the system, failing which may lead to shut down of the plant and other fatal consequences. Different real-time transmission scheduling algorithms manage how the channel resources are allocated each time depending on the packet arrivals to minimize the age of the information. However, unlike other real-time networks like broadcasting and sensor networks, etc. in cyber-physical systems, cyber and physical devices have different requirements to improve their quality of performance. Balancing between the performance criteria of both cyber and physical units seems to be a great challenge. In this work, the minimization of staleness of the real-time updates by minimizing the age of information and their effect on network performance have been studied extensively for this purpose. Two greedy scheduling policies have been proposed: one for the total age minimization and another one for stale age and jitter minimization. Their performances and complexities are compared with other existing scheduling algorithms. Moreover, the optimality of each of our proposed algorithms are proved analytically and claims are validated via simulation results too. Eventually, these results come to a conclusion that minimum age does not always guarantee the maximum freshness of information and satisfactory system performance at the same time.",
      "fieldsOfStudy": [
        "Computer Science"
      ],
      "topics": [
        "network performance",
        "satisfactory system performance",
        "minimum age",
        "stale age",
        "timely information",
        "sensor networks",
        "Age",
        "information",
        "existing scheduling algorithms",
        "fatal consequences",
        "real-time networks",
        "performance",
        "real-time wireless networks",
        "Different real-time transmission scheduling algorithms",
        "jitter minimization",
        "cyber-physical production systems"
      ]
    },
    "org": {
      "title": "Two timescale convergent Q-learning for sleep-scheduling in wireless sensor networks",
      "url": "https://www.semanticscholar.org/paper/1b7ca0bc530041cf7975570a2622855d2a3ad5ea",
      "abstract": null,
      "fieldsOfStudy": [
        "Computer Science"
      ]
    }
  },
  {
    "sim": 0.5546700248637177,
    "gen": {
      "title": "Learning to Optimize Tensor Programs",
      "url": "https://www.semanticscholar.org/paper/cb91c2f8d3cac0b655a39be318b603334eb18987",
      "abstract": "We introduce a learning-based framework to optimize tensor programs for deep learning workloads. Efficient implementations of tensor operators, such as matrix multiplication and high dimensional convolution, are key enablers of effective deep learning systems. However, existing systems rely on manually optimized libraries such as cuDNN where only a narrow range of server class GPUs are well-supported. The reliance on hardware-specific operator libraries limits the applicability of high-level graph optimizations and incurs significant engineering costs when deploying to new hardware targets. We use learning to remove this engineering burden. We learn domain-specific statistical cost models to guide the search of tensor operator implementations over billions of possible program variants. We further accelerate the search by effective model transfer across workloads. Experimental results show that our framework delivers performance competitive with state-of-the-art hand-tuned libraries for low-power CPU, mobile GPU, and server-class GPU.",
      "fieldsOfStudy": [
        "Computer Science",
        "Mathematics"
      ],
      "topics": [
        "deep learning workloads",
        "mobile GPU",
        "effective deep learning systems",
        "GPU",
        "significant engineering costs",
        "tensor operator implementations",
        "new hardware targets",
        "tensor operators",
        "tensor programs",
        "high dimensional convolution",
        "server-class GPU",
        "effective model transfer",
        "possible program variants",
        "workloads",
        "learning"
      ]
    },
    "org": {
      "title": "IRLAS: Inverse Reinforcement Learning for Architecture Search",
      "url": "https://www.semanticscholar.org/paper/f8be3ede4a63e67dc2cf0c5a03cf7e3005f78782",
      "abstract": "In this paper, we propose an inverse reinforcement learning method for architecture search (IRLAS), which trains an agent to learn to search network structures that are topologically inspired by human-designed network. Most existing architecture search approaches totally neglect the topological characteristics of architectures, which results in complicated architecture with a high inference latency. Motivated by the fact that human-designed networks are elegant in topology with a fast inference speed, we propose a mirror stimuli function inspired by biological cognition theory to extract the abstract topological knowledge of an expert human-design network (ResNeXt). To avoid raising a too strong prior over the search space, we introduce inverse reinforcement learning to train the mirror stimuli function and exploit it as a heuristic guidance for architecture search, easily generalized to different architecture search algorithms. On CIFAR-10, the best architecture searched by our proposed IRLAS achieves 2.60% error rate. For ImageNet mobile setting, our model achieves a state-of-the-art top-1 accuracy 75.28%, while being 2~4x faster than most auto-generated architectures. A fast version of this model achieves 10% faster than MobileNetV2, while maintaining a higher accuracy.",
      "fieldsOfStudy": [
        "Computer Science"
      ],
      "topics": [
        "different architecture search algorithms",
        "complicated architecture",
        "architectures",
        "Most existing architecture search approaches",
        "network structures",
        "auto-generated architectures",
        "inverse reinforcement learning",
        "biological cognition theory",
        "best architecture",
        "human-designed networks",
        "ResNeXt",
        "high inference latency",
        "topology",
        "accuracy",
        "fast inference speed",
        "IRLAS"
      ]
    }
  },
  {
    "sim": 0.42896418814522064,
    "gen": {
      "title": "Discovering Small Target Sets in Social Networks: A Fast and Effective Algorithm",
      "url": "https://www.semanticscholar.org/paper/eb401ed829ae032b01ceeb4d96e752303b6b47d9",
      "abstract": null,
      "fieldsOfStudy": [
        "Computer Science",
        "Mathematics",
        "Physics"
      ]
    },
    "org": {
      "title": "Compression Aware Physical Database Design",
      "url": "https://www.semanticscholar.org/paper/67ac66e8b0894b5550f78bbc3e70375ac8f27c31",
      "abstract": "Modern RDBMSs support the ability to compress data using methods such as null suppression and dictionary encoding. Data compression offers the promise of significantly reducing storage requirements and improving I/O performance for decision support queries. However, compression can also slow down update and query performance due to the CPU costs of compression and decompression. In this paper, we study how data compression affects choice of appropriate physical database design, such as indexes, for a given workload. We observe that approaches that decouple the decision of whether or not to choose an index from whether or not to compress the index can result in poor solutions. Thus, we focus on the novel problem of integrating compression into physical database design in a scalable manner. We have implemented our techniques by modifying Microsoft SQL Server and the Database Engine Tuning Advisor (DTA) physical design tool. Our techniques are general and are potentially applicable to DBMSs that support other compression methods. Our experimental results on real world as well as TPC-H benchmark workloads demonstrate the effectiveness of our techniques.",
      "fieldsOfStudy": [
        "Computer Science"
      ],
      "topics": [
        "appropriate physical database design",
        "compression methods",
        "query performance",
        "decision support queries",
        "dictionary encoding",
        "compression",
        "poor solutions",
        "Microsoft SQL Server",
        "indexes",
        "null suppression",
        "performance",
        "storage requirements",
        "SQL Server",
        "methods"
      ]
    }
  },
  {
    "sim": 0.6555263988231265,
    "gen": {
      "title": "On Multiagent Q-Learning in a Semi-Competitive Domain",
      "url": "https://www.semanticscholar.org/paper/fa466323f685599a7ae5b40d0ca921cb9a2c0a0d",
      "abstract": null,
      "fieldsOfStudy": [
        "Computer Science"
      ]
    },
    "org": {
      "title": "From Poincar\u00e9 Recurrence to Convergence in Imperfect Information Games: Finding Equilibrium via Regularization",
      "url": "https://www.semanticscholar.org/paper/af111ed91c177a14d403fce77e1c9c2fb99b0061",
      "abstract": "In this paper we investigate the Follow the Regularized Leader dynamics in sequential imperfect information games (IIG). We generalize existing results of Poincare recurrence from normal-form games to zero-sum two-player imperfect information games and other sequential game settings. We then investigate how adapting the reward (by adding a regularization term) of the game can give strong convergence guarantees in monotone games. We continue by showing how this reward adaptation technique can be leveraged to build algorithms that converge exactly to the Nash equilibrium. Finally, we show how these insights can be directly used to build state-of-the-art model-free algorithms for zero-sum two-player Imperfect Information Games (IIG).",
      "fieldsOfStudy": [
        "Computer Science",
        "Mathematics"
      ],
      "topics": [
        "sequential game settings",
        "monotone games",
        "IIG",
        "strong convergence guarantees",
        "Imperfect Information Games",
        "normal-form games",
        "game",
        "algorithms",
        "Poincare recurrence",
        "existing results",
        "Nash equilibrium",
        "zero-sum two-player Imperfect Information Games",
        "Poincare",
        "reward adaptation technique"
      ]
    }
  },
  {
    "sim": 0.5207702954699072,
    "gen": {
      "title": "Dependable Demand Response Management in the Smart Grid: A Stackelberg Game Approach",
      "url": "https://www.semanticscholar.org/paper/c5fd1605484f6b15b344695749c9178423185fb1",
      "abstract": "Demand Response Management (DRM) is a key component in the smart grid to effectively reduce power generation costs and user bills. However, it has been an open issue to address the DRM problem in a network of multiple utility companies and consumers where every entity is concerned about maximizing its own benefit. In this paper, we propose a Stackelberg game between utility companies and end-users to maximize the revenue of each utility company and the payoff of each user. We derive analytical results for the Stackelberg equilibrium of the game and prove that a unique solution exists. We develop a distributed algorithm which converges to the equilibrium with only local information available for both utility companies and end-users. Though DRM helps to facilitate the reliability of power supply, the smart grid can be succeptible to privacy and security issues because of communication links between the utility companies and the consumers. We study the impact of an attacker who can manipulate the price information from the utility companies. We also propose a scheme based on the concept of shared reserve power to improve the grid reliability and ensure its dependability.",
      "fieldsOfStudy": [
        "Computer Science"
      ],
      "topics": [
        "user bills",
        "utility companies",
        "utility company",
        "utility companies",
        "power generation costs",
        "shared reserve power",
        "power supply",
        "consumers",
        "local information",
        "end-users",
        "communication links",
        "benefit",
        "user"
      ]
    },
    "org": {
      "title": "Jeux stochastiques et contr\u00f4le de puissance distribu\u00e9",
      "url": "https://www.semanticscholar.org/paper/520370145240c2885dbd3c4632144c998ba86a39",
      "abstract": "Transmitters of a multiple access channel are assumed to freely choose their power control strategy in order to be energy-efficient. We show that in a stochastic game framework, we can develop energy-efficient distributed control strategies which only require partial knowledge of the entire system. Achievable utility equilibrium region is characterized and based on time-sharing, an explicit power control strategy is proposed.",
      "fieldsOfStudy": [
        "Computer Science",
        "Geography"
      ],
      "topics": [
        "energy-efficient distributed control strategies",
        "partial knowledge",
        "explicit power control strategy",
        "power control strategy",
        "order",
        "entire system",
        "Achievable utility equilibrium region",
        "time-sharing",
        "multiple access channel",
        "stochastic game framework",
        "Transmitters"
      ]
    }
  },
  {
    "sim": 0.5781413325315937,
    "gen": {
      "title": "Fast Algorithms for Intimate-Core Group Search in Weighted Graphs",
      "url": "https://www.semanticscholar.org/paper/7bbd2c39d7b2a51f6a3b5b7e19e4bb6c07cfa187",
      "abstract": null,
      "fieldsOfStudy": [
        "Computer Science"
      ]
    },
    "org": {
      "title": "Finding More Relevance: Propagating Similarity on Markov Random Field for Image Retrieval",
      "url": "https://www.semanticscholar.org/paper/27a55ec8d5642734cf2156d552a6f6c7ad2633a0",
      "abstract": "To effectively retrieve objects from large corpus with high accuracy is a challenge task. In this paper, we propose a method that propagates visual feature level similarities on a Markov random field (MRF) to obtain a high level correspondence in image space for image pairs. The proposed correspondence between image pair reflects not only the similarity of low-level visual features but also the relations built through other images in the database and it can be easily integrated into the existing bag-of-visual-words(BoW) based systems to reduce the missing rate. We evaluate our method on the standard Oxford-5K, Oxford-105K and Paris-6K dataset. The experiment results show that the proposed method significantly improves the retrieval accuracy on three datasets and exceeds the current state-of-the-art retrieval performance.",
      "fieldsOfStudy": [
        "Computer Science"
      ],
      "topics": [
        "visual feature level similarities",
        "image pairs",
        "images",
        "image space",
        "Paris-6K dataset",
        "high accuracy",
        "low-level visual features",
        "high level correspondence",
        "MRF",
        "Markov",
        "missing rate",
        "visual-words(BoW",
        "large corpus",
        "Paris-6",
        "retrieval accuracy"
      ]
    }
  },
  {
    "sim": 0.5035055900139233,
    "gen": {
      "title": "Convolutional Matrix Factorization for Document Context-Aware Recommendation",
      "url": "https://www.semanticscholar.org/paper/af9c4dda90e807246a2f6fa0a922bbf8029767cf",
      "abstract": "Sparseness of user-to-item rating data is one of the major factors that deteriorate the quality of recommender system. To handle the sparsity problem, several recommendation techniques have been proposed that additionally consider auxiliary information to improve rating prediction accuracy. In particular, when rating data is sparse, document modeling-based approaches have improved the accuracy by additionally utilizing textual data such as reviews, abstracts, or synopses. However, due to the inherent limitation of the bag-of-words model, they have difficulties in effectively utilizing contextual information of the documents, which leads to shallow understanding of the documents. This paper proposes a novel context-aware recommendation model, convolutional matrix factorization (ConvMF) that integrates convolutional neural network (CNN) into probabilistic matrix factorization (PMF). Consequently, ConvMF captures contextual information of documents and further enhances the rating prediction accuracy. Our extensive evaluations on three real-world datasets show that ConvMF significantly outperforms the state-of-the-art recommendation models even when the rating data is extremely sparse. We also demonstrate that ConvMF successfully captures subtle contextual difference of a word in a document. Our implementation and datasets are available at http://dm.postech.ac.kr/ConvMF.",
      "fieldsOfStudy": [
        "Computer Science"
      ],
      "topics": [
        "rating data",
        "documents",
        "textual data",
        "convolutional matrix factorization",
        "probabilistic matrix factorization",
        "contextual information",
        "recommender system",
        "synopses",
        "document modeling-based approaches",
        "convolutional neural network",
        "shallow understanding",
        "auxiliary information",
        "rating prediction accuracy",
        "recommendation techniques"
      ]
    },
    "org": {
      "title": "UBSegNet: Unified Biometric Region of Interest Segmentation Network",
      "url": "https://www.semanticscholar.org/paper/95f326edda47d0b659d9de02bb5288ec4d3eea8b",
      "abstract": "Digital human identity management, can now be seen as a social necessity, as it is essentially required in almost every public sector such as, financial inclusions, security, banking, social networking e.t.c. Hence, in today's rampantly emerging world with so many adversarial entities, relying on a single biometric trait is being too optimistic. In this paper, we have proposed a novel end-to-end, Unified Biometric ROI Segmentation Network (U BSegN et), for extracting region of interest from five different biometric traits viz. face, iris, palm, knuckle and 4-slap fingerprint. The architecture of the proposed U BSegN et consists of two stages: (i) Trait classification and (ii) Trait localization. For these stages, we have used a state of the art region based convolutional neural network (RCNN), comprising of three major parts namely convolutional layers, region proposal network (RPN) along with classification and regression heads. The model has been evaluated over various huge publicly available biometric databases. To the best of our knowledge this is the first unified architecture proposed, segmenting multiple biometric traits. It has been tested over around 5000 * 5 = 25, 000 images (5000 images per trait) and produces very good results. Our work on unified biometric segmentation, opens up the vast opportunities in the field of multiple biometric traits based authentication systems.",
      "fieldsOfStudy": [
        "Computer Science"
      ],
      "topics": [
        "multiple biometric traits",
        "Unified Biometric ROI Segmentation Network",
        "convolutional neural network",
        "trait",
        "authentication systems",
        "region",
        "financial inclusions",
        "U BSegN",
        "different biometric traits",
        "single biometric trait",
        "huge publicly available biometric databases",
        "et",
        "classification and regression heads",
        "security, banking, social networking e.t.c",
        "multiple biometric traits based authentication systems",
        "region proposal network",
        "social networking",
        "U BSegN et",
        "banking",
        "security"
      ]
    }
  },
  {
    "sim": 0.4822825517743061,
    "gen": {
      "title": "Quantifying Location Privacy",
      "url": "https://www.semanticscholar.org/paper/d33a63cb8ae09a51b823ef1d0ac34acc3dea8ece",
      "abstract": "It is a well-known fact that the progress of personal communication devices leads to serious concerns about privacy in general, and location privacy in particular. As a response to these issues, a number of Location-Privacy Protection Mechanisms (LPPMs) have been proposed during the last decade. However, their assessment and comparison remains problematic because of the absence of a systematic method to quantify them. In particular, the assumptions about the attacker's model tend to be incomplete, with the risk of a possibly wrong estimation of the users' location privacy. In this paper, we address these issues by providing a formal framework for the analysis of LPPMs, it captures, in particular, the prior information that might be available to the attacker, and various attacks that he can perform. The privacy of users and the success of the adversary in his location-inference attacks are two sides of the same coin. We revise location privacy by giving a simple, yet comprehensive, model to formulate all types of location-information disclosure attacks. Thus, by formalizing the adversary's performance, we propose and justify the right metric to quantify location privacy. We clarify the difference between three aspects of the adversary's inference attacks, namely their accuracy, certainty, and correctness. We show that correctness determines the privacy of users. In other words, the expected estimation error of the adversary is the metric of users' location privacy. We rely on well-established statistical methods to formalize and implement the attacks in a tool: the Location-Privacy Meter that measures the location privacy of mobile users, given various LPPMs. In addition to evaluating some example LPPMs, by using our tool, we assess the appropriateness of some popular metrics for location privacy: entropy and k-anonymity. The results show a lack of satisfactory correlation between these two metrics and the success of the adversary in inferring the users' actual locations.",
      "fieldsOfStudy": [
        "Computer Science"
      ],
      "topics": [
        "privacy",
        "attacks",
        "mobile users",
        "users",
        "LPPMs",
        "location-information disclosure attacks",
        "users location privacy",
        "LPPMs",
        "correctness",
        "location-inference attacks",
        "users actual locations",
        "The privacy",
        "location privacy"
      ]
    },
    "org": {
      "title": "The robust bilevel continuous knapsack problem with uncertain coefficients in the follower\u2019s objective",
      "url": "https://www.semanticscholar.org/paper/0fcbd633be11f9345598639d5a9809dee3fa594b",
      "abstract": null,
      "fieldsOfStudy": [
        "Computer Science",
        "Mathematics"
      ]
    }
  },
  null,
  {
    "sim": 0.4499353836340336,
    "gen": {
      "title": "Continuous Result Delta Evaluation of IR Systems",
      "url": "https://www.semanticscholar.org/paper/8b2ca217b87cbf83be51c13b3a86b3be512b0cde",
      "abstract": "Classical evaluation of information retrieval systems evaluates a system in a static test collection. In the case of Web search, the evaluation environment (EE) is continuously changing and the hypothesis of using a static test collection is not representative of this changing reality. Moreover, the changes in the evaluation environment, as the document set, the topics set, the relevance judgments, and the chosen metrics, have an impact on the performance measurement [1, 4]. To the best of our knowledge, there is no way to evaluate two versions of a search engine with evolving EEs. We aim at proposing a continuous framework to evaluate different versions of a search engine in different evaluation environments. The classical paradigm relies on a controlled test collection (i.e., set of topics, corpus of documents and relevant assessments) as a stable and meaningful EE that guarantees the reproducibility of system results. We define the different EEs as a dynamic test collection (DTC). A DTC is a list of test collections based on a controlled evolution of a static test collection. The DTC allows us to quantify and relate the differences between the test collection elements, called Knowledge delta (K)\u0394, and the performance differences between systems evaluated on these varying test collections, called Result delta (R)\u0394. Finally, the continuous evaluation is characterized by K\u0394s and R\u0394s. The related changes in both deltas will allow for interpreting the evaluations in systems performances. The expected contributions of the thesis are: (i) a pivot strategy based on R\u0394 to compare systems evaluated in different EEs; (ii) a formalization of DTC to simulate the continuous evaluation and provide significant R\u0394 in evolving contexts; and (iii) a continuous evaluation framework that incorporates K\u0394 to explain R\u0394 of evaluated systems. It is not possible to measure the R\u0394 of two systems evaluated in different EEs, because the performance variations are dependent on the changes in the EEs. [1]. To get an estimation of this R\u0394 measure, we propose to use a reference system, called the pivot system, which would be evaluated within the two EEs considered. Then, the R\u0394 value is measured using the relative distance between the pivot system and each evaluated system. Our results [2, 3] show that using the pivot strategy we improve the correctness of the ranking of systems (RoS) evaluated in two EEs (i.e., similarity with the RoS evaluated in the ground truth), compared to the RoS constructed with the absolute performance values for each system evaluated in the different EEs. The correctness of the RoS depends on the system defined as pivot and the metric. The proposal focus moves to a continuous evaluation as a repeated assessment of the same or different versions of a web search across evolving EEs. Current test collections do not consider the evolution of documents, topics and relevance judgements. We require a DTC to extract R\u0394s of the compared system and its relation with the changes on the EEs (K\u0394). We provide a method to define a DTC from static test collections based on controlled features as a way to better simulate the evolving EE. According to our preliminary experiments, a system evaluated in our proposed DTC shows more variable performances, and larger R\u0394s, than when it is evaluated in several random shards or bootstraps of documents. As future work, we will integrate the K\u0394s to formalize an explainable continuous evaluation framework. The pivot strategy tells us when the performance of the system is improving across EEs. The DTC provides us with the required EEs to identify significant R\u0394s, and the inclusion of K\u0394s in the framework will define a set of factors that explain the system's performance changes.",
      "fieldsOfStudy": null,
      "topics": [
        "systems performances",
        "evaluated systems",
        "systems",
        "system results",
        "different EEs",
        "information retrieval systems",
        "EEs",
        "Current test collections",
        "different evaluation environments",
        "variable performances",
        "different versions",
        "Classical evaluation",
        "documents",
        "evolving EEs",
        "significant R\u0394s"
      ]
    },
    "org": {
      "title": "Research Data Explored II: the Anatomy and Reception of figshare",
      "url": "https://www.semanticscholar.org/paper/1c9803432191dde8edae8c712d5632e66913c2d9",
      "abstract": "This is the second paper in a series of bibliometric studies of research data. In this paper, we present an analysis of figshare, one of the largest multidisciplinary repositories for research materials to date. We analysed the structure of items archived in figshare, their usage, and their reception in two altmetrics sources (PlumX and ImpactStory). We found that figshare acts (1) as a personal repository for yet unpublished materials, (2) as a platform for newly published research materials, and (3) as an archive for PLOS. Depending on the function, we found different bibliometric characteristics. Items archived from PLOS tend to be coming from the natural sciences and are often unviewed and non-downloaded. Self-archived items, however, come from a variety of disciplines and exhibit some patterns of higher usage. In the altmetrics analysis, we found that Twitter was the social media service where research data gained most attention; generally, research data published in 2014 were most popular across social media services. PlumX detects considerably more items in social media and also finds higher altmetric scores than ImpactStory.",
      "fieldsOfStudy": [
        "Computer Science"
      ],
      "topics": [
        "research data",
        "research materials",
        "social media",
        "higher altmetric scores",
        "ImpactStory",
        "higher usage",
        "attention",
        "newly published research materials",
        "PLOS",
        "different bibliometric characteristics",
        "social media service",
        "bibliometric studies",
        "items"
      ]
    }
  },
  {
    "sim": 0.33446464494359307,
    "gen": {
      "title": "Censer: Curriculum Semi-supervised Learning for Speech Recognition Based on Self-supervised Pre-training",
      "url": "https://www.semanticscholar.org/paper/a3da80679c02a3451d730fb98f63a2d77056d533",
      "abstract": "Recent studies have shown that the benefits provided by selfsupervised pre-training and self-training (pseudo-labeling) are complementary. Semi-supervised fine-tuning strategies under the pre-training framework, however, remain insufficiently studied. Besides, modern semi-supervised speech recognition algorithms either treat unlabeled data indiscriminately or filter out noisy samples with a confidence threshold. The dissimilarities among different unlabeled data are often ignored. In this paper, we propose Censer, a semi-supervised speech recognition algorithm based on self-supervised pre-training to maximize the utilization of unlabeled data. The pre-training stage of Censer adopts wav2vec2.0 and the fine-tuning stage employs an improved semisupervised learning algorithm from slimIPL, which leverages unlabeled data progressively according to their pseudo labels\u2019 qualities. We also incorporate a temporal pseudo label pool and an exponential moving average to control the pseudo labels\u2019 update frequency and to avoid model divergence. Experimental results on Libri-Light and LibriSpeech datasets manifest our proposed method achieves better performance compared to existing approaches while being more unified.",
      "fieldsOfStudy": [
        "Computer Science",
        "Engineering"
      ],
      "topics": [
        "unlabeled data",
        "different unlabeled data",
        "model divergence",
        "existing approaches",
        "noisy samples",
        "training",
        "temporal pseudo label pool",
        "better performance",
        "Semi-supervised fine-tuning strategies",
        "semi-supervised speech recognition algorithm",
        "algorithm",
        "The pre-training stage",
        "pseudo labels\u2019 update frequency",
        "pre-training framework",
        "pseudo-labeling",
        "selfsupervised pre",
        "update frequency"
      ]
    },
    "org": {
      "title": "Machine Learning Based IoT Intrusion Detection System: An MQTT Case Study",
      "url": "https://www.semanticscholar.org/paper/0d60465e40e67ba7361a27e1b9f49540d7cba391",
      "abstract": "The Internet of Things (IoT) is one of the main research fields in the Cybersecurity domain. This is due to (a) the increased dependency on automated device, and (b) the inadequacy of general purpose Intrusion Detection Systems (IDS) to be deployed for special purpose networks usage. Numerous lightweight protocols are being proposed for IoT devices communication usage. One of the recent IoT machine-to-machine communication protocols is Message Queuing Telemetry Transport (MQTT) protocol. However, as per the authors best knowledge, there are no available IDS datasets that include MQTT benign or attack instances and thus, no IDS experimental results available. In this paper, we evaluate the effectiveness of six Machine Learning (ML) techniques to detect MQTT-based attacks. Three abstraction levels of features are assessed, namely, packet-based, uni-directional flow, and bidirectional flow features. An MQTT simulated dataset is generated and used for the training and evaluation processes. The dataset is released with an open access licence to help the research community further analyse the challenges. The experimental results demonstrated the adequacy of the proposed ML models to suit MQTT-based networks IDS requirements. Moreover, the results emphasise on the importance of using flow-based features to discriminate MQTT-based attacks from benign traffic, while packet-based features are sufficient for traditional networking attacks",
      "fieldsOfStudy": [
        "Computer Science"
      ],
      "topics": [
        "bidirectional flow features",
        "traditional networking attacks",
        "special purpose networks usage",
        "MQTT-based networks IDS requirements",
        "MQTT",
        "IDS",
        "MQTT-based attacks",
        "features",
        "general purpose Intrusion Detection Systems",
        "IoT devices communication usage",
        "benign traffic",
        "flow-based features",
        "packet-based features",
        "Message Queuing Telemetry Transport",
        "Intrusion Detection Systems",
        "MQTT benign or attack instances",
        "Numerous lightweight protocols",
        "IDS experimental results",
        "available IDS datasets"
      ]
    }
  },
  {
    "sim": 0.23013895330306822,
    "gen": {
      "title": "Resource Allocation for IRS-Enabled Secure Multiuser Multi-Carrier Downlink URLLC Systems",
      "url": "https://www.semanticscholar.org/paper/b817fec2702ce9fb715cb360fad48a5294a7f059",
      "abstract": "Secure ultra-reliable low-latency communication (URLLC) has been recently investigated with the fundamental limits of finite block length (FBL) regime in mind. Analysis has revealed that when eavesdroppers outnumber BS antennas or enjoy a more favorable channel condition compared to the legitimate users, base station (BS) transmit power should increase exorbitantly to meet quality of service (QoS) constraints. Channel-induced impairments such as shadowing and/or blockage pose a similar challenge. These practical considerations can drastically limit secure URLLC performance in FBL regime. Deployment of an intelligent reflecting surface (IRS) can endow such systems with much-needed resiliency and robustness to satisfy stringent latency, availability, and reliability requirements. We address this problem and propose to minimize the total BS transmit power by simultaneously designing the beamformers and artificial noise at the BS and phase-shifts at the IRS, while guaranteeing the required number of securely transmitted bits with the desired packet error probability, information leakage, and maximum affordable delay. The proposed optimization problem is non-convex and we apply block coordinate descent and successive convex approximation to iteratively solve a series of convex sub-problems instead. The proposed algorithm converges to a sub-optimal solution in a few iterations and attains substantial power saving and robustness compared to baseline schemes.",
      "fieldsOfStudy": [
        "Engineering",
        "Computer Science"
      ],
      "topics": [
        "maximum affordable delay",
        "FBL regime",
        "substantial power saving",
        "successive convex approximation",
        "mind",
        "reliability requirements",
        "block coordinate descent",
        "power",
        "problems",
        "BS antennas",
        "baseline schemes",
        "information leakage",
        "stringent latency",
        "base station",
        "convex sub",
        "BS",
        "total BS transmit power",
        "artificial noise",
        "systems"
      ]
    },
    "org": {
      "title": "Personalized news recommendation with context trees",
      "url": "https://www.semanticscholar.org/paper/0aadec9721e54c01d17faa4b6e41e6d7070225d0",
      "abstract": "The proliferation of online news creates a need for filtering interesting articles. Compared to other products, however, recommending news has specific challenges: news preferences are subject to trends, users do not want to see multiple articles with similar content, and frequently we have insufficient information to profile the reader. In this paper, we introduce a class of news recommendation systems based on context trees. They can provide high-quality news recommendations to anonymous visitors based on present browsing behaviour. Using an unbiased testing methodology, we show that they make accurate and novel recommendations, and that they are sufficiently flexible for the challenges of news recommendation.",
      "fieldsOfStudy": [
        "Computer Science",
        "Mathematics"
      ],
      "topics": [
        "news recommendation systems",
        "news preferences",
        "multiple articles",
        "interesting articles",
        "present browsing behaviour",
        "similar content",
        "news",
        "online news",
        "insufficient information",
        "specific challenges",
        "context trees",
        "high-quality news recommendations",
        "anonymous visitors",
        "users"
      ]
    }
  },
  {
    "sim": 0.8055323273880252,
    "gen": {
      "title": "Performance Evaluation of AODV Routing Protocol in MANET using NS-3 Simulator",
      "url": "https://www.semanticscholar.org/paper/ed272d907f14eac4922b7f66a18bbddeed5d622c",
      "abstract": "Network Simulator is a current and ongoing networking tool for research, especially in the mobile networking area. Network simulation simplifies the design and analysis of network protocols such as AODV routing protocol in MANET. NS-3 is one of the active open source discrete event simulators that supports various characteristic of AODV routing protocol in MANET. In this paper, NS-3 was used to evaluate the performance of AODV routing protocol and analyze the influence of mobility speed and node density. The performance parameters used for the measurement are packet delivery ratio (PDR), throughput and average end-to-end delay (AED). The results acquired from the simulation verify that the mobility speed and node density affect the efficiency of AODV in MANET",
      "fieldsOfStudy": [
        "Computer Science"
      ],
      "topics": [
        "AODV routing protocol",
        "AODV",
        "MANET",
        "network protocols",
        "node density",
        "mobility speed",
        "AED",
        "end",
        "characteristic",
        "packet delivery ratio",
        "Network simulation",
        "mobile networking area",
        "research",
        "throughput",
        "PDR"
      ]
    },
    "org": {
      "title": "Evaluation and Performance of Reactive Protocols Using Mobility Model",
      "url": "https://www.semanticscholar.org/paper/f8e6109659a666b2d4834f16db16294595676249",
      "abstract": "A Mobile Ad-hoc Network (MANET) is a self-motivated wireless network which has no centralized point. It is an independent network that is connected by wireless link so, in which every point or device work as a router. In this network every node forward the packets to the destination as a router and it's not operating as an ending point. In this network every node adjusts them self by on his way in any direction because they are independent and change their position regularly. There are exist three main types of routing protocols which are reactive, proactive and final is hybrid protocols. This whole work compares the performance of some reactive protocols which also known as on - demand protocols, which are DSR, AODV and the final is AOMDV. DSR and AODV are reactive protocols which connected the devices on the network when needed by a doorway. The AOMDV protocol was designed for ad hoc networks whenever any route or link fail and also maintain routes with sequence numbers to avoid looping.",
      "fieldsOfStudy": [
        "Computer Science"
      ],
      "topics": [
        "hybrid protocols",
        "protocols",
        "device",
        "sequence numbers",
        "reactive protocols",
        "ending point",
        "routes",
        "wireless link",
        "The AOMDV protocol",
        "centralized point",
        "DSR",
        "AOMDV",
        "point",
        "routing protocols",
        "looping",
        "link",
        "router",
        "AODV"
      ]
    }
  },
  {
    "sim": 0.5070308076980411,
    "gen": {
      "title": "CLIPS 6.0 - C LANGUAGE INTEGRATED PRODUCTION SYSTEM, VERSION 6.0 (UNIX VERSION)",
      "url": "https://www.semanticscholar.org/paper/66fe71e3f8dc6c1645f03d3e4a673e28d4840327",
      "abstract": "CLIPS, the C Language Integrated Production System, is a complete environment for developing expert systems -- programs which are specifically intended to model human expertise or knowledge. It is designed to allow artificial intelligence research, development, and delivery on conventional computers. CLIPS 6.0 provides a cohesive tool for handling a wide variety of knowledge with support for three different programming paradigms: rule-based, object-oriented, and procedural. Rule-based programming allows knowledge to be represented as heuristics, or \"rules-of-thumb\" which specify a set of actions to be performed for a given situation. Object-oriented programming allows complex systems to be modeled as modular components (which can be easily reused to model other systems or create new components). The procedural programming capabilities provided by CLIPS 6.0 allow CLIPS to represent knowledge in ways similar to those allowed in languages such as C, Pascal, Ada, and LISP. Using CLIPS 6.0, one can develop expert system software using only rule-based programming, only object-oriented programming, only procedural programming, or combinations of the three. CLIPS provides extensive features to support the rule-based programming paradigm including seven conflict resolution strategies, dynamic rule priorities, and truth maintenance. CLIPS 6.0 supports more complex nesting of conditional elements in the if portion of a rule (\"and\", \"or\", and \"not\" conditional elements can be placed within a \"not\" conditional element). In addition, there is no longer a limitation on the number of multifield slots that a deftemplate can contain. The CLIPS Object-Oriented Language (COOL) provides object-oriented programming capabilities. Features supported by COOL include classes with multiple inheritance, abstraction, encapsulation, polymorphism, dynamic binding, and message passing with message-handlers. CLIPS 6.0 supports tight integration of the rule-based programming features of CLIPS with COOL (that is, a rule can pattern match on objects created using COOL). CLIPS 6.0 provides the capability to define functions, overloaded functions, and global variables interactively. In addition, CLIPS can be embedded within procedural code, called as a subroutine, and integrated with languages such as C, FORTRAN and Ada. CLIPS can be easily extended by a user through the use of several well-defined protocols. CLIPS provides several delivery options for programs including the ability to generate stand alone executables or to load programs from text or binary files. CLIPS 6.0 provides support for the modular development and execution of knowledge bases with the defmodule construct. CLIPS modules allow a set of constructs to be grouped together such that explicit control can be maintained over restricting the access of the constructs by other modules. This type of control is similar to global and local scoping used in languages such as C or Ada. By restricting access to deftemplate and defclass constructs, modules can function as blackboards, permitting only certain facts and instances to be seen by other modules. Modules are also used by rules to provide execution control. The CRSV (Cross-Reference, Style, and Verification) utility included with previous version of CLIPS is no longer supported. The capabilities provided by this tool are now available directly within CLIPS 6.0 to aid in the development, debugging, and verification of large rule bases. COSMIC offers four distribution versions of CLIPS 6.0: UNIX (MSC-22433), VMS (MSC-22434), MACINTOSH (MSC-22429), and IBM PC (MSC-22430). Executable files, source code, utilities, documentation, and examples are included on the program media. All distribution versions include identical source code for the command line version of CLIPS 6.0. This source code should compile on any platform with an ANSI C compiler. Each distribution version of CLIPS 6.0, except that for the Macintosh platform, includes an executable for the command line version. For the UNIX version of CLIPS 6.0, the command line interface has been successfully implemented on a Sun4 running SunOS, a DECstation running DEC RISC ULTRIX, an SGI Indigo Elan running IRIX, a DEC Alpha AXP running OSF/1, and an IBM RS/6000 running AIX. Command line interface executables are included for Sun4 computers running SunOS 4.1.1 or later and for the DEC RISC ULTRIX platform. The makefiles may have to be modified slightly to be used on other UNIX platforms. The UNIX, Macintosh, and IBM PC versions of CLIPS 6.0 each have a platform specific interface. Source code, a makefile, and an executable for the Windows 3.1 interface version of CLIPS 6.0 are provided only on the IBM PC distribution diskettes. Source code, a makefile, and an executable for the Macintosh interface version of CLIPS 6.0 are provided only on the Macintosh distribution diskettes. Likewise, for the UNIX version of CLIPS 6.0, only source code and a makefile for an X-Windows interface are provided. The X-Windows interface requires MIT's X Window System, Version 11, Release 4 (X11R4), the Athena Widget Set, and the Xmu library. The source code for the Athena Widget Set is provided on the distribution medium. The X-Windows interface has been successfully implemented on a Sun4 running SunOS 4.1.2 with the MIT distribution of X11R4 (not OpenWindows), an SGI Indigo Elan running IRIX 4.0.5, and a DEC Alpha AXP running OSF/1 1.2. The VAX version of CLIPS 6.0 comes only with the generic command line interface. ASCII makefiles for the command line version of CLIPS are provided on all the distribution media for UNIX, VMS, and DOS. Four executables are provided with the IBM PC version: a windowed interface executable for Windows 3.1 built using Borland C++ v3.1, an editor for use with the windowed interface, a command line version of CLIPS for Windows 3.1, and a 386 command line executable for DOS built using Zortech C++ v3.1. All four executables are capable of utilizing extended memory and require an 80386 CPU or better. Users needing an 8086/8088 or 80286 executable must recompile the CLIPS source code themselves. Users who wish to recompile the DOS executable using Borland C++ or MicroSoft C must use a DOS extender program to produce an executable capable of using extended memory. The version of CLIPS 6.0 for IBM PC compatibles requires DOS v3.3 or later and/or Windows 3.1 or later. It is distributed on a set of three 1.4Mb 3.5 inch diskettes. A hard disk is required. The Macintosh version is distributed in compressed form on two 3.5 inch 1.4Mb Macintosh format diskettes, and requires System 6.0.5, or higher, and 1Mb RAM. The version for DEC VAX/VMS is available in VAX BACKUP format on a 1600 BPI 9-track magnetic tape (standard distribution medium) or a TK50 tape cartridge. The UNIX version is distributed in UNIX tar format on a .25 inch streaming magnetic tape cartridge (Sun QIC-24). For the UNIX version, alternate distribution media and formats are available upon request. The CLIPS 6.0 documentation includes a User's Guide and a three volume Reference Manual consisting of Basic and Advanced Programming Guides and an Interfaces Guide. An electronic version of the documentation is provided on the distribution medium for each version: in MicroSoft Word format for the Macintosh and PC versions of CLIPS, and in both PostScript format and MicroSoft Word for Macintosh format for the UNIX and DEC VAX versions of CLIPS. CLIPS was developed in 1986 and Version 6.0 was released in 1993.",
      "fieldsOfStudy": [
        "Computer Science"
      ],
      "topics": [
        "Command line interface executables",
        "IBM PC versions",
        "modules",
        "UNIX platforms",
        "dynamic rule priorities",
        "large rule bases",
        "systems",
        "rules",
        "previous version",
        "standard distribution medium",
        "DEC RISC ULTRIX",
        "DEC Alpha AXP",
        "conditional elements",
        "alternate distribution media",
        "knowledge bases",
        "CLIPS modules",
        "CLIPS",
        "Version"
      ]
    },
    "org": {
      "title": "File System in Data-Centric Computing",
      "url": "https://www.semanticscholar.org/paper/220959e68a51dea667bf2851fa07b53e5e8a8661",
      "abstract": "The moving computation on the edge or near to data is the new trend that can break the bandwidth wall and to unleash the power of next generation NVM or SCM memory. File system is the important OS subsystem that plays the role of mediator between the user-space application and storage device. The key goal of the file system is to represent the file abstraction and to build the files' namespace. In the current paradigm the file system needs to copy the metadata and user data in the DRAM of the host with the goal to access and to modify the user data on the host side. The DAX approach doesn't change the concept but to build the way to bypass the page cache via the direct access to file's content in persistent memory. Generally speaking, for the case of data-centric computing, the file system needs to solve the opposite task not to copy data into page cache but to deliver the processing activity near data on the storage device side.",
      "fieldsOfStudy": [
        "Computer Science"
      ],
      "topics": [
        "persistent memory",
        "data",
        "page cache",
        "storage device side",
        "generation NVM or SCM memory",
        "host side",
        "SCM",
        "user data",
        "NVM",
        "file system",
        "files content",
        "mediator",
        "user-space application and storage device",
        "file abstraction",
        "SCM memory",
        "storage device",
        "generation NVM"
      ]
    }
  },
  {
    "sim": 0.4798064084774115,
    "gen": {
      "title": "Learning by Exporting and Wage Profiles: New Evidence from Brazil\u2217",
      "url": "https://www.semanticscholar.org/paper/02e8e60ebb50729acae91c63aaff80fb0e8f9ea0",
      "abstract": "Export activity shapes workers\u2019 experience-wage profiles. Using detailed Brazilian manufacturing employer-employee and customs data, we document that workers\u2019 experience-wage profiles are steeper at exporters than at non-exporters. Aside from selfselection of more capable firms into exporting, we show that workers\u2019 experience-wage profiles are steeper when firms export to high-income destinations. We then develop and quantify a model with export market entry, wage renegotiations, and human capital accumulation to interpret the data and perform counterfactual experiments. We find that human capital growth can explain roughly 40% of differences in wage profiles between exporters and non-exporters as well as the gains in experience returns after entry into high-income destinations. We also show that increased human capital per worker can account for one-half of the overall gains in real income from trade openness. In slowing human capital accumulation, trade liberalization can induce welfare losses if the trade partners are low-income destinations.",
      "fieldsOfStudy": null,
      "topics": [
        "wage profiles",
        "real income",
        "exporters",
        "trade openness",
        "experience returns",
        "wage renegotiations",
        "trade liberalization",
        "human capital accumulation",
        "low-income destinations",
        "human capital growth",
        "increased human capital",
        "counterfactual experiments",
        "worker",
        "slowing human capital accumulation",
        "experience-wage profiles"
      ]
    },
    "org": {
      "title": "Differentially Private Contextual Linear Bandits",
      "url": "https://www.semanticscholar.org/paper/46eebe70e934200b1bdf4b8fddfaa083321419b1",
      "abstract": "We study the contextual linear bandit problem, a version of the standard stochastic multi-armed bandit (MAB) problem where a learner sequentially selects actions to maximize a reward which depends also on a user provided per-round context. Though the context is chosen arbitrarily or adversarially, the reward is assumed to be a stochastic function of a feature vector that encodes the context and selected action. Our goal is to devise private learners for the contextual linear bandit problem. \nWe first show that using the standard definition of differential privacy results in linear regret. So instead, we adopt the notion of joint differential privacy, where we assume that the action chosen on day $t$ is only revealed to user $t$ and thus needn't be kept private that day, only on following days. We give a general scheme converting the classic linear-UCB algorithm into a joint differentially private algorithm using the tree-based algorithm. We then apply either Gaussian noise or Wishart noise to achieve joint-differentially private algorithms and bound the resulting algorithms' regrets. In addition, we give the first lower bound on the additional regret any private algorithms for the MAB problem must incur.",
      "fieldsOfStudy": [
        "Computer Science",
        "Mathematics"
      ],
      "topics": [
        "following days",
        "selected action",
        "days",
        "private learners",
        "linear regret",
        "actions",
        "user",
        "t$",
        "joint differential privacy",
        "private algorithms",
        "MAB",
        "differential privacy results",
        "contextual linear bandit problem",
        "linear",
        "round context",
        "differential privacy",
        "joint differentially private algorithm"
      ]
    }
  },
  null,
  {
    "sim": 0.49077539036545526,
    "gen": {
      "title": "Augmenting Cybersecurity in Autonomous Vehicles: Innovative Recommendations for Aspiring Entrepreneurs",
      "url": "https://www.semanticscholar.org/paper/9285cedac6ab25dbdfaf1bc9dde01dee5f9976d7",
      "abstract": "Autonomous and connected vehicles are a collection of smart components such as sensors, Radar/Lidar, Electronic Control Units (ECUs), and cameras programmed through voluminous lines of codes, which enable a seamless communication system between the user, vehicle, and infrastructure. The concept of autonomous vehicles is gaining significant traction in the global markets, generating higher revenues, and witnessing high market penetration, thus attracting entrepreneurs to make new and disruptive innovations. However, the underlying technology and communication system of autonomous and connected vehicles are prone to the cybersecurity threats, which circumvent any product that uses sensors, transmits data over the network, and can be controlled through a remote computer or a mobile application. In this article, we attempt to outline the concerns that an aspiring entrepreneur should be aware of before entering into the autonomous vehicle industry, followed by innovative recommendations that can guide addressing the article's issues. The presented recommendations and guidelines are based on the cybersecurity principles of confidentiality, integrity, and availability.",
      "fieldsOfStudy": [
        "Computer Science"
      ],
      "topics": [
        "autonomous vehicles",
        "high market penetration",
        "vehicle",
        "higher revenues",
        "Electronic Control Units",
        "voluminous lines",
        "entrepreneurs",
        "sensors",
        "innovative recommendations",
        "availability",
        "smart components",
        "autonomous and connected vehicles",
        "autonomous vehicle industry",
        "significant traction",
        "communication system"
      ]
    },
    "org": {
      "title": "Continuous Operator Authentication for Teleoperated Systems Using Hidden Markov Models",
      "url": "https://www.semanticscholar.org/paper/074af21c12f00a1a84f52c155f3c23fdcd0a1503",
      "abstract": "In this article, we present a novel approach for continuous operator authentication in teleoperated robotic processes based on Hidden Markov Models (HMM). While HMMs were originally developed and widely used in speech recognition, they have shown great performance in human motion and activity modeling. We make an analogy between human language and teleoperated robotic processes (i.e., words are analogous to a teleoperator\u2019s gestures, sentences are analogous to the entire teleoperated task or process) and implement HMMs to model the teleoperated task. To test the continuous authentication performance of the proposed method, we conducted two sets of analyses. We built a virtual reality (VR) experimental environment using a commodity VR headset (HTC Vive) and haptic feedback enabled controller (Sensable PHANToM Omni) to simulate a real teleoperated task. An experimental study with 10 subjects was then conducted. We also performed simulated continuous operator authentication by using the JHU-ISI Gesture and Skill Assessment Working Set (JIGSAWS). The performance of the model was evaluated based on the continuous (real-time) operator authentication accuracy as well as resistance to a simulated impersonation attack. The results suggest that the proposed method is able to achieve 70% (VR experiment) and 81% (JIGSAWS dataset) continuous classification accuracy with as short as a 1-second sample window. It is also capable of detecting an impersonation attack in real-time.",
      "fieldsOfStudy": [
        "Computer Science"
      ],
      "topics": [
        "teleoperated robotic processes",
        "simulated continuous operator authentication",
        "continuous classification accuracy",
        "process",
        "real teleoperated task",
        "Hidden Markov Models",
        "Skill Assessment Working Set",
        "teleoperated task",
        "JIGSAWS",
        "HMM",
        "activity modeling",
        "great performance",
        "continuous operator authentication",
        "JIGSAWS dataset",
        "Sensable PHANToM Omni",
        "haptic feedback enabled controller"
      ]
    }
  },
  null,
  {
    "sim": 0.47492549155600816,
    "gen": {
      "title": "An Empirical Analysis of Anonymity in Zcash",
      "url": "https://www.semanticscholar.org/paper/ab4da6d37196c4fbec6bcb3b7508e31506fe30bc",
      "abstract": "Among the now numerous alternative cryptocurrencies derived from Bitcoin, Zcash is often touted as the one with the strongest anonymity guarantees, due to its basis in well-regarded cryptographic research. In this paper, we examine the extent to which anonymity is achieved in the deployed version of Zcash. We investigate all facets of anonymity in Zcash's transactions, ranging from its transparent transactions to the interactions with and within its main privacy feature, a shielded pool that acts as the anonymity set for users wishing to spend coins privately. We conclude that while it is possible to use Zcash in a private way, it is also possible to shrink its anonymity set considerably by developing simple heuristics based on identifiable patterns of usage.",
      "fieldsOfStudy": [
        "Computer Science"
      ],
      "topics": [
        "Zcash",
        "identifiable patterns",
        "usage",
        "coins",
        "simple heuristics",
        "strongest anonymity guarantees",
        "Zcashs transactions",
        "users",
        "main privacy feature",
        "Bitcoin, Zcash",
        "anonymity",
        "transparent transactions",
        "Bitcoin",
        "well-regarded cryptographic research",
        "shielded pool",
        "deployed version"
      ]
    },
    "org": {
      "title": "Store-to-Leak Forwarding: Leaking Data on Meltdown-resistant CPUs",
      "url": "https://www.semanticscholar.org/paper/20b6d5cad324dacf1a506edd2d5a59adbeb1c89b",
      "abstract": "Meltdown and Spectre exploit microarchitectural changes the CPU makes during transient out-of-order execution. Using side-channel techniques, these attacks enable leaking arbitrary data from memory. As state-of-the-art software mitigations for Meltdown may incur significant performance overheads, they are only seen as a temporary solution. Thus, software mitigations are disabled on more recent processors, which are not susceptible to Meltdown anymore. \nIn this paper, we show that Meltdown-like attacks are still possible on recent CPUs which are not vulnerable to the original Meltdown attack. We show that the store buffer---a microarchitectural optimization to reduce the latency for data stores---in combination with the TLB enables powerful attacks. We present several ASLR-related attacks, including a KASLR break from unprivileged applications, and breaking ASLR from JavaScript. We can also mount side-channel attacks, breaking the atomicity of TSX, and monitoring control flow of the kernel. Furthermore, when combined with a simple Spectre gadget, we can leak arbitrary data from memory. Our paper shows that Meltdown-like attacks are still possible, and software fixes are still necessary to ensure proper isolation between the kernel and user space.",
      "fieldsOfStudy": [
        "Computer Science"
      ],
      "topics": [
        "user space",
        "powerful attacks",
        "arbitrary data",
        "data stores",
        "Meltdown",
        "control flow",
        "original Meltdown attack",
        "Meltdown-like attacks",
        "unprivileged applications",
        "memory",
        "recent processors",
        "recent CPUs",
        "significant performance overheads",
        "ASLR-related attacks",
        "proper isolation"
      ]
    }
  },
  null,
  {
    "sim": 0.3987188079055035,
    "gen": {
      "title": "Distributed Optimal Guidance Laws for Multiple Unmanned Aerial Vehicles Attacking A Moving Target",
      "url": "https://www.semanticscholar.org/paper/745a3063d95524b1f44da820834926fd98e018ac",
      "abstract": "In this paper, two cooperative guidance laws based on two-point boundary value are designed to deal with the problem of cooperative encirclement and simultaneous attack under condition of both known target acceleration and unknown target acceleration. The only requirement for the multi-attacker communication network is that it contains a directed spanning tree. The guidance laws can function properly as long as at least one attacker can observed the target. The acceleration components along the attacker-target line of sight in the novel guidance laws can reduce the relative remaining distance between each of the attackers and the target at the same speed, thus completing simultaneous attack and avoiding the calculation of the remaining time. The components of the guidance laws perpendicular to the attacker-target line of sight can make the normal overload of relative motion zero, so that the trajectory will be smooth and the collision problem within the attacker can be avoided. Simulation results verified the practicability of the novel guidance laws.",
      "fieldsOfStudy": [
        "Computer Science",
        "Engineering"
      ],
      "topics": [
        "unknown target acceleration",
        "simultaneous attack",
        "relative motion",
        "known target acceleration",
        "sight",
        "cooperative encirclement",
        "attacker-target line",
        "relative remaining distance",
        "cooperative guidance laws",
        "target",
        "condition",
        "remaining time",
        "guidance laws"
      ]
    },
    "org": {
      "title": "Allocating Marketing Resources Over Social Networks: A Long-Term Analysis",
      "url": "https://www.semanticscholar.org/paper/e68148e9510210c70041c711a8d1395a5f725247",
      "abstract": "In this letter, we consider a network of consumers who are under the combined influence of their neighbors and external influencing entities (the marketers). The consumers\u2019 opinion follows a hybrid dynamics whose opinion jumps are due to the marketing campaigns. By using the relevant static game model proposed in previous works, we prove that although the marketers are in competition and therefore create tension in the network, the network reaches a consensus. Exploiting this key result, we propose a coopetition marketing strategy which combines the one-shot Nash equilibrium actions and a policy of no advertising. Under reasonable sufficient conditions, it is proved that the proposed coopetition strategy profile Pareto-dominates the one-shot Nash equilibrium strategy. This is a very encouraging result to tackle the much more challenging problem of designing Pareto-optimal and equilibrium strategies for the considered dynamical marketing game.",
      "fieldsOfStudy": [
        "Economics",
        "Computer Science",
        "Engineering",
        "Business"
      ],
      "topics": [
        "external influencing entities",
        "tension",
        "previous works",
        "proposed coopetition strategy profile",
        "coopetition marketing strategy",
        "considered dynamical marketing game",
        "competition",
        "Pareto",
        "Pareto-optimal and equilibrium strategies",
        "one-shot Nash equilibrium strategy",
        "marketing campaigns",
        "consumers",
        "relevant static game model",
        "one-shot Nash equilibrium actions",
        "reasonable sufficient conditions",
        "Nash",
        "opinion"
      ]
    }
  },
  {
    "sim": 0.5484889368383448,
    "gen": {
      "title": "Phase field modeling of quasi-static and dynamic crack propagation: COMSOL implementation and case studies",
      "url": "https://www.semanticscholar.org/paper/5b3d358555daff4b241e8a7205fe179d65ccedaa",
      "abstract": null,
      "fieldsOfStudy": [
        "Computer Science"
      ]
    },
    "org": {
      "title": "Generalized Summation-by-Parts Operators for the Second Derivative",
      "url": "https://www.semanticscholar.org/paper/92257685e5d0bef126b5125c16efd104b74b3eb7",
      "abstract": "The generalization of summation-by-parts operators for the first derivative of Del Rey Fernandez, Boom, and Zingg [J. Comput. Phys., 266 (2014), pp. 214--239] is extended to approximations of second derivatives with a constant or variable coefficient. This enables the construction of second-derivative operators with one or more of the following characteristics: (i) nonrepeating interior point operators, (ii) nonuniform nodal distributions, and (iii) exclusion of one or both boundary nodes. Definitions are proposed that give rise to generalized summation-by-parts operators that result in consistent, conservative, and stable discretizations of partial differential equations with or without mixed derivatives. It is proven that approximations to the second derivative with a variable coefficient can be constructed using the constituent matrices of the constant-coefficient operator. Moreover, for operators with a repeating interior point operator, a decomposition is proposed that makes the application of such o...",
      "fieldsOfStudy": [
        "Computer Science",
        "Mathematics"
      ],
      "topics": [
        "interior point operators",
        "operators",
        "J. Comput",
        "partial differential equations",
        "second-derivative operators",
        "repeating interior point operator",
        "Del Rey Fernandez",
        "nodal distributions",
        "Zingg [J. Comput",
        "constant-coefficient operator",
        "Zingg",
        "Boom",
        "mixed derivatives",
        "variable coefficient",
        "second",
        "approximations",
        "second derivative",
        "constant or variable coefficient"
      ]
    }
  },
  {
    "sim": 0.3920887196569942,
    "gen": {
      "title": "A new efficient hyperelastic finite element model for graphene and its application to carbon nanotubes and nanocones",
      "url": "https://www.semanticscholar.org/paper/c9d48674c0458cbefe9f5a188aabac1d48c3fb06",
      "abstract": null,
      "fieldsOfStudy": [
        "Computer Science",
        "Physics",
        "Materials Science"
      ]
    },
    "org": {
      "title": "Multi-fidelity modeling with different input domain definitions using deep Gaussian processes",
      "url": "https://www.semanticscholar.org/paper/352e0bb891106ddc363323a641f1309cce369c61",
      "abstract": null,
      "fieldsOfStudy": [
        "Computer Science",
        "Mathematics"
      ]
    }
  },
  null,
  null,
  {
    "sim": 0.7014882574385498,
    "gen": {
      "title": "Learning to Play No-Press Diplomacy with Best Response Policy Iteration",
      "url": "https://www.semanticscholar.org/paper/8a01410e7d7010537492e6355261458d305d1386",
      "abstract": "Recent advances in deep reinforcement learning (RL) have led to considerable progress in many 2-player zero-sum games, such as Go, Poker and Starcraft. The purely adversarial nature of such games allows for conceptually simple and principled application of RL methods. However real-world settings are many-agent, and agent interactions are complex mixtures of common-interest and competitive aspects. We consider Diplomacy, a 7-player board game designed to accentuate dilemmas resulting from many-agent interactions. It also features a large combinatorial action space and simultaneous moves, which are challenging for RL algorithms. We propose a simple yet effective approximate best response operator, designed to handle large combinatorial action spaces and simultaneous moves. We also introduce a family of policy iteration methods that approximate fictitious play. With these methods, we successfully apply RL to Diplomacy: we show that our agents convincingly outperform the previous state-of-the-art, and game theoretic equilibrium analysis shows that the new process yields consistent improvements.",
      "fieldsOfStudy": [
        "Computer Science",
        "Mathematics",
        "Political Science"
      ],
      "topics": [
        "RL algorithms",
        "RL methods",
        "games",
        "consistent improvements",
        "RL",
        "large combinatorial action spaces",
        "simultaneous moves",
        "complex mixtures",
        "fictitious play",
        "considerable progress",
        "policy iteration methods",
        "many-agent interactions",
        "Starcraft",
        "many-agent",
        "game theoretic equilibrium analysis"
      ]
    },
    "org": {
      "title": "From Poincar\u00e9 Recurrence to Convergence in Imperfect Information Games: Finding Equilibrium via Regularization",
      "url": "https://www.semanticscholar.org/paper/af111ed91c177a14d403fce77e1c9c2fb99b0061",
      "abstract": "In this paper we investigate the Follow the Regularized Leader dynamics in sequential imperfect information games (IIG). We generalize existing results of Poincare recurrence from normal-form games to zero-sum two-player imperfect information games and other sequential game settings. We then investigate how adapting the reward (by adding a regularization term) of the game can give strong convergence guarantees in monotone games. We continue by showing how this reward adaptation technique can be leveraged to build algorithms that converge exactly to the Nash equilibrium. Finally, we show how these insights can be directly used to build state-of-the-art model-free algorithms for zero-sum two-player Imperfect Information Games (IIG).",
      "fieldsOfStudy": [
        "Computer Science",
        "Mathematics"
      ],
      "topics": [
        "sequential game settings",
        "monotone games",
        "IIG",
        "strong convergence guarantees",
        "Imperfect Information Games",
        "normal-form games",
        "game",
        "algorithms",
        "Poincare recurrence",
        "existing results",
        "Nash equilibrium",
        "zero-sum two-player Imperfect Information Games",
        "Poincare",
        "reward adaptation technique"
      ]
    }
  },
  null,
  null,
  null,
  {
    "sim": 0.2847521536441784,
    "gen": {
      "title": "Sparse reduced-order modelling: sensor-based dynamics to full-state estimation",
      "url": "https://www.semanticscholar.org/paper/f16fe95e9563710e9ef0eb3ef61a9228a7e86266",
      "abstract": "We propose a general dynamic reduced-order modelling framework for typical experimental data: time-resolved sensor data and optional non-time-resolved particle image velocimetry (PIV) snapshots. This framework can be decomposed into four building blocks. First, the sensor signals are lifted to a dynamic feature space without false neighbours. Second, we identify a sparse human-interpretable nonlinear dynamical system for the feature state based on the sparse identification of nonlinear dynamics (SINDy). Third, if PIV snapshots are available, a local linear mapping from the feature state to the velocity field is performed to reconstruct the full state of the system. Fourth, a generalized feature-based modal decomposition identifies coherent structures that are most dynamically correlated with the linear and nonlinear interaction terms in the sparse model, adding interpretability. Steps 1 and 2 define a black-box model. Optional steps 3 and 4 lift the black-box dynamics to a grey-box model in terms of the identified coherent structures, if non-time-resolved full-state data are available. This grey-box modelling strategy is successfully applied to the transient and post-transient laminar cylinder wake, and compares favourably with a proper orthogonal decomposition model. We foresee numerous applications of this highly flexible modelling strategy, including estimation, prediction and control. Moreover, the feature space may be based on intrinsic coordinates, which are unaffected by a key challenge of modal expansion: the slow change of low-dimensional coherent structures with changing geometry and varying parameters.",
      "fieldsOfStudy": [
        "Computer Science",
        "Physics"
      ],
      "topics": [
        "coherent structures",
        "nonlinear dynamics",
        "varying parameters",
        "typical experimental data",
        "non-time-resolved full-state data",
        "PIV snapshots",
        "low-dimensional coherent structures",
        "identified coherent structures",
        "changing geometry",
        "interpretability",
        "terms",
        "modal expansion",
        "proper orthogonal decomposition model",
        "feature state",
        "false neighbours",
        "time-resolved sensor data"
      ]
    },
    "org": {
      "title": "The Role of Taste Affinity in Agent-Based Models for Social Recommendation",
      "url": "https://www.semanticscholar.org/paper/b907fbb72f4ec5705dacc344c937a059729cfeb4",
      "abstract": "In the Internet era, online social media emerged as the main tool for sharing opinions and information among individuals. In this work, we study an adaptive model of a social network where directed links connect users with similar tastes, and over which information propagates through social recommendation. Agent-based simulations of two different artificial settings for modeling user tastes are compared with patterns seen in real data, suggesting that users differing in their scope of interests is a more realistic assumption than users differing only in their particular interests. We further introduce an extensive set of similarity metrics based on users' past assessments, and evaluate their use in the given social recommendation model with both artificial simulations and real data. Superior recommendation performance is observed for similarity metrics that give preference to users with small scope \u2014 who thus act as selective filters in social recommendation.",
      "fieldsOfStudy": [
        "Computer Science",
        "Physics"
      ],
      "topics": [
        "user tastes",
        "social recommendation",
        "users",
        "online social media",
        "interests",
        "real data",
        "Superior recommendation performance",
        "small scope",
        "given social recommendation model",
        "similar tastes",
        "similarity metrics",
        "users past assessments",
        "information",
        "selective filters",
        "directed links"
      ]
    }
  },
  null,
  {
    "sim": 0.262535057354897,
    "gen": {
      "title": "Couverture vaccinale dans une antenne m\u00e9dicale des arm\u00e9es et plaquette d'information sur la vaccination",
      "url": "https://www.semanticscholar.org/paper/29288ee1aacd784d5efd465f1ac2ade18419034a",
      "abstract": "Introduction : dans les armees la vaccination est reglementaire donc obligatoire. La derniere etude de couverture vaccinale dans les armees a ete realise par le Centre d\u2019Epidemiologie et de Sante Publique des Armees (CESPA) en 2005 et concluait a une couverture vaccinale insuffisante. Afin d\u2019ameliorer cette couverture le CESPA diffuse aux militaires une plaquette d\u2019information sur la vaccination. L'objectif principal de cette etude etait d'effectuer une enquete de couverture vaccinale sur une antenne medicale. L'objectif secondaire etait d'effectuer une evaluation qualitative de la plaquette d'information sur la vaccination dans les armees version 2016 diffusee par le CESPA. \nMateriel et methodes : l\u2019enquete de couverture vaccinale a ete menee dans l\u2019antenne medicale de Nimes qui soutient des militaires issus de differents corps d\u2019arme. La couverture vaccinale a ete calculee pour chaque valence du calendrier vaccinal des armees pour l\u2019antenne medicale et pour chaque corps d\u2019arme represente a partir d\u2019une base de donnees unique anonymisee. Durant un mois, les militaires consultant a l\u2019antenne medicale ont renseigne anonymement un auto-questionnaire d\u2019evaluation de la plaquette d\u2019information. \nResultats : l\u2019etude portait sur tous les militaires soutenus par l\u2019antenne medicale soit 2101 militaires. Pour l\u2019antenne medicale, les couvertures vaccinales etaient les suivantes : 73,8% pour la grippe, 99,3% pour le vaccin Diphterie Tetanos Poliomyelite, 85,6% pour le vaccin Rougeole Rubeole Oreillons, 97,4% pour l\u2019Hepatite B, 96,9% pour l\u2019Hepatite A, 91,1% pour la Fievre Jaune, 71,7% pour la Fievre Typhoide et 94,5% pour le vaccin anti-Meningites tetravalent. Les repondants au questionnaire montraient une bonne adhesion a la vaccination mais se sentaient peu concerne par la vaccination. Ils avaient moyennement bien compris les informations relayees par le document. Ils ont juge la plaquette d\u2019information globalement utile et bien presentee. \nDiscussion-conclusion : la couverture vaccinale dans l\u2019antenne medicale etudiee etait globalement satisfaisante et superieure a celle rapportee par l\u2019etude de 2005. Une etude nationale pourrait confirmer la tendance sur l\u2019ensemble des armees. La plaquette d\u2019information du CESPA sur la vaccination semble etre un bon outil pedagogique et peut etre amelioree dans l\u2019objectif de mieux coller aux interrogations des militaires.",
      "fieldsOfStudy": null,
      "topics": [
        "la couverture vaccinale dans l\u2019antenne medicale etudiee",
        "la vaccination",
        "sur la",
        "l\u2019antenne medicale et pour chaque corps",
        "diffuse aux militaires une plaquette d\u2019information sur la vaccination",
        "85,6% pour",
        "la plaquette d\u2019information globalement",
        "la",
        "l\u2019antenne medicale etudiee",
        "La plaquette d\u2019information du CESPA sur la vaccination semble etre",
        "la grippe",
        "Pour l\u2019antenne medicale",
        "85,6%",
        "militaires",
        "CESPA",
        "chaque corps d\u2019arme",
        "anti-Meningites tetravalent",
        "l\u2019Hepatite A",
        "l\u2019antenne medicale",
        "base",
        "le vaccin Rougeole Rubeole Oreillons",
        "le vaccin Diphterie Tetanos Poliomyelite",
        "questionnaire",
        "71,7%",
        "73,8%"
      ]
    },
    "org": {
      "title": "AMBERT: A Pre-trained Language Model with Multi-Grained Tokenization",
      "url": "https://www.semanticscholar.org/paper/59c0076b3d814588e320820b95563965733d1875",
      "abstract": "Pre-trained language models such as BERT have exhibited remarkable performances in many tasks in natural language understanding (NLU). The tokens in the models are usually fine-grained in the sense that for languages like English they are words or sub-words and for languages like Chinese they are characters. In English, for example, there are multi-word expressions which form natural lexical units and thus the use of coarse-grained tokenization also appears to be reasonable. In fact, both fine-grained and coarse-grained tokenizations have advantages and disadvantages for learning of pre-trained language models. In this paper, we propose a novel pre-trained language model, referred to as AMBERT (A Multi-grained BERT), on the basis of both fine-grained and coarse-grained tokenizations. For English, AMBERT takes both the sequence of words (fine-grained tokens) and the sequence of phrases (coarse-grained tokens) as input after tokenization, employs one encoder for processing the sequence of words and the other encoder for processing the sequence of the phrases, utilizes shared parameters between the two encoders, and finally creates a sequence of contextualized representations of the words and a sequence of contextualized representations of the phrases. Experiments have been conducted on benchmark datasets for Chinese and English, including CLUE, GLUE, SQuAD and RACE. The results show that AMBERT outperforms the existing best performing models in almost all cases, particularly the improvements are significant for Chinese.",
      "fieldsOfStudy": [
        "Computer Science"
      ],
      "topics": [
        "multi-word expressions",
        "natural language understanding",
        "words",
        "languages",
        "contextualized representations",
        "natural lexical units",
        "novel pre-trained language model",
        "characters",
        "tokenization",
        "Chinese",
        "phrases",
        "shared parameters",
        "tasks",
        "A Multi-grained BERT",
        "RACE",
        "coarse-grained tokenization"
      ]
    }
  },
  {
    "sim": 0.4684992554048357,
    "gen": {
      "title": "Probabilistic Generation of Random Networks Taking into Account Information on Motifs Occurrence",
      "url": "https://www.semanticscholar.org/paper/9b6dbde54fb7dcc3d8ced210671f478a7d286edc",
      "abstract": "Because of the huge number of graphs possible even with a small number of nodes, inference on network structure is known to be a challenging problem. Generating large random directed graphs with prescribed probabilities of occurrences of some meaningful patterns (motifs) is also difficult. We show how to generate such random graphs according to a formal probabilistic representation, using fast Markov chain Monte Carlo methods to sample them. As an illustration, we generate realistic graphs with several hundred nodes mimicking a gene transcription interaction network in Escherichia coli.",
      "fieldsOfStudy": [
        "Computer Science",
        "Biology",
        "Mathematics",
        "Medicine"
      ],
      "topics": [
        "fast Markov chain Monte Carlo methods",
        "Monte Carlo",
        "network structure",
        "large random directed graphs",
        "random graphs",
        "realistic graphs",
        "graphs",
        "prescribed probabilities",
        "motifs",
        "nodes",
        "gene transcription interaction network",
        "Markov",
        "Escherichia",
        "transcription",
        "Escherichia coli",
        "challenging problem",
        "occurrences",
        "inference",
        "formal probabilistic representation"
      ]
    },
    "org": {
      "title": "Un duel probabiliste pour d\u00e9partager deux pr\u00e9sidents (LIA @ DEFT'2005)",
      "url": "https://www.semanticscholar.org/paper/ef359f70fa96698bb5d20900a4af37dcf3f12f50",
      "abstract": "We present a set of probabilistic models applied to binary classification as defined in the DEFT'05 challenge. The challenge consisted a mixture of two differents problems in Natural Language Processing : identification of author (a sequence of Fran\\c{c}ois Mitterrand's sentences might have been inserted into a speech of Jacques Chirac) and thematic break detection (the subjects addressed by the two authors are supposed to be different). Markov chains, Bayes models and an adaptative process have been used to identify the paternity of these sequences. A probabilistic model of the internal coherence of speeches which has been employed to identify thematic breaks. Adding this model has shown to improve the quality results. A comparison with different approaches demostrates the superiority of a strategy that combines learning, coherence and adaptation. Applied to the DEFT'05 data test the results in terms of precision (0.890), recall (0.955) and Fscore (0.925) measure are very promising.",
      "fieldsOfStudy": [
        "Computer Science"
      ],
      "topics": [
        "thematic break detection",
        "thematic breaks",
        "different approaches",
        "Jacques Chirac",
        "author",
        "speeches",
        "coherence",
        "Natural Language Processing",
        "adaptation",
        "learning",
        "Fran\\c{c}ois Mitterrands sentences",
        "measure",
        "Fscore",
        "Bayes models",
        "probabilistic models",
        "Mitterrand",
        "identification",
        "terms"
      ]
    }
  },
  {
    "sim": 0.33154550197248045,
    "gen": {
      "title": "Usages des technologies et dissolution du contr\u00f4le social\u00a0: cas de l\u2019appropriation d\u2019une base de donn\u00e9es \u00e0 vis\u00e9e collaborative",
      "url": "https://www.semanticscholar.org/paper/7676b2bda31c36196d1ddce08d2136e44d0d2a56",
      "abstract": "La portee des divers controles organisationnels sur les comportements est depuis longtemps reconnue. Il importe aujourd\u2019hui d\u2019etudier de quelles nouvelles formes emergentes de controle la digitalisation des organisations s\u2019accompagne pour pouvoir determiner quelles sont leurs implications sur les comportements. Cette recherche s\u2019inscrit dans ce cadre. Les recherches precedentes se sont surtout interessees aux liens entre technologies et controle des resultats ou supervision, c\u2019est-a-dire aux controles dont la source est exterieure au groupe social. Ici, il est plus particulierement question de l\u2019evolution des controles qui emanent du groupe sur les individus qui le composent, autrement dit des phenomenes relevant du controle social. La demarche repose sur l\u2019etude des usages d\u2019une base de donnees collaborative dans un contexte de developpement de produits unissant des equipes de R&D et des equipes chargees de l\u2019industrialisation des nouveaux produits. Une grille d\u2019analyse du controle social est elaboree a partir de la litterature en controle organisationnel pour decrire le controle social au sein de la R&D. Le cas etudie de maniere semi-longitudinale suggere une renegociation des modes d\u2019organisation historiques entre ces deux principales parties prenantes au processus de developpement de produits. Les ajustements induits sur les pratiques de la R&D se concentrent autour de la mise en partage des representations du travail realise. On observe que ces ajustements sont pousses notamment par la constitution d\u2019une supervision indirecte et qu\u2019ils traduisent une dissolution des fondements du controle social exerce par le groupe sur les individus. Ce resultat complete des preuves empiriques de plus en plus nombreuses quant a l\u2019emergence d\u2019une nouvelle forme sociomaterielle de controle.",
      "fieldsOfStudy": [
        "Political Science"
      ],
      "topics": [
        "processus de developpement de produits",
        "un contexte de developpement de produits",
        "de l\u2019industrialisation des nouveaux produits",
        "et controle des",
        "qui emanent du groupe sur",
        "de la R&D.",
        "traduisent une dissolution des fondements du controle social exerce par",
        "d\u2019analyse du controle social est elaboree",
        "des representations",
        "controle organisationnel pour decrire",
        "complete des preuves",
        "est depuis longtemps reconnue",
        "sur",
        "La portee des divers",
        "controle social",
        "la R&D.",
        "controles",
        "produits",
        "leurs implications",
        "phenomenes",
        "au groupe social",
        "supervision",
        "modes d\u2019organisation",
        "le controle social au sein",
        "resultats",
        "l\u2019evolution",
        "R&D",
        "l\u2019industrialisation"
      ]
    },
    "org": {
      "title": "Impact and Productivity of PhD Graduates of Computer Science/Engineering Departments of Hellenic Universities",
      "url": "https://www.semanticscholar.org/paper/1cc4d5b3dc254493acfdd6b60cc98957954d45bc",
      "abstract": "This article presents an anatomy of PhD programmes in Hellenic universities' departments of computer science/engineering from the perspective of research productivity and impact. The study aims at showing the dynamics of research conducted in computer science/engineering departments, and after recognizing weaknesses, to motivate the stakeholders to take actions that will improve competition and excellence. Beneficiaries of this investigation are the following entities: a) the departments themselves can assess their performance relative to that of other departments and then set strategic goals and design procedures to achieve them, b) supervisors can assess the part of their research conducted with PhDs and set their own goals, c) former PhDs who can identify their relative success, and finally d) prospective PhD students who can consider the efficacy of departments and supervisors in conducting high-impact research as one more significant factor in designing the doctoral studies they will follow.",
      "fieldsOfStudy": [
        "Computer Science",
        "Engineering"
      ],
      "topics": [
        "departments",
        "research productivity",
        "departments",
        "research",
        "prospective PhD students",
        "PhDs",
        "strategic goals",
        "impact",
        "computer science/engineering departments",
        "excellence",
        "supervisors",
        "PhDs",
        "high-impact research",
        "design procedures",
        "PhD programmes",
        "competition",
        "Hellenic universities departments",
        "computer science/engineering"
      ]
    }
  },
  {
    "sim": -0.015262075703259992,
    "gen": {
      "title": "Leveraging Applications of Formal Methods, Verification and Validation. Software Engineering: 11th International Symposium, ISoLA 2022, Rhodes, Greece, October 22\u201330, 2022, Proceedings, Part II",
      "url": "https://www.semanticscholar.org/paper/7db656c700a490a5c5e1b15f30dc23f2a4e3f629",
      "abstract": null,
      "fieldsOfStudy": [
        "Computer Science"
      ]
    },
    "org": {
      "title": "Fine-Grained Image-to-Image Transformation Towards Visual Recognition",
      "url": "https://www.semanticscholar.org/paper/750011b8dd8d6a533ef7efd69392fb3aadbfa6e5",
      "abstract": "Existing image-to-image transformation approaches primarily focus on synthesizing visually pleasing data. Generating images with correct identity labels is challenging yet much less explored. It is even more challenging to deal with image transformation tasks with large deformation in poses, viewpoints, or scales while preserving the identity, such as face rotation and object viewpoint morphing. In this paper, we aim at transforming an image with a fine-grained category to synthesize new images that preserve the identity of the input image, which can thereby benefit the subsequent fine-grained image recognition and few-shot learning tasks. The generated images, transformed with large geometric deformation, do not necessarily need to be of high visual quality but are required to maintain as much identity information as possible. To this end, we adopt a model based on generative adversarial networks to disentangle the identity related and unrelated factors of an image. In order to preserve the fine-grained contextual details of the input image during the deformable transformation, a constrained nonalignment connection method is proposed to construct learnable highways between intermediate convolution blocks in the generator. Moreover, an adaptive identity modulation mechanism is proposed to transfer the identity information into the output image effectively. Extensive experiments on the CompCars and Multi-PIE datasets demonstrate that our model preserves the identity of the generated images much better than the state-of-the-art image-to-image transformation models, and as a result significantly boosts the visual recognition performance in fine-grained few-shot learning.",
      "fieldsOfStudy": [
        "Computer Science"
      ],
      "topics": [
        "image transformation tasks",
        "image",
        "new images",
        "Generating images",
        "correct identity labels",
        "object viewpoint morphing",
        "large geometric deformation",
        "large deformation",
        "high visual quality",
        "intermediate convolution blocks",
        "output image",
        "generated images",
        "viewpoints",
        "images"
      ]
    }
  },
  null,
  null,
  null,
  null,
  {
    "sim": 0.5000552524554879,
    "gen": {
      "title": "Hyperbolic problems : theory, numerics, applications : proceedings of the ninth International Conference on Hyperbolic Problems held in CalTech, Pasadena, March 25-29, 2002",
      "url": "https://www.semanticscholar.org/paper/5fc2bc96ff76e4846153413d6e5b41c23b4977cf",
      "abstract": "Plenary Talks.- Diffusion Limits of Kinetic Models.- Viscosity Solutions for Nonlinear Hyperbolic Systems.- Phase Plane Behavior of Solitary Waves in Nonlinear Layered Media.- Energy Method for Equations in Gas Dynamics.- Convergence of the Upwind Interface Source Method for Hyperbolic Conservation Laws.- An Overview on High Order Numerical Methods for Convection Dominated PDEs.- Analysis of High Order Difference Methods for Multiscale Complex Compressible Flows.- Nonlinear Boundary Layers of the Boltzmann Equation Invited Talks.- Invited Talks.- An Homogenized Hyperbolic Model of Multiclass Traffic Flow: a Few Examples.- The GRP Treatment of 2-D Complex Wave Structures.- BV Solutions of Semidiscrete Upwind Scheme.- A Supplement to Entropy Condition.- Robust Treatment of Interfaces for Fluid Flows and Computer Graphics.- Asymptotic Convergence to Diffusive Wave of Bipolar Hydrodynamical Model for Semiconductors.- Simplification, Conservation and Adaptivity in the Front Tracking Method.- Discrete Adjoint Approximations with Shocks.- Far-field Evaluation via Nonreflecting Boundary Conditions.- L1 Stability for the One-dimensional Broadwell Model of a Discrete Velocity Gas.- Recent Progress on Quantum Hydrodynamic Models for Semiconductors.- Critical Thresholds and Conditional Stability for Euler Equations and Related Models.- High-order Asymptotically Strong-stability-preserving Methods for Hyperbolic Systems with Stiff Relaxation.- Contributed Talks.- Some Results on the Boundary Control of Systems of Conservation Laws.- Dynamics of Collapsing Bubbles Near Walls.- The Riemann Problem for a Two-phase Model.- Evaluation of a Well-posed Perfectly Matched Layer for Computational Acoustics.- New Space Staggered and Time Interleaved 2nd Order Finite Volume Methods.- High-resolution Riemann-solver-free Methods for Conservation Laws.- Riemann Problem for Conservation Laws with an Umbilic Point.- Admissibility of Shock Waves and Uniqueness of the Riemann Problem.- Eulerian Approximate Ray Tracing and Applications to Grid Generation.- Non-convex Flux Functions and Compound Shock Waves in Sediment Beds.- Derivation of Schrodinger Poisson as the Non-relativistic Limit of Klein-Gordon Maxwell.- Multigrid for Atmospheric Data Assimilation: Analysis.- A Theory of Implicit Methods for Scalar Conservation Laws.- High-Order Schemes for Multi-Dimensional.- On a Model for Continuous Sedimentation in Vessels with Discontinuous Cross-sectional Area.- Numerical Approximation of the Navier-Stokes Equations with Several Independent Specific Entropies.- Existence and Stability of Multidimensional Transonic Shocks for the Euler Equations for Steady Potential Fluids in Unbounded Domains.- Analysis on a Model for the Dynamic Combustion of a Compressible, Reacting Fluid.- Discontinuous Solutions of Hamilton-Jacobi Equations: Existence, Uniqueness, and Regularity.- Conservation Laws and O.D.E.s.: A Traffic Problem.- Phase Transitions and Chapman-Jouguet Combustions.- Weak Stability of Multidimensional Shocks.- Propagation and Interaction of Delta-Shock Waves of a Hyperbolic System of Conservation Laws.- Multidimensional Compressible Flows with Symmetry.- Efficient Higher-Order Finite Volume Schemes for (Real Gas) Magnetohydrodynamics.- A New Approach to Divergence Cleaning in Magnetohydrodynamic Simulations.- Global Solutions for a Dusty Media Model.- A Constrained Transport Upwind Scheme for Divergence-free Advection.- Green's Function Pointwise Estimates for the Modified Lax-Friedrichs Scheme.- A Class of Global Non Smooth Axisymmetric Solutions to the Euler Equation s of an Isentropic Perfect Gas in 2 Space Dimensions.- A Numerical Method for Cont rollability Problems for the Wave Equation.- Numerical Study of a Viscous Consolidation Model.- Goal-Oriented A Posteriori Error Estimation for Multiple Target Functionals.- The Riemann Problem for a Phase Transition Problem in Thermoelastic Materials.- The Global Existence and Large Time Behavior of Solutions to the Multidimensional Euler-Poisson Equations.- Relaxation Schemes for Conservation Laws with Discontinuous Coefficients.- Relaxation Models and Finite Element Schemes for the Shallow Water Equations.- Second-order Approximation of the Viscous Saint-Venant System and Comparison with Experiments.- Dynamics of Multilayer Shear Flows.- Effects of Viscosity on a Shock Wave Solution of the Euler Equations.- An Accurate Deterministic Projection Method for Hyperbolic Systems with Stiff Source Term.- Adaptive Second Order Central Schemes on Unstructured Staggered Grids.- Physical Symmetries and Hyperbolic GLM Divergence Correction Scheme for Maxwell and MHD Equations.- Mathematical Modelling of Traffic Flows.- New High-Resolution Central-Upwind Schemes for Nonlinear Hyperbolic Conservation Laws.- Afternotes on PHM: Harmonic ENO methods.- One-dimensional Stability of Viscous Shock and Relaxation Profiles.- Simulation of Cavitation in Thermodynamic Equilibrium.- Semi-discrete Finite Element Method for a Class of Visco-elastic Problems with Long Memory under Condition of Friction.- Global Solutions for a Free Boundary Problem of Hyperbolic Type in the Case of Non-negative Peeling Speed.- Stability Analysis for Periodic Solution Waves in Viscous Conservation Laws.- Scalar Conservation Laws with Boundary Effect.- To the Theory of Generalized Entropy Solutions of the Cauchy Problem for a First Order Quasilinear Equation in the Class of Locally Integrable Functions.- Pressure Linearization Method for the Computation of Real Fluids.- Modeling and Computing Geophysical Mass Flows.- Adaptive Application of Characteristic Projection for Central Schemes.- Comparison of Several Difference Schemes for the Euler Equations in 1D and 2D.- Adaptive Time Integration for Hyperboli c Conservation Equations.- A Constrained Transport Method for the Shallow Water MHD Equations.- Solutions with Linear Profile of Velocity to the Euler Equations in Several Dimensions.- An Adaptive Order Godunov Type Central Scheme.- Numerical Study of Dynamic Phase Transitions in 2-D with a Relaxed Scheme.- Particle Method for Simulation of Free Surface Flows.- Three-dimensional Numerical Modelling of Convective Instability by Supernova Explosion with Nested Grid s Scheme on Multiprocessors Systems.- A Local Level-Set Concept for Front Tracking on Arbitrary Grids.- Large-Time Behavior of Solutions to the Multi-Dimensional Euler Equations with Damping.- Isentropic Gas Dynamics with Large Data.- Third and Fourth Order Weighted ENO Schemes for Hamilton-Jacobi Equations on 2D Unstructured Meshes.- High Resolution Conjugate Filters for Hyperbolic Conservation Laws.- List of Participants.",
      "fieldsOfStudy": [
        "Mathematics"
      ],
      "topics": [
        "Viscous Conservation Laws.- Scalar Conservation Laws",
        "Nonlinear Hyperbolic Conservation Laws.- Afternotes",
        "Conservation Laws.- Riemann Problem",
        "Hyperbolic Conservation Laws.- An Overview",
        "Conservation Laws.- Dynamics",
        "Conservation Laws.- Multidimensional Compressible Flows",
        "Euler Equations",
        "High Order Numerical Methods",
        "Regularity.- Conservation Laws",
        "Conservation",
        "Nonlinear Hyperbolic Systems.- Phase",
        "Hyperbolic Systems",
        "Semi-discrete Finite Element Method",
        "Hyperbolic Conservation Laws.-",
        "Relaxation Schemes",
        "Finite Element Schemes",
        "Adaptive Second Order Central Schemes",
        "Several Difference Schemes",
        "Hyperboli c Conservation Equations.-",
        "Hyperbolic GLM Divergence Correction Scheme",
        "Conservation Laws",
        "Nonlinear Hyperbolic Systems.-"
      ]
    },
    "org": {
      "title": "Gaussian processes with skewed Laplace spectral mixture kernels for long-term forecasting",
      "url": "https://www.semanticscholar.org/paper/a961e47b6674966751c9fd58c171317572e9b3ff",
      "abstract": null,
      "fieldsOfStudy": [
        "Computer Science"
      ]
    }
  },
  null,
  null,
  {
    "sim": 0.7152000290776995,
    "gen": {
      "title": "Sensor Scheduling in Variance Based Event Triggered Estimation With Packet Drops",
      "url": "https://www.semanticscholar.org/paper/fe3120024bdc9dbe41fc63a77f449dceac58ad01",
      "abstract": "This paper considers a remote state estimation problem with multiple sensors observing a dynamical process, where sensors transmit local state estimates over an independent and identically distributed (i.i.d.) packet dropping channel to a remote estimator. At every discrete time instant, the remote estimator decides whether each sensor should transmit or not, with each sensor transmission incurring a fixed energy cost. The channel is shared such that collisions will occur if more than one sensor transmits at a time. Performance is quantified via an optimization problem that minimizes a convex combination of the expected estimation error covariance at the remote estimator and expected energy usage across the sensors. For transmission schedules dependent only on the estimation error covariance at the remote estimator, this work establishes structural results on the optimal scheduling which show that: 1) for unstable systems, if the error covariance is large then a sensor will always be scheduled to transmit and 2) there is a threshold-type behavior in switching from one sensor transmitting to another. Specializing to the single sensor case, these structural results demonstrate that a threshold policy (i.e., transmit if the error covariance exceeds a certain threshold and don't transmit otherwise) is optimal. We also consider the situation where sensors transmit measurements instead of state estimates, and establish structural results including the optimality of threshold policies for the single sensor, scalar case. These results provide a theoretical justification for the use of such threshold policies in variance based event triggered estimation. Numerical studies confirm the qualitative behavior predicted by our structural results.",
      "fieldsOfStudy": [
        "Mathematics",
        "Computer Science"
      ],
      "topics": [
        "sensors",
        "multiple sensors",
        "structural results",
        "threshold policies",
        "state estimates",
        "estimation",
        "single sensor case",
        "estimation error covariance",
        "unstable systems",
        "energy usage",
        "remote state estimation problem",
        "scalar case",
        "variance based event triggered estimation",
        "local state estimates",
        "expected energy usage",
        "sensor transmission",
        "single sensor, scalar case"
      ]
    },
    "org": {
      "title": "Channel-Aware Random Access in the Presence of Channel Estimation Errors",
      "url": "https://www.semanticscholar.org/paper/ca55cb5217c6eab1012407858927a67653241d87",
      "abstract": "In this paper, we consider the random access of nodes adapting their transmissions based on the local channel state information (CSI) in a decentralized manner, which we call channel-aware random access (CARA). The CSI at each node is not perfect but has some errors. Thus, the performance of CARA depends on the accuracy of CSI, which is the focus of this paper. Specifically, an exact stability analysis is carried out for the scenario in which two sources that have bursty packet arrivals compete to access a common receiver. The analysis needs to resolve the complex coupling between the queues at the sources having interdependent services. The analysis also takes into account the compound effects of the multipacket reception capability at the receiver. In summary, our contributions in this paper are twofold: first, we obtain an exact stability region of CARA in the presence of channel estimation errors; such an assessment is necessary as errors in channel estimation are inevitable in practical situations. Second, we compare the performance of CARA to that achieved by the class of stationary scheduling policies making decisions in a centralized manner based on the CSI feedback.",
      "fieldsOfStudy": [
        "Computer Science",
        "Mathematics"
      ],
      "topics": [
        "channel estimation",
        "practical situations",
        "errors",
        "CSI",
        "bursty packet arrivals",
        "interdependent services",
        "channel-aware random access",
        "local channel state information",
        "stationary scheduling policies",
        "nodes",
        "common receiver",
        "CARA",
        "decisions",
        "CSI feedback"
      ]
    }
  },
  null,
  {
    "sim": 0.7116802791646266,
    "gen": {
      "title": "Collaborative Filtering with Topic and Social Latent Factors Incorporating Implicit Feedback",
      "url": "https://www.semanticscholar.org/paper/0c75eb246154c0bf2307364d5639ead2ef328396",
      "abstract": "Recommender systems (RSs) provide an effective way of alleviating the information overload problem by selecting personalized items for different users. Latent factors-based collaborative filtering (CF) has become the popular approaches for RSs due to its accuracy and scalability. Recently, online social networks and user-generated content provide diverse sources for recommendation beyond ratings. Although social matrix factorization (Social MF) and topic matrix factorization (Topic MF) successfully exploit social relations and item reviews, respectively; both of them ignore some useful information. In this article, we investigate the effective data fusion by combining the aforementioned approaches. First, we propose a novel model MR3 to jointly model three sources of information (i.e., ratings, item reviews, and social relations) effectively for rating prediction by aligning the latent factors and hidden topics. Second, we incorporate the implicit feedback from ratings into the proposed model to enhance its capability and to demonstrate its flexibility. We achieve more accurate rating prediction on real-life datasets over various state-of-the-art methods. Furthermore, we measure the contribution from each of the three data sources and the impact of implicit feedback from ratings, followed by the sensitivity analysis of hyperparameters. Empirical studies demonstrate the effectiveness and efficacy of our proposed model and its extension.",
      "fieldsOfStudy": [
        "Computer Science"
      ],
      "topics": [
        "rating prediction",
        "item reviews",
        "ratings",
        "personalized items",
        "Topic MF",
        "different users",
        "information",
        "social matrix factorization",
        "social relations",
        "diverse sources",
        "hidden topics",
        "scalability",
        "online social networks",
        "Social MF",
        "topic matrix factorization"
      ]
    },
    "org": {
      "title": "How Useful are Reviews for Recommendation? A Critical Review and Potential Improvements",
      "url": "https://www.semanticscholar.org/paper/1ffc4ff67aae335fdcb501eda4fd39b4011a3b1b",
      "abstract": "We investigate a growing body of work that seeks to improve recommender systems through the use of review text. Generally, these papers argue that since reviews 'explain' users' opinions, they ought to be useful to infer the underlying dimensions that predict ratings or purchases. Schemes to incorporate reviews range from simple regularizers to neural network approaches. Our initial findings reveal several discrepancies in reported results, partly due to (e.g.) copying results across papers despite changes in experimental settings or data pre-processing. First, we attempt a comprehensive analysis to resolve these ambiguities. Further investigation calls for discussion on a much larger problem about the \"importance\" of user reviews for recommendation. Through a wide range of experiments, we observe several cases where state-of-the-art methods fail to outperform existing baselines, especially as we deviate from a few narrowly-defined settings where reviews are useful. We conclude by providing hypotheses for our observations, that seek to characterize under what conditions reviews are likely to be helpful. Through this work, we aim to evaluate the direction in which the field is progressing and encourage robust empirical evaluation.",
      "fieldsOfStudy": [
        "Computer Science"
      ],
      "topics": [
        "user reviews",
        "review text",
        "reviews",
        "neural network approaches",
        "purchases",
        "robust empirical evaluation",
        "experimental settings",
        "existing baselines",
        "reported results",
        "ratings",
        "simple regularizers",
        "results",
        "cases",
        "discrepancies",
        "recommender systems",
        "processing",
        "data pre"
      ]
    }
  },
  {
    "sim": 0.44480172390340444,
    "gen": {
      "title": "Optimal Power Allocation for Energy Harvesting and Power Grid Coexisting Wireless Communication Systems",
      "url": "https://www.semanticscholar.org/paper/50c7c6f72d0d3b1371cbbc33dcd0e7b0824b1993",
      "abstract": "This paper considers the power allocation of a single-link wireless communication with joint energy harvesting and grid power supply. We formulate the problem as minimizing the grid power consumption with random energy and data arrival in fading channel, and analyze the structure of the optimal power allocation policy in some special cases. For the case that all the packets are arrived before transmission, it is a dual problem of throughput maximization, and the optimal solution is found by the two-stage water filling (WF) policy, which allocates the harvested energy in the first stage, and then allocates the power grid energy in the second stage. For the random data arrival case, we first assume grid energy or harvested energy supply only, and then combine the results to obtain the optimal structure of the coexisting system. Specifically, the reverse multi-stage WF policy is proposed to achieve the optimal power allocation when the battery capacity is infinite. Finally, some heuristic online schemes are proposed, of which the performance is evaluated by numerical simulations.",
      "fieldsOfStudy": [
        "Engineering",
        "Computer Science",
        "Mathematics"
      ],
      "topics": [
        "grid energy",
        "grid power supply",
        "harvested energy supply",
        "joint energy harvesting",
        "power grid energy",
        "grid power consumption",
        "numerical simulations",
        "random energy and data arrival",
        "WF",
        "power allocation",
        "throughput maximization",
        "harvested energy",
        "reverse multi-stage WF policy",
        "optimal structure"
      ]
    },
    "org": {
      "title": "Beyond the Central Limit Theorem: Universal and Non-universal Simulations of Random Variables by General Mappings",
      "url": "https://www.semanticscholar.org/paper/caf003fa77a2ab632d428acdb57a37fd584b0ac7",
      "abstract": "Motivated by the Central Limit Theorem, in this paper, we study both universal and non-universal simulations of random variables with an arbitrary target distribution $Q_{Y}$ by general mappings, not limited to linear ones (as in the Central Limit Theorem). We derive the fastest convergence rate of the approximation errors for such problems. Interestingly, we show that for discontinuous or absolutely continuous $P_{X}$, the approximation error for the universal simulation is almost as small as that for the non-universal one; and moreover, for both universal and non-universal simulations, the approximation errors by general mappings are strictly smaller than those by linear mappings. Furthermore, we also generalize these results to simulation from Markov processes, and simulation of random elements (or general random variables).",
      "fieldsOfStudy": [
        "Mathematics",
        "Computer Science"
      ],
      "topics": [
        "general mappings",
        "linear mappings",
        "linear ones",
        "random variables",
        "non-universal one",
        "simulation",
        "universal and non-universal simulations",
        "random elements",
        "linear",
        "universal simulation",
        "problems",
        "Central Limit Theorem",
        "Markov processes",
        "arbitrary target distribution",
        "approximation errors"
      ]
    }
  },
  {
    "sim": 0.43600019417141433,
    "gen": {
      "title": "Entangled Watermarks as a Defense against Model Extraction",
      "url": "https://www.semanticscholar.org/paper/7fbe6eed6b8e5758ccf9398746a064e3f382ba2b",
      "abstract": "Machine learning involves expensive data collection and training procedures. Model owners may be concerned that valuable intellectual property can be leaked if adversaries mount model extraction attacks. Because it is difficult to defend against model extraction without sacrificing significant prediction accuracy, watermarking leverages unused model capacity to have the model overfit to outlier input-output pairs, which are not sampled from the task distribution and are only known to the defender. The defender then demonstrates knowledge of the input-output pairs to claim ownership of the model at inference. The effectiveness of watermarks remains limited because they are distinct from the task distribution and can thus be easily removed through compression or other forms of knowledge transfer. \nWe introduce Entangled Watermarking Embeddings (EWE). Our approach encourages the model to learn common features for classifying data that is sampled from the task distribution, but also data that encodes watermarks. An adversary attempting to remove watermarks that are entangled with legitimate data is also forced to sacrifice performance on legitimate data. Experiments on MNIST, Fashion-MNIST, and Google Speech Commands validate that the defender can claim model ownership with 95% confidence after less than 10 queries to the stolen copy, at a modest cost of 1% accuracy in the defended model's performance.",
      "fieldsOfStudy": [
        "Computer Science",
        "Mathematics"
      ],
      "topics": [
        "legitimate data",
        "model extraction",
        "unused model capacity",
        "Model owners",
        "data",
        "significant prediction accuracy",
        "performance",
        "knowledge transfer",
        "watermarks",
        "forms",
        "ownership",
        "outlier input-output pairs",
        "knowledge",
        "valuable intellectual property",
        "model extraction attacks",
        "expensive data collection and training procedures"
      ]
    },
    "org": {
      "title": "Optimization in SMT with ${\\mathcal LA}$ (\u211a) Cost Functions",
      "url": "https://www.semanticscholar.org/paper/869d5612c019788623ca856b92b3c03ea9fe572c",
      "abstract": null,
      "fieldsOfStudy": [
        "Computer Science"
      ]
    }
  },
  {
    "sim": 0.41729481645511735,
    "gen": {
      "title": "Intelligent Data Engineering and Automated Learning - IDEAL 2007, 8th International Conference, Birmingham, UK, December 16-19, 2007, Proceedings",
      "url": "https://www.semanticscholar.org/paper/207516bb54dcabd03997be29948a3c53f20dad7d",
      "abstract": null,
      "fieldsOfStudy": [
        "Computer Science"
      ]
    },
    "org": {
      "title": "Light Field Salient Object Detection: A Review and Benchmark",
      "url": "https://www.semanticscholar.org/paper/610157d3f619e3f39df817d345b29c7adfff164d",
      "abstract": null,
      "fieldsOfStudy": [
        "Computer Science"
      ]
    }
  },
  {
    "sim": 0.6091635037737978,
    "gen": {
      "title": "MIFAS: Multi\u2010source heterogeneous information fusion with adaptive importance sampling for link prediction",
      "url": "https://www.semanticscholar.org/paper/224b227f1c4926f8833b27feff767f371fbd860c",
      "abstract": "Link prediction plays an important role in constructing knowledge graph. Recently, graph representation learning models yield state\u2010of\u2010the\u2010art results. However, existing models concentrate merely on triples or graph structures and mostly ignore textual descriptions, resulting in incomplete or partial information. In this paper, we propose a novel graph representation learning model to address this challenge, namely multi\u2010source heterogeneous information fusion with adaptive importance sampling. Our model leverages multiple sources, such triple, graph structure and textual description, and generate rich\u2010attribute embeddings for entities, encapsulating relations simultaneously. We also propose an adaptive importance sampling algorithm to boost aggregation of useful features from local neighbours. Additionally, we also boost node aggregation of useful features from local neighbours by adaptive importance sampling algorithm in our model. Experimental results on two benchmark datasets show that our proposed model significantly outperforms state\u2010of\u2010the\u2010art methods.",
      "fieldsOfStudy": [
        "Computer Science"
      ],
      "topics": [
        "adaptive importance",
        "graph structures",
        "heterogeneous information fusion",
        "local neighbours",
        "knowledge graph",
        "existing models",
        "useful features",
        "textual descriptions",
        "state\u2010of\u2010the\u2010art results",
        "state\u2010of\u2010the\u2010art methods",
        "algorithm",
        "novel graph representation learning model",
        "graph representation learning models",
        "adaptive importance sampling algorithm",
        "aggregation"
      ]
    },
    "org": {
      "title": "A Spectral Framework for Anomalous Subgraph Detection",
      "url": "https://www.semanticscholar.org/paper/cbddf9ea4385ba40407d1a2c9928d6e69d87484a",
      "abstract": "A wide variety of application domains is concerned with data consisting of entities and their relationships or connections, formally represented as graphs. Within these diverse application areas, a common problem of interest is the detection of a subset of entities whose connectivity is anomalous with respect to the rest of the data. While the detection of such anomalous subgraphs has received a substantial amount of attention, no application-agnostic framework exists for analysis of signal detectability in graph-based data. In this paper, we describe a framework that enables such analysis using the principal eigenspace of a graph's residuals matrix, commonly called the modularity matrix in community detection. Leveraging this analytical tool, we show that the framework has a natural power metric in the spectral norm of the anomalous subgraph's adjacency matrix (signal power) and of the background graph's residuals matrix (noise power). We propose several algorithms based on spectral properties of the residuals matrix, with more computationally expensive techniques providing greater detection power. Detection and identification performance are presented for a number of signal and noise models, including clusters and bipartite foregrounds embedded into simple random backgrounds, as well as graphs with community structure and realistic degree distributions. The trends observed verify intuition gleaned from other signal processing areas, such as greater detection power when the signal is embedded within a less active portion of the background. We demonstrate the utility of the proposed techniques in detecting small, highly anomalous subgraphs in real graphs derived from Internet traffic and product co-purchases.",
      "fieldsOfStudy": [
        "Mathematics",
        "Computer Science"
      ],
      "topics": [
        "graphs",
        "real graphs",
        "greater detection power",
        "signal power",
        "community detection",
        "realistic degree distributions",
        "noise power",
        "simple random backgrounds",
        "anomalous subgraphs",
        "community structure",
        "signal detectability",
        "signal processing areas",
        "data",
        "graph-based data",
        "background graphs residuals matrix",
        "Detection"
      ]
    }
  },
  {
    "sim": 0.5334650986726666,
    "gen": {
      "title": "A Neural Network Approach to Jointly Modeling Social Networks and Mobile Trajectories",
      "url": "https://www.semanticscholar.org/paper/d34549fed1f15caba0727d0a0678ff016a42ad10",
      "abstract": "Two characteristics of location-based services are mobile trajectories and the ability to facilitate social networking. The recording of trajectory data contributes valuable resources towards understanding users\u2019 geographical movement behaviors. Social networking is possible when users are able to quickly connect to anyone nearby. A social network with location based services is known as location-based social network (LBSN). As shown in Cho et al. [2013], locations that are frequently visited by socially related persons tend to be correlated, which indicates the close association between social connections and trajectory behaviors of users in LBSNs. To better analyze and mine LBSN data, we need to have a comprehensive view of each of these two aspects, i.e., the mobile trajectory data and the social network. Specifically, we present a novel neural network model that can jointly model both social networks and mobile trajectories. Our model consists of two components: the construction of social networks and the generation of mobile trajectories. First we adopt a network embedding method for the construction of social networks: a networking representation can be derived for a user. The key to our model lies in generating mobile trajectories. Second, we consider four factors that influence the generation process of mobile trajectories: user visit preference, influence of friends, short-term sequential contexts, and long-term sequential contexts. To characterize the last two contexts, we employ the RNN and GRU models to capture the sequential relatedness in mobile trajectories at the short or long term levels. Finally, the two components are tied by sharing the user network representations. Experimental results on two important applications demonstrate the effectiveness of our model. In particular, the improvement over baselines is more significant when either network structure or trajectory data is sparse.",
      "fieldsOfStudy": [
        "Computer Science"
      ],
      "topics": [
        "mobile trajectories",
        "trajectory behaviors",
        "trajectory data",
        "users",
        "generating mobile trajectories",
        "social networking",
        "social connections",
        "users\u2019 geographical movement behaviors",
        "user network representations",
        "short-term sequential contexts",
        "location-based social network",
        "user visit preference",
        "geographical movement behaviors",
        "mobile trajectory data"
      ]
    },
    "org": {
      "title": "Learning Post-Hoc Causal Explanations for Recommendation",
      "url": "https://www.semanticscholar.org/paper/7f24e1793db4f2510fd3412f2d87cb9273692033",
      "abstract": "State-of-the-art recommender systems have the ability to generate high-quality recommendations, but usually cannot provide intuitive explanations to humans due to the usage of black-box prediction models. The lack of transparency has highlighted the critical importance of improving the explainability of recommender systems. In this paper, we propose to extract causal rules from the user interaction history as post-hoc explanations for the black-box sequential recommendation mechanisms, whilst maintain the predictive accuracy of the recommendation model. Our approach firstly achieves counterfactual examples with the aid of a perturbation model, and then extracts personalized causal relationships for the recommendation model through a causal rule mining algorithm. Experiments are conducted on several state-of-the-art sequential recommendation models and real-world datasets to verify the performance of our model on generating causal explanations. Meanwhile, We evaluate the discovered causal explanations in terms of quality and fidelity, which show that compared with conventional association rules, causal rules can provide personalized and more effective explanations for the behavior of black-box recommendation models.",
      "fieldsOfStudy": [
        "Computer Science"
      ],
      "topics": [
        "causal explanations",
        "causal rules",
        "intuitive explanations",
        "post-hoc explanations",
        "conventional association rules",
        "personalized causal relationships",
        "black-box prediction models",
        "recommendation model",
        "causal rule mining algorithm",
        "high-quality recommendations",
        "quality",
        "black-box sequential recommendation mechanisms",
        "recommender systems",
        "discovered causal explanations"
      ]
    }
  },
  {
    "sim": 0.7963998052321064,
    "gen": {
      "title": "Gaussian Interference Channel Capacity to Within One Bit",
      "url": "https://www.semanticscholar.org/paper/602905a5f42f77ef16cd160fac9a3fe33abbb845",
      "abstract": "The capacity of the two-user Gaussian interference channel has been open for 30 years. The understanding on this problem has been limited. The best known achievable region is due to Han and Kobayashi but its characterization is very complicated. It is also not known how tight the existing outer bounds are. In this work, we show that the existing outer bounds can in fact be arbitrarily loose in some parameter ranges, and by deriving new outer bounds, we show that a very simple and explicit Han-Kobayashi type scheme can achieve to within a single bit per second per hertz (bit/s/Hz) of the capacity for all values of the channel parameters. We also show that the scheme is asymptotically optimal at certain high signal-to-noise ratio (SNR) regimes. Using our results, we provide a natural generalization of the point-to-point classical notion of degrees of freedom to interference-limited scenarios.",
      "fieldsOfStudy": [
        "Computer Science",
        "Mathematics"
      ],
      "topics": [
        "new outer bounds",
        "Kobayashi",
        "existing outer bounds",
        "Han",
        "point",
        "SNR",
        "hertz",
        "second",
        "channel parameters",
        "fact",
        "freedom",
        "noise",
        "degrees",
        "interference-limited scenarios",
        "single bit",
        "bit",
        "Hz",
        "parameter ranges"
      ]
    },
    "org": {
      "title": "On the Performance of Low Density Parity Check Codes for Gaussian Interference Channels",
      "url": "https://www.semanticscholar.org/paper/1c5f3670a62ac36e679e5269f7b2539f44743a7d",
      "abstract": "In this paper, two-user Gaussian interference channel(GIC) is revisited with the objective of developing implementable (explicit) channel codes. Specifically, low density parity check (LDPC) codes are adopted for use over these channels, and their benefits are studied. Different scenarios on the level of interference are considered. In particular, for strong interference channel examples with binary phase shift keying (BPSK), it is demonstrated that rates better than those offered by single user codes with time sharing are achievable. Promising results are also observed with quadrature-shift-keying (QPSK). Under general interference a Han-Kobayashi coding based scheme is employed splitting the information into public and private parts, and utilizing appropriate iterative decoders at the receivers. Using QPSK modulation at the two transmitters, it is shown that rate points higher than those achievable by time sharing are obtained.",
      "fieldsOfStudy": [
        "Mathematics",
        "Computer Science"
      ],
      "topics": [
        "time sharing",
        "single user codes",
        "appropriate iterative decoders",
        "binary phase shift keying",
        "QPSK modulation",
        "strong interference",
        "general interference",
        "interference",
        "use",
        "implementable (explicit) channel codes",
        "QPSK",
        "BPSK",
        "public and private parts",
        "low density parity check (LDPC) codes",
        "examples",
        "strong interference channel examples",
        "rate points",
        "rates"
      ]
    }
  },
  {
    "sim": 0.7143640739500121,
    "gen": {
      "title": "Energy-Efficient Relay Selection and Power Allocation for Two-Way Relay Channel with Analog Network Coding",
      "url": "https://www.semanticscholar.org/paper/cfa9521bf5a536331515e28888906ff50dfb9a33",
      "abstract": "In this letter, we consider a two-way relay channel (TWRC) with two end nodes and k relay nodes, where end nodes have the full channel-state information (CSI) and relay nodes only have the channel-amplitude information (CAI). With the objective of minimizing transmit power consumption at required end-to-end rates, energy-efficient relay selection (RS) and power allocation (PA) scheme is studied for TWRC based on analog network coding (ANC). Firstly, we propose an energy-efficient single RS and PA (E-SRS-PA) scheme, where the best relay node is selected to minimize total transmit power. Then, we prove that E-SRS-PA scheme is the optimal energy-efficient RS and PA (OE-RS-PA) scheme in ANC-based TWRC, and thus the optimal number of relay nodes to be selected in energy efficiency sense is equal to one. In addition, the closed-form expressions of optimal power allocation of E-SRS-PA scheme are derived. Numerical simulations confirm the optimality of proposed E-SRS-PA and demonstrate the energy efficiency of ANC-based TWRC compared with the other relaying schemes.",
      "fieldsOfStudy": [
        "Computer Science"
      ],
      "topics": [
        "k relay nodes",
        "end nodes",
        "optimal power allocation",
        "energy efficiency sense",
        "PA",
        "total transmit power",
        "E-SRS-PA scheme",
        "ANC",
        "transmit power consumption",
        "end",
        "TWRC",
        "analog network coding",
        "best relay node",
        "RS",
        "ANC-based TWRC",
        "proposed E-SRS-PA"
      ]
    },
    "org": {
      "title": "On linear coherent estimation with spatial collaboration",
      "url": "https://www.semanticscholar.org/paper/3e97a395fdd74bde49fcbfb5b899df3a8d7dae2a",
      "abstract": "We consider a power-constrained sensor network, consisting of multiple sensor nodes and a fusion center (FC), that is deployed for the purpose of estimating a common random parameter of interest. In contrast to the distributed framework, the sensor nodes are allowed to update their individual observations by (linearly) combining observations from neighboring nodes. The updated observations are communicated to the FC using an analog amplify-and-forward modulation scheme and through a coherent multiple access channel. The optimal collaborative strategy is obtained by minimizing the cumulative transmission power subject to a maximum distortion constraint. For the distributed scenario (i.e., with no observation sharing), the solution reduces to the power-allocation problem considered by Xiao et. al.. Collaboration among neighbors significantly improves power efficiency of the network in the low local-SNR regime, as demonstrated through an insightful example and numerical simulations.",
      "fieldsOfStudy": [
        "Computer Science",
        "Mathematics"
      ],
      "topics": [
        "multiple sensor nodes",
        "numerical simulations",
        "neighboring nodes",
        "power efficiency",
        "interest",
        "Xiao et",
        "observations",
        "FC",
        "coherent multiple access channel",
        "cumulative transmission power subject",
        "common random parameter",
        "sensor nodes",
        "maximum distortion constraint",
        "power-constrained sensor network",
        "The updated observations",
        "Xiao et. al",
        "cumulative transmission power",
        "Xiao"
      ]
    }
  },
  {
    "sim": 0.4640607110079783,
    "gen": {
      "title": "Efficient Discovery of Spectrum Opportunities with MAC-Layer Sensing in Cognitive Radio Networks",
      "url": "https://www.semanticscholar.org/paper/f7a824ca588388a64fbdcf6c115d3e350f511bef",
      "abstract": "Sensing/monitoring of spectrum-availability has been identified as a key requirement for dynamic spectrum allocation in cognitive radio networks (CRNs). An important issue associated with MAC-layer sensing in CRNs is how often to sense the availability of licensed channels and in which order to sense those channels. To resolve this issue, we address (1) how to maximize the discovery of spectrum opportunities by sensing-period adaptation and (2) how to minimize the delay in finding an available channel. Specifically, we develop a sensing-period optimization mechanism and an optimal channel-sequencing algorithm, as well as an environment- adaptive channel-usage pattern estimation method. Our simulation results demonstrate the efficacy of the proposed schemes and its significant performance improvement over nonoptimal schemes. The sensing-period optimization discovers more than 98 percent of the analytical maximum of discoverable spectrum-opportunities, regardless of the number of channels sensed. For the scenarios tested, the proposed scheme is shown to discover up to 22 percent more opportunities than nonoptimal schemes, which may become even greater with a proper choice of initial sensing periods. The idle-channel discovery delay with the optimal channel-sequencing technique ranges from 0.08 to 0.35 seconds under the tested scenarios, which is much faster than nonoptimal schemes. Moreover, our estimation method is shown to track time-varying channel-parameters accurately.",
      "fieldsOfStudy": [
        "Computer Science"
      ],
      "topics": [
        "licensed channels",
        "channels",
        "nonoptimal schemes",
        "initial sensing periods",
        "spectrum opportunities",
        "dynamic spectrum allocation",
        "environment- adaptive channel-usage pattern estimation method",
        "available channel",
        "nonoptimal",
        "CRNs",
        "cognitive radio networks",
        "optimal channel-sequencing algorithm",
        "optimal channel-sequencing technique",
        "time-varying channel-parameters",
        "channels",
        "period adaptation",
        "Sensing"
      ]
    },
    "org": {
      "title": "Two Birds with One Network: Unifying Failure Event Prediction and Time-to-failure Modeling",
      "url": "https://www.semanticscholar.org/paper/f2691f11b65f49b04bbff87e9c90dc15c55c86f1",
      "abstract": "One of the key challenges in predictive maintenance is to predict the impending downtime of an equipment with a reasonable prediction horizon so that countermeasures can be put in place. Classically, this problem has been posed in two different ways which are typically solved independently: (1) Remaining useful life (RUL) estimation as a long-term prediction task to estimate how much time is left in the useful life of the equipment and (2) Failure prediction (FP) as a short-term prediction task to assess the probability of a failure within a pre-specified time window. As these two tasks are related, performing them separately is sub-optimal and might results in inconsistent predictions for the same equipment. In order to alleviate these issues, we propose two methods: Deep Weibull model (DW-RNN) and multi-task learning (MTL-RNN). DW-RNN is able to learn the underlying failure dynamics by fitting Weibull distribution parameters using a deep neural network, learned with a survival likelihood, without training directly on each task. While DW-RNN makes an explicit assumption on the data distribution, MTL-RNN exploits the implicit relationship between the long-term RUL and short-term FP tasks to learn the underlying distribution. Additionally, both our methods can leverage the non-failed equipment data for RUL estimation. We demonstrate that our methods consistently outperform baseline RUL methods that can be used for FP while producing consistent results for RUL and FP. We also show that our methods perform at par with baselines trained on the objectives optimized for either of the two tasks.",
      "fieldsOfStudy": [
        "Computer Science",
        "Mathematics"
      ],
      "topics": [
        "RUL estimation",
        "inconsistent predictions",
        "fitting Weibull distribution parameters",
        "baseline RUL methods",
        "RUL",
        "FP",
        "multi-task learning",
        "short-term prediction task",
        "consistent results",
        "pre-specified time window",
        "long-term RUL and short-term FP tasks",
        "place",
        "Deep Weibull model",
        "non-failed equipment data",
        "Weibull distribution parameters",
        "useful life (RUL"
      ]
    }
  },
  {
    "sim": 0.5793939756014865,
    "gen": {
      "title": "Chapter 4 Statistical Complexity and Fisher-Shannon Information : Applications",
      "url": "https://www.semanticscholar.org/paper/beac154e91070ff3eb733f6c113ae1f166206a61",
      "abstract": "In this chapter, a statistical measure of complexity and the FisherShannon information product are introduced and their properties are discussed. These measures are based on the interplay between the Shannon information, or a function of it, and the separation of the set of accessible states to a system from the equiprobability distribution, i.e. the disequilibrium or the Fisher information, respectively. Different applications in discrete and continuous systems are shown. Some of them are concerned with quantum systems, from prototypical systems such as the H-atom, the harmonic oscillator and the square well to other ones such as Helike ions, Hooke\u2019s atoms or just the periodic table. In all of them, these statistical indicators show an interesting behavior able to discern and highlight some conformational properties of those systems. 4.1 A Statistical Measure of Complexity. Some Applications This century has been told to be the century of complexity [1]. Nowadays the question \u201cwhat is complexity?\u201d is circulating over the scientific crossroads of physics, biology, mathematics and computer science, although under the present understanding of the world could be no urgent to answer this question. However, many different points of view have been developed to this respect and hence a lot of different answers can be found in the literature. Here we explain in detail one of these options. On the most basic grounds, an object, a procedure, or system is said to be \u201ccomplex\u201d when it does not match patterns regarded as simple. This sounds rather like an oxymoron but common knowledge tells us what is simple and complex: simplified systems or idealizations are always a starting point to solve scientific problems. The notion of \u201ccomplexity\u201d in physics [2, 3] starts by considering the perfect crystal R. L\u00f3pez-Ruiz ( ) Department of Computer Science, Faculty of Science and BIFI, University of Zaragoza, Zaragoza 50009, Spain e-mail: rilopez@unizar.es K.D. Sen (ed.), Statistical Complexity, DOI 10.1007/978-90-481-3890-6_4, \u00a9 Springer Science+Business Media B.V. 2011 65 66 R. L\u00f3pez-Ruiz et al. and the isolated ideal gas as examples of simple models and therefore as systems with zero \u201ccomplexity\u201d. Let us briefly recall their main characteristics with \u201corder\u201d, \u201cinformation\u201d and \u201cequilibrium\u201d. A perfect crystal is completely ordered and the atoms are arranged following stringent rules of symmetry. The probability distribution for the states accessible to the perfect crystal is centered around a prevailing state of perfect symmetry. A small piece of \u201cinformation\u201d is enough to describe the perfect crystal: the distances and the symmetries that define the elementary cell. The \u201cinformation\u201d stored in this system can be considered minimal. On the other hand, the isolated ideal gas is completely disordered. The system can be found in any of its accessible states with the same probability. All of them contribute in equal measure to the \u201cinformation\u201d stored in the ideal gas. It has therefore a maximum \u201cinformation\u201d. These two simple systems are extrema in the scale of \u201corder\u201d and \u201cinformation\u201d. It follows that the definition of \u201ccomplexity\u201d must not be made in terms of just \u201corder\u201d or \u201cinformation\u201d. It might seem reasonable to propose a measure of \u201ccomplexity\u201d by adopting some kind of distance from the equiprobable distribution of the accessible states of the system. Defined in this way, \u201cdisequilibrium\u201d would give an idea of the probabilistic hierarchy of the system. \u201cDisequilibrium\u201d would be different from zero if there are privileged, or more probable, states among those accessible. But this would not work. Going back to the two examples we began with, it is readily seen that a perfect crystal is far from an equidistribution among the accessible states because one of them is totally prevailing, and so \u201cdisequilibrium\u201d would be maximum. For the ideal gas, \u201cdisequilibrium\u201d would be zero by construction. Therefore such a distance or \u201cdisequilibrium\u201d (a measure of a probabilistic hierarchy) cannot be directly associated with \u201ccomplexity\u201d. In Fig. 4.1 we sketch an intuitive qualitative behavior for \u201cinformation\u201d H and \u201cdisequilibrium\u201d D for systems ranging from the perfect crystal to the ideal gas. This graph suggests that the product of these two quantities could be used as a measure of \u201ccomplexity\u201d: C = H \u00b7 D. The function C has indeed the features and asymtotical properties that one would expect intuitively: it vanishes for the perfect crystal and for the isolated ideal gas, and it is different from zero for the rest of the systems of particles. We will follow these guidelines to establish a quantitative measure of \u201ccomplexity\u201d. Before attempting any further progress, however, we must recall that \u201ccomplexity\u201d cannot be measured univocally, because it depends on the nature of the description (which always involves a reductionist process) and on the scale of observation. Let us take an example to illustrate this point. A computer chip can look very different at different scales. It is an entangled array of electronic elements at microscopic scale but only an ordered set of pins attached to a black box at a macroscopic scale. We shall now discuss a measure of \u201ccomplexity\u201d based on the statistical description of systems. Let us assume that the system has N accessible states {x1, x2, . . . , xN } when observed at a given scale. We will call this an N -system. 4 Statistical Complexity and Fisher-Shannon Information: Applications 67 Fig. 4.1 Sketch of the intuitive notion of the magnitudes of \u201cinformation\u201d (H ) and \u201cdisequilibrium\u201d (D) for the physical systems and the behavior intuitively required for the magnitude \u201ccomplexity\u201d. The quantity C = H \u00b7 D is proposed to measure such a magnitude Our understanding of the behavior of this system determines the corresponding probabilities {p1,p2, . . . , pN } (with the condition \u2211Ni=1 pi = 1) of each state (pi > 0 for all i). Then the knowledge of the underlying physical laws at this scale is incorporated into a probability distribution for the accessible states. It is possible to find a quantity measuring the amount of \u201cinformation\u201d. Under to the most elementary conditions of consistency, Shannon [4] determined the unique function H(p1,p2, . . . , pN) that accounts for the \u201cinformation\u201d stored in a system:",
      "fieldsOfStudy": null,
      "topics": [
        "systems",
        "simplified systems",
        "prototypical systems",
        "N accessible states",
        "quantum systems",
        "information",
        "perfect symmetry",
        "states",
        "different scales",
        "complexity",
        "microscopic scale",
        "disequilibrium",
        "different points",
        "accessible states"
      ]
    },
    "org": {
      "title": "Community detection in networks with unequal groups",
      "url": "https://www.semanticscholar.org/paper/688796e1c5240f7650ed3d1e658c091a36ceca85",
      "abstract": "Recently, a phase transition has been discovered in the network community detection problem below which no algorithm can tell which nodes belong to which communities with success any better than a random guess. This result has, however, so far been limited to the case where the communities have the same size or the same average degree. Here we consider the case where the sizes or average degrees differ. This asymmetry allows us to assign nodes to communities with better-than-random success by examining their local neighborhoods. Using the cavity method, we show that this removes the detectability transition completely for networks with four groups or fewer, while for more than four groups the transition persists up to a critical amount of asymmetry but not beyond. The critical point in the latter case coincides with the point at which local information percolates, causing a global transition from a less-accurate solution to a more-accurate one.",
      "fieldsOfStudy": [
        "Mathematics",
        "Medicine",
        "Computer Science",
        "Physics"
      ],
      "topics": [
        "communities",
        "average degrees",
        "nodes",
        "success",
        "asymmetry",
        "network community detection problem",
        "networks",
        "less-accurate solution",
        "average degree",
        "global transition",
        "detectability transition",
        "local neighborhoods",
        "random guess",
        "phase transition",
        "size",
        "local information"
      ]
    }
  },
  {
    "sim": 0.5917668395723976,
    "gen": {
      "title": "Minimizing finite sums with the stochastic average gradient",
      "url": "https://www.semanticscholar.org/paper/73068d3d5dacf987848eadd9af5b5fad8f7cf9c6",
      "abstract": null,
      "fieldsOfStudy": [
        "Mathematics",
        "Computer Science"
      ]
    },
    "org": {
      "title": "hIPPYlib: An Extensible Software Framework for Large-Scale Inverse Problems",
      "url": "https://www.semanticscholar.org/paper/c500edc76927e24faa3839ee9e291ec190d1f0a5",
      "abstract": "We present an extensible software framework, hIPPYlib, for solution of large-scale deterministic and Bayesian inverse problems governed by partial differential equations (PDEs) with infinite-dimensional parameter fields (which are high-dimensional after discretization). hIPPYlib overcomes the prohibitive nature of Bayesian inversion for this class of problems by implementing state-of-the-art scalable algorithms for PDE-based inverse problems that exploit the structure of the underlying operators, notably the Hessian of the log-posterior. The key property of the algorithms implemented in hIPPYlib is that the solution of the deterministic and linearized Bayesian inverse problem is computed at a cost, measured in linearized forward PDE solves, that is independent of the parameter dimension. The mean of the posterior is approximated by the MAP point, which is found by minimizing the negative log-posterior. This deterministic nonlinear least-squares optimization problem is solved with an inexact matrix-free Newton-CG method. The posterior covariance is approximated by the inverse of the Hessian of the negative log posterior evaluated at the MAP point. This Gaussian approximation is exact when the parameter-to-observable map is linear; otherwise, its logarithm agrees to two derivatives with the log-posterior at the MAP point, and thus it can serve as a proposal for Hessian-based MCMC methods. The construction of the posterior covariance is made tractable by invoking a low-rank approximation of the Hessian of the log-likelihood. Scalable tools for sample generation are also implemented. hIPPYlib makes all of these advanced algorithms easily accessible to domain scientists and provides an environment that expedites the development of new algorithms. hIPPYlib is also a teaching tool to educate researchers and practitioners who are new to inverse problems and the Bayesian inference framework.",
      "fieldsOfStudy": [
        "Computer Science",
        "Mathematics"
      ],
      "topics": [
        "inverse problems",
        "problems",
        "new algorithms",
        "Hessian",
        "MAP",
        "Bayesian inversion",
        "Hessian-based MCMC methods",
        "PDE-based inverse problems",
        "Bayesian",
        "partial differential equations",
        "MAP point",
        "infinite-dimensional parameter fields",
        "deterministic and linearized Bayesian inverse problem",
        "discretization",
        "negative log",
        "linearized forward PDE solves",
        "observable map"
      ]
    }
  },
  null,
  null,
  {
    "sim": 0.34523323812488493,
    "gen": {
      "title": "Machine learning based channel modeling for molecular MIMO communications",
      "url": "https://www.semanticscholar.org/paper/f042045f01e58cfed2dcf4214347b08281caeb2f",
      "abstract": "In diffusion-based molecular communication, information particles locomote via a diffusion process, characterized by random movement and heavy tail distribution for the random arrival time. As a result, the molecular communication shows lower transmission rates than the traditional communication. To compensate for such low rates, researchers have recently proposed the molecular multiple-input multiple-output (MIMO) technique. Although channel models exist for single-input single-output (SISO) systems for some simple environments, extending the results to multiple molecular emitters complicates the modeling process. In this paper, we introduce a novel machine learning technique for modeling the molecular MIMO channel and confirm the effectiveness via extensive numerical studies.",
      "fieldsOfStudy": [
        "Computer Science",
        "Engineering"
      ],
      "topics": [
        "multiple molecular emitters",
        "random movement",
        "heavy tail distribution",
        "extensive numerical studies",
        "lower transmission rates",
        "low rates",
        "random arrival time",
        "diffusion-based molecular communication",
        "information particles",
        "molecular MIMO channel",
        "channel models",
        "molecular communication",
        "novel machine learning technique",
        "diffusion process",
        "modeling process"
      ]
    },
    "org": {
      "title": "Least costly energy management for series hybrid electric vehicles",
      "url": "https://www.semanticscholar.org/paper/21f1925caaa7e9942e4044920d485d1243644ce4",
      "abstract": null,
      "fieldsOfStudy": [
        "Computer Science",
        "Engineering",
        "Mathematics"
      ]
    }
  },
  null,
  {
    "sim": 0.7207715557089205,
    "gen": {
      "title": "Granularity of Locks and Degrees of Consistency in a Shared Data Base",
      "url": "https://www.semanticscholar.org/paper/b0d7bfd07752108b53d885c2835004d49ca693c9",
      "abstract": "The problem of choosing the appropriate Hranularit~ (size) of lockable objects is introduced and the tradeoff between concurrency and overhead is discusseS. A locking protocol which allows simultaneous locking at various granularities by different transactions is presented. It is based on the introduction of additional lock modes besides the conventional share mode an5 exclusive mode. A proof is given of the equivalence of this protocol to a conventional one.",
      "fieldsOfStudy": [
        "Computer Science"
      ],
      "topics": [
        "additional lock modes",
        "conventional share mode an5 exclusive mode",
        "different transactions",
        "lockable objects",
        "granularities",
        "simultaneous locking",
        "concurrency",
        "conventional one",
        "A locking protocol",
        "appropriate Hranularit~ (size",
        "tradeoff",
        "protocol",
        "equivalence",
        "introduction",
        "A proof",
        "conventional share mode",
        "an5 exclusive mode",
        "overhead",
        "size",
        "appropriate Hranularit~"
      ]
    },
    "org": {
      "title": "Oh-RAM! One and a Half Round Atomic Memory",
      "url": "https://www.semanticscholar.org/paper/fb160f568b285ed9adf05228ce3ad89cb3e8db42",
      "abstract": null,
      "fieldsOfStudy": [
        "Computer Science"
      ]
    }
  },
  null,
  {
    "sim": 0.7832894337173631,
    "gen": {
      "title": "Multi-task Deep Reinforcement Learning with PopArt",
      "url": "https://www.semanticscholar.org/paper/65769b53e71ea7c52b3a07ad32bd4fdade6a0173",
      "abstract": "The reinforcement learning (RL) community has made great strides in designing algorithms capable of exceeding human performance on specific tasks. These algorithms are mostly trained one task at the time, each new task requiring to train a brand new agent instance. This means the learning algorithm is general, but each solution is not; each agent can only solve the one task it was trained on. In this work, we study the problem of learning to master not one but multiple sequentialdecision tasks at once. A general issue in multi-task learning is that a balance must be found between the needs of multiple tasks competing for the limited resources of a single learning system. Many learning algorithms can get distracted by certain tasks in the set of tasks to solve. Such tasks appear more salient to the learning process, for instance because of the density or magnitude of the in-task rewards. This causes the algorithm to focus on those salient tasks at the expense of generality. We propose to automatically adapt the contribution of each task to the agent\u2019s updates, so that all tasks have a similar impact on the learning dynamics. This resulted in state of the art performance on learning to play all games in a set of 57 diverse Atari games. Excitingly, our method learned a single trained policy - with a single set of weights - that exceeds median human performance. To our knowledge, this was the first time a single agent surpassed human-level performance on this multi-task domain. The same approach also demonstrated state of the art performance on a set of 30 tasks in the 3D reinforcement learning platform DeepMind Lab.",
      "fieldsOfStudy": [
        "Computer Science",
        "Mathematics"
      ],
      "topics": [
        "multiple tasks",
        "tasks",
        "specific tasks",
        "certain tasks",
        "Such tasks",
        "multiple sequentialdecision tasks",
        "multi-task learning",
        "median human performance",
        "Many learning algorithms",
        "new task",
        "salient tasks",
        "DeepMind Lab",
        "Atari",
        "30 tasks"
      ]
    },
    "org": {
      "title": "Kickstarting Deep Reinforcement Learning",
      "url": "https://www.semanticscholar.org/paper/ebf3b0c284cb776d89951e4e67a59df6403fc9a6",
      "abstract": "We present a method for using previously-trained 'teacher' agents to kickstart the training of a new 'student' agent. To this end, we leverage ideas from policy distillation and population based training. Our method places no constraints on the architecture of the teacher or student agents, and it regulates itself to allow the students to surpass their teachers in performance. We show that, on a challenging and computationally-intensive multi-task benchmark (DMLab-30), kickstarted training improves the data efficiency of new agents, making it significantly easier to iterate on their design. We also show that the same kickstarting pipeline can allow a single student agent to leverage multiple 'expert' teachers which specialize on individual tasks. In this setting kickstarting yields surprisingly large gains, with the kickstarted agent matching the performance of an agent trained from scratch in almost 10x fewer steps, and surpassing its final performance by 42 percent. Kickstarting is conceptually simple and can easily be incorporated into reinforcement learning experiments.",
      "fieldsOfStudy": [
        "Mathematics",
        "Computer Science"
      ],
      "topics": [
        "student agents",
        "new agents",
        "population based training",
        "performance",
        "individual tasks",
        "training",
        "fewer steps",
        "reinforcement learning experiments",
        "kickstarted agent",
        "scratch",
        "agent",
        "final performance",
        "new student agent",
        "policy distillation",
        "kickstarted training",
        "teacher or student agents",
        "DMLab-30"
      ]
    }
  },
  null,
  {
    "sim": 0.5952133999864342,
    "gen": {
      "title": "What is Twitter, a social network or a news media?",
      "url": "https://www.semanticscholar.org/paper/b2b057987c398ecf332f50f42891161c99e14aa4",
      "abstract": "Twitter, a microblogging service less than three years old, commands more than 41 million users as of July 2009 and is growing fast. Twitter users tweet about any topic within the 140-character limit and follow others to receive their tweets. The goal of this paper is to study the topological characteristics of Twitter and its power as a new medium of information sharing.\n We have crawled the entire Twitter site and obtained 41.7 million user profiles, 1.47 billion social relations, 4,262 trending topics, and 106 million tweets. In its follower-following topology analysis we have found a non-power-law follower distribution, a short effective diameter, and low reciprocity, which all mark a deviation from known characteristics of human social networks [28]. In order to identify influentials on Twitter, we have ranked users by the number of followers and by PageRank and found two rankings to be similar. Ranking by retweets differs from the previous two rankings, indicating a gap in influence inferred from the number of followers and that from the popularity of one's tweets. We have analyzed the tweets of top trending topics and reported on their temporal behavior and user participation. We have classified the trending topics based on the active period and the tweets and show that the majority (over 85%) of topics are headline news or persistent news in nature. A closer look at retweets reveals that any retweeted tweet is to reach an average of 1,000 users no matter what the number of followers is of the original tweet. Once retweeted, a tweet gets retweeted almost instantly on next hops, signifying fast diffusion of information after the 1st retweet.\n To the best of our knowledge this work is the first quantitative study on the entire Twittersphere and information diffusion on it.",
      "fieldsOfStudy": [
        "Computer Science",
        "Political Science"
      ],
      "topics": [
        "user participation",
        "Twitter users",
        "information diffusion",
        "topics",
        "persistent news",
        "users",
        "headline news",
        "information sharing",
        "followers",
        "human social networks",
        "retweets",
        "fast diffusion",
        "information",
        "known characteristics",
        "nature",
        "trending topics"
      ]
    },
    "org": {
      "title": "You Are What You Eat (and Drink): Identifying Cultural Boundaries by Analyzing Food and Drink Habits in Foursquare",
      "url": "https://www.semanticscholar.org/paper/892b6402ba0c42843254242faff64813c6e4d15c",
      "abstract": "\n \n Food and drink are two of the most basic needs of human beings. However, as society evolved, food and drink became also a strong cultural aspect, being able to describe strong differences among people. Traditional methods used to analyze cross-cultural differences are mainly based on surveys and, for this reason, they are very difficult to represent a significant statistical sample at a global scale. In this paper, we propose a new methodology to identify cultural boundaries and similarities across populations at different scales based on the analysis of Foursquare check-ins. This approach might be useful not only for economic purposes, but also to support existing and novel marketing and social applications. Our methodology consists of the following steps. First, we map food and drink related check-ins extracted from Foursquare into users' cultural preferences. Second, we identify particular individual preferences, such as the taste for a certain type of food or drink, e.g., pizza or sake, as well as temporal habits, such as the time and day of the week when an individual goes to a restaurant or a bar. Third, we show how to analyze this information to assess the cultural distance between two countries, cities or even areas of a city. Fourth, we apply a simple clustering technique, using this cultural distance measure, to draw cultural boundaries across countries, cities and regions.\n \n",
      "fieldsOfStudy": [
        "Sociology",
        "Computer Science",
        "Physics",
        "Geography"
      ],
      "topics": [
        "cultural boundaries",
        "cities",
        "cross-cultural differences",
        "different scales",
        "Foursquare",
        "strong differences",
        "users cultural preferences",
        "social applications",
        "particular individual preferences",
        "Foursquare check-ins",
        "regions",
        "strong cultural aspect",
        "human beings",
        "cultural distance measure",
        "temporal habits",
        "drink"
      ]
    }
  },
  null,
  {
    "sim": 0.6450649216477954,
    "gen": {
      "title": "Analyzing the Performance of Mutation Operators to Solve the Travelling Salesman Problem",
      "url": "https://www.semanticscholar.org/paper/c0bf46a895fcb6e2143bf537fb30a32608464276",
      "abstract": "The genetic algorithm includes some parameters that should be adjusted, so as to get reliable results. Choosing a representation of the problem addressed, an initial population, a method of selection, a crossover operator, mutation operator, the probabilities of crossover and mutation, and the insertion method creates a variant of genetic algorithms. Our work is part of the answer to this perspective to find a solution for this combinatorial problem. What are the best parameters to select for a genetic algorithm that creates a variety efficient to solve the Travelling Salesman Problem (TSP)? In this paper, we present a comparative analysis of different mutation operators, surrounded by a dilated discussion that justifying the relevance of genetic operators chosen to solving the TSP problem.",
      "fieldsOfStudy": [
        "Computer Science"
      ],
      "topics": [
        "mutation operator",
        "genetic operators",
        "different mutation operators",
        "TSP",
        "mutation",
        "crossover",
        "reliable results",
        "selection",
        "TSP problem",
        "crossover operator",
        "combinatorial problem",
        "genetic algorithm",
        "initial population"
      ]
    },
    "org": {
      "title": "Long-term evolution of genetic programming populations",
      "url": "https://www.semanticscholar.org/paper/a7df0be2490ef08742aaec2ed152e69d7829d444",
      "abstract": "Evolving binary mux-6 trees for up to 100 000 generations, during which some programs grow to more than a hundred million nodes, suggests the landscape which GP explores contains some very smooth regions. Although the GP population evolves under crossover, our unbounded GP appears not to evolve building blocks. We do see periods of tens even hundreds of generations where even although each member of the population occupies a different point in the genotypic search space, they are lie at exactly the same point in the phenotypic landscape. Phenotypic convergence whilst retaining genotypic diversity is typical of GP and, we suggest inherent in highly redundant variable length representations. Based on technical report RN/17/05 arXiv:1703.08481",
      "fieldsOfStudy": [
        "Computer Science",
        "Mathematics"
      ],
      "topics": [
        "retaining genotypic diversity",
        "building blocks",
        "Phenotypic convergence",
        "GP",
        "highly redundant variable length representations",
        "binary mux-6 trees",
        "lie",
        "generations",
        "genotypic search space",
        "phenotypic landscape",
        "technical report",
        "crossover",
        "different point",
        "GP population",
        "smooth regions",
        "genotypic diversity",
        "unbounded GP"
      ]
    }
  },
  null,
  {
    "sim": 0.5515870385037692,
    "gen": {
      "title": "The Fractional Langevin Equation: Brownian Motion Revisited",
      "url": "https://www.semanticscholar.org/paper/1ebab09a8afd342ff53a8cd1b897b8afb076ccfb",
      "abstract": "We have revisited the Brownian motion on the basis of the fractional Langevin equation which turns out to be a particular case of the generalized Langevin equation introduced by Kubo on 1966. The importance of our approach is to model the Brownian motion more realistically than the usual one based on the classical Langevin equation, in that it takes into account also the retarding effects due to hydrodynamic backflow, i.e. the added mass and the Basset memory drag. On the basis of the two fluctuation-dissipation theorems and of the techniques of the Fractional Calculus we have provided the analytical expressions of the correlation functions (both for the random force and the particle velocity) and of the mean squared particle displacement. The random force has been shown to be represented by a superposition of the usual white noise with a \"fractional\" noise. The velocity correlation function is no longer expressed by a simple exponential but exhibits a slower decay, proportional to $t^{-3/2}$ as $t \\to \\infty$, which indeed is more realistic. Finally, the mean squared displacement has been shown to maintain, for sufficiently long times, the linear behaviour which is typical of normal diffusion, with the same diffusion coefficient of the classical case. However, the Basset history force induces a retarding effect in the establishing of the linear behaviour, which in some cases could appear as a manifestation of anomalous diffusion to be correctly interpreted in experimental measurements.",
      "fieldsOfStudy": [
        "Physics",
        "Mathematics"
      ],
      "topics": [
        "anomalous diffusion",
        "normal diffusion",
        "experimental measurements",
        "Langevin",
        "hydrodynamic backflow",
        "fractional Langevin equation",
        "classical Langevin equation",
        "Kubo",
        "generalized Langevin equation",
        "linear",
        "diffusion coefficient",
        "classical case",
        "account",
        "usual white noise",
        "Basset memory drag",
        "\\infty$"
      ]
    },
    "org": {
      "title": "Effect of Carouseling on Angular Rate Sensor Error Processes",
      "url": "https://www.semanticscholar.org/paper/cc5b5ef59bfb3f4a35d09cdf6edabfe368f13552",
      "abstract": "Carouseling is an efficient method to mitigate the measurement errors of inertial sensors, particularly microelectromechanical systems (MEMS) gyroscopes. In this paper, the effect of carouseling on the most significant stochastic error processes of an MEMS gyroscope, i.e., additive bias, white noise, 1/f noise, and rate random walk (RRW) is investigated. Variance propagation equations for these processes under averaging and carouseling are defined. Furthermore, a novel approach to generating 1/f noise is presented. The experimental results show that carouseling reduces the contributions of additive bias, 1/f noise, and RRW significantly compared with plain averaging, which can be used to improve the accuracy of dead reckoning systems.",
      "fieldsOfStudy": [
        "Computer Science",
        "Engineering"
      ],
      "topics": [
        "white noise",
        "dead reckoning systems",
        "plain averaging",
        "MEMS",
        "RRW",
        "random walk",
        "MEMS gyroscope",
        "inertial sensors",
        "1/f noise",
        "carouseling",
        "Variance propagation equations",
        "i.e., additive bias",
        "significant stochastic error processes",
        "measurement errors",
        "additive bias",
        "averaging",
        "accuracy"
      ]
    }
  },
  {
    "sim": 0.39810291579702084,
    "gen": {
      "title": "Time varying networks and the weakness of strong ties",
      "url": "https://www.semanticscholar.org/paper/5c5bafe5016dd9c2ca212ded568a77064d28711e",
      "abstract": null,
      "fieldsOfStudy": [
        "Physics",
        "Computer Science",
        "Medicine"
      ]
    },
    "org": {
      "title": "A hybrid particle volume-of-fluid method for curvature estimation in multiphase flows",
      "url": "https://www.semanticscholar.org/paper/61730870a14800417e4b21f366f129340bd11587",
      "abstract": null,
      "fieldsOfStudy": [
        "Physics",
        "Computer Science",
        "Mathematics"
      ]
    }
  },
  {
    "sim": 0.6636778445021904,
    "gen": {
      "title": "Maximizing Social Influence in Nearly Optimal Time",
      "url": "https://www.semanticscholar.org/paper/d7f9c3253552e13f24c3b73bc055ef60388af57c",
      "abstract": "Diffusion is a fundamental graph process, underpinning such phenomena as epidemic disease contagion and the spread of innovation by word-of-mouth. We address the algorithmic problem of finding a set of k initial seed nodes in a network so that the expected size of the resulting cascade is maximized, under the standard independent cascade model of network diffusion. Runtime is a primary consideration for this problem due to the massive size of the relevant input networks. \nWe provide a fast algorithm for the influence maximization problem, obtaining the near-optimal approximation factor of (1 - 1/e - epsilon), for any epsilon > 0, in time O((m+n)k log(n) / epsilon^2). Our algorithm is runtime-optimal (up to a logarithmic factor) and substantially improves upon the previously best-known algorithms which run in time Omega(mnk POLY(1/epsilon)). Furthermore, our algorithm can be modified to allow early termination: if it is terminated after O(beta(m+n)k log(n)) steps for some beta < 1 (which can depend on n), then it returns a solution with approximation factor O(beta). Finally, we show that this runtime is optimal (up to logarithmic factors) for any beta and fixed seed size k.",
      "fieldsOfStudy": [
        "Mathematics",
        "Computer Science",
        "Physics"
      ],
      "topics": [
        "network diffusion",
        "epidemic disease contagion",
        "approximation factor",
        "mouth",
        "epsilon^2",
        "phenomena",
        "logarithmic factors",
        "k initial seed nodes",
        "Diffusion",
        "standard independent cascade model",
        "time",
        "word",
        "relevant input networks",
        "log(n",
        "innovation",
        "Omega(mnk POLY(1/epsilon",
        "fixed seed size k.",
        "resulting cascade"
      ]
    },
    "org": {
      "title": "Learning Large-Scale Generalized Hypergeometric Distribution (GHD) DAG models",
      "url": "https://www.semanticscholar.org/paper/13427a8b4509b3d29129d269671f9c5c63777aa0",
      "abstract": "We introduce a new class of identifiable DAG models where the conditional distribution of each node given its parents belongs to a family of generalized hypergeometric distributions (GHD). A family of generalized hypergeometric distributions includes a lot of discrete distributions such as the binomial, Beta-binomial, negative binomial, Poisson, hyper-Poisson, and many more. We prove that if the data drawn from the new class of DAG models, one can fully identify the graph structure. We further present a reliable and polynomial-time algorithm that recovers the graph from finitely many data. We show through theoretical results and numerical experiments that our algorithm is statistically consistent in high-dimensional settings (p>n) if the indegree of the graph is bounded, and out-performs state-of-the-art DAG learning algorithms.",
      "fieldsOfStudy": [
        "Mathematics",
        "Computer Science"
      ],
      "topics": [
        "generalized hypergeometric distributions",
        "DAG models",
        "identifiable DAG models",
        "discrete distributions",
        "DAG",
        "Poisson",
        "Beta-binomial, negative binomial",
        "finitely many data",
        "graph structure",
        "numerical experiments",
        "conditional distribution",
        "high-dimensional settings",
        "GHD",
        "hyper-Poisson",
        "negative binomial",
        "hyper",
        "Beta-binomial"
      ]
    }
  },
  {
    "sim": 0.38280434699044186,
    "gen": {
      "title": "Learning Canonical F -Correlation Projection for Compact Multiview Representation",
      "url": "https://www.semanticscholar.org/paper/3511c7cdcec281cf8f5ff9b8946a8eb5ddaea2f8",
      "abstract": "Canonical correlation analysis (CCA) matters in multiview representation learning. But, CCA and its most variants are essentially based on explicit or implicit covariance matrices. It means that they have no ability to model the nonlinear relationship among features due to intrinsic linearity of covariance. In this paper, we address the preceding problem and propose a novel canonical F -correlation framework by exploring and exploiting the nonlinear relationship between different features. The framework projects each feature rather than observation into a certain new space by an arbitrary nonlinear mapping, thus resulting in more flexibility in real applications. With this framework as a tool, we propose a correlative covariation projection (CCP) method by using an explicit nonlinear mapping. Moreover, we further propose a multiset version of CCP dubbed MCCP for learning compact representation of more than two views. The proposed MCCP is solved by an iterative method, and we prove the convergence of this iteration. A series of experimental results on six benchmark datasets demonstrate the effectiveness of our proposed CCP and MCCP methods.",
      "fieldsOfStudy": null,
      "topics": [
        "real applications",
        "flexibility",
        "different features",
        "covariance",
        "multiview representation learning",
        "features",
        "compact representation",
        "explicit or implicit covariance matrices",
        "intrinsic linearity",
        "explicit nonlinear mapping",
        "arbitrary nonlinear mapping",
        "MCCP",
        "Canonical correlation analysis",
        "nonlinear relationship"
      ]
    },
    "org": {
      "title": "Suppression effect on explosive percolations",
      "url": "https://www.semanticscholar.org/paper/0ecc00e3c5c7296a3731868440be3b9937c30fb3",
      "abstract": "Percolation transitions (PTs) of networks, leading to the formation of a macroscopic cluster, are conventionally considered to be continuous transitions. However, a modified version of the classical random graph model was introduced in which the growth of clusters was suppressed, and a PT occurs explosively at a delayed transition point. Whether the explosive PT is indeed discontinuous or continuous becomes controversial. Here, we show that the behavior of the explosive PT depends on detailed dynamic rules. Thus, when dynamic rules are designed to suppress the growth of all clusters, the discontinuity of the order parameter tends to a finite value as the system size increases, indicating that the explosive PT could be discontinuous.",
      "fieldsOfStudy": [
        "Physics",
        "Medicine",
        "Computer Science"
      ],
      "topics": [
        "continuous transitions",
        "Percolation transitions",
        "detailed dynamic rules",
        "dynamic rules",
        "delayed transition point",
        "PTs",
        "networks",
        "system size increases",
        "macroscopic cluster",
        "explosive PT",
        "finite value",
        "classical random graph model",
        "clusters",
        "order parameter",
        "system size",
        "percolation transitions"
      ]
    }
  },
  {
    "sim": 0.5107171158915847,
    "gen": {
      "title": "Why is Differential Evolution Better than Grid Search for Tuning Defect Predictors?",
      "url": "https://www.semanticscholar.org/paper/215da0f8e588e02099a80198f6092e6b8388280e",
      "abstract": "Context: One of the black arts of data mining is learning the magic parameters which control the learners. In software analytics, at least for defect prediction, several methods, like grid search and differential evolution (DE), have been proposed to learn these parameters, which has been proved to be able to improve the performance scores of learners. \nObjective: We want to evaluate which method can find better parameters in terms of performance score and runtime cost. \nMethods: This paper compares grid search to differential evolution, which is an evolutionary algorithm that makes extensive use of stochastic jumps around the search space. \nResults: We find that the seemingly complete approach of grid search does no better, and sometimes worse, than the stochastic search. When repeated 20 times to check for conclusion validity, DE was over 210 times faster than grid search to tune Random Forests on 17 testing data sets with F-Measure \nConclusions: These results are puzzling: why does a quick partial search be just as effective as a much slower, and much more, extensive search? To answer that question, we turned to the theoretical optimization literature. Bergstra and Bengio conjecture that grid search is not more effective than more randomized searchers if the underlying search space is inherently low dimensional. This is significant since recent results show that defect prediction exhibits very low intrinsic dimensionality-- an observation that explains why a fast method like DE may work as well as a seemingly more thorough grid search. This suggests, as a future research direction, that it might be possible to peek at data sets before doing any optimization in order to match the optimization algorithm to the problem at hand.",
      "fieldsOfStudy": [
        "Computer Science",
        "Mathematics"
      ],
      "topics": [
        "grid search",
        "underlying search space",
        "extensive use",
        "data sets",
        "performance score",
        "runtime cost",
        "better parameters",
        "search space",
        "quick partial search",
        "stochastic search",
        "learners",
        "differential evolution",
        "DE",
        "hand",
        "methods",
        "stochastic jumps",
        "conclusion validity",
        "randomized searchers"
      ]
    },
    "org": {
      "title": "Combining Explicit and Symbolic Approaches for Better On-the-Fly LTL Model Checking",
      "url": "https://www.semanticscholar.org/paper/2677924e3fe7d30255fc89379e17f1d93c14ba7c",
      "abstract": "We present two new hybrid techniques that replace the synchronized product used in the automata-theoretic approach for LTL model checking. The proposed products are explicit graphs of aggregates (symbolic sets of states) that can be interpreted as B\\\"uchi automata. These hybrid approaches allow on the one hand to use classical emptiness-check algorithms and build the graph on-the-fly, and on the other hand, to have a compact encoding of the state space thanks to the symbolic representation of the aggregates. The Symbolic Observation Product assumes a globally stuttering property (e.g., LTL \\ X) to aggregate states. The Self-Loop Aggregation Product} does not require the property to be globally stuttering (i.e., it can tackle full LTL), but dynamically detects and exploits a form of stuttering where possible. Our experiments show that these two variants, while incomparable with each other, can outperform other existing approaches.",
      "fieldsOfStudy": [
        "Mathematics",
        "Computer Science"
      ],
      "topics": [
        "LTL model checking",
        "B\\\"uchi automata",
        "states",
        "existing approaches",
        "symbolic sets",
        "LTL",
        "LTL",
        "aggregates",
        "explicit graphs",
        "state space",
        "automata-theoretic approach",
        "symbolic representation",
        "classical emptiness-check algorithms",
        "These hybrid approaches",
        "hand",
        "stuttering"
      ]
    }
  },
  {
    "sim": 0.47096626069607606,
    "gen": {
      "title": "The importance of being earnest in crowdsourcing systems",
      "url": "https://www.semanticscholar.org/paper/7638753f9a3d216494600a57007e75e142a8f44a",
      "abstract": "This paper presents the first systematic investigation of the potential performance gains for crowdsourcing systems, deriving from available information at the requester about individual worker earnestness (reputation). In particular, we first formalize the optimal task assignment problem when workers' reputation estimates are available, as the maximization of a monotone (submodular) function subject to Matroid constraints. Then, being the optimal problem NP-hard, we propose a simple but efficient greedy heuristic task allocation algorithm. We also propose a simple \u201cmaximum a-posteriori\u201c decision rule. Finally, we test and compare different solutions, showing that system performance can greatly benefit from information about workers' reputation. Our main findings are that: i) even largely inaccurate estimates of workers' reputation can be effectively exploited in the task assignment to greatly improve system performance; ii) the performance of the maximum a-posteriori decision rule quickly degrades as worker reputation estimates become inaccurate; iii) when workers' reputation estimates are significantly inaccurate, the best performance can be obtained by combining our proposed task assignment algorithm with the LRA decision rule introduced in the literature.",
      "fieldsOfStudy": [
        "Computer Science"
      ],
      "topics": [
        "reputation",
        "individual worker earnestness",
        "system performance",
        "workers reputation estimates",
        "Matroid constraints",
        "available information",
        "crowdsourcing systems",
        "workers reputation",
        "proposed task assignment algorithm",
        "information",
        "Matroid",
        "optimal task assignment problem",
        "LRA decision rule",
        "potential performance gains",
        "simple but efficient greedy heuristic task allocation algorithm"
      ]
    },
    "org": {
      "title": "Achievable secrecy rates over MIMOME Gaussian channels with GMM signals in low-noise regime",
      "url": "https://www.semanticscholar.org/paper/a3696dfd0f51b57ec26a735e34474f1de328bbee",
      "abstract": "We consider a wiretap multiple-input multiple-output multiple-eavesdropper (MIMOME) channel, where agent Alice aims at transmitting a secret message to agent Bob, while leaking no information on it to an eavesdropper agent Eve. We assume that Alice has more antennas than both Bob and Eve, and that she has only statistical knowledge of the channel towards Eve. We focus on the low-noise regime, and assess the secrecy rates that are achievable when the secret message determines the distribution of a multivariate Gaussian mixture model (GMM) from which a realization is generated and transmitted over the channel. In particular, we show that if Eve has fewer antennas than Bob, secret transmission is always possible at low-noise. Moreover, we show that in the low-noise limit the secrecy capacity of our scheme coincides with its unconstrained capacity, by providing a class of covariance matrices that allow to attain such limit without the need of wiretap coding.",
      "fieldsOfStudy": [
        "Computer Science",
        "Mathematics"
      ],
      "topics": [
        "Eve",
        "agent Alice",
        "secret transmission",
        "Bob",
        "fewer antennas",
        "limit",
        "antennas",
        "wiretap coding",
        "covariance matrices",
        "Alice",
        "secret message",
        "multivariate Gaussian mixture model",
        "GMM",
        "Gaussian",
        "agent Bob",
        "statistical knowledge",
        "eavesdropper agent"
      ]
    }
  },
  null,
  {
    "sim": 0.3536026448527314,
    "gen": {
      "title": "Limit of detection of troponin discharge strategy versus usual care: randomised controlled trial",
      "url": "https://www.semanticscholar.org/paper/247c62559d1b69ad251e88fe2dc49fcc6078edeb",
      "abstract": "Introduction The clinical effectiveness of a \u2018rule-out\u2019 acute coronary syndrome (ACS) strategy for emergency department patients with chest pain, incorporating a single undetectable high-sensitivity cardiac troponin (hs-cTn) taken at presentation, together with a non-ischaemic ECG, remains unknown. Methods A randomised controlled trial, across eight hospitals in the UK, aimed to establish the clinical effectiveness of an undetectable hs-cTn and ECG (limit of detection and ECG discharge (LoDED)) discharge strategy. Eligible adult patients presented with chest pain; the treating clinician intended to perform investigations to rule out an ACS; the initial ECG was non-ischaemic; and peak symptoms occurred <6\u2009hours previously. Participants were randomised 1:1 to either the LoDED strategy or the usual rule-out strategy. The primary outcome was discharge from the hospital within 4\u2009hours of arrival, without a major adverse cardiac event (MACE) within 30 days. Results Between June 2018 and March 2019, 632 patients were randomised; 3 were later withdrawn. Of 629 patients (age 53.8 (SD 16.1) years, 41% women), 7% had a MACE within 30 days. For the LoDED strategy, 141 of 309 (46%) patients were discharged within 4\u2009hours, without MACE within 30 days, and for usual care, 114 of 311 (37%); pooled adjusted OR 1.58 (95% CI 0.84 to 2.98). No patient with an initial undetectable hs-cTn had a MACE within 30 days. Conclusion The LoDED strategy facilitates safe early discharge in >40% of patients with chest pain. Clinical effectiveness is variable when compared with existing rule-out strategies and influenced by wider system factors. Trial registration number ISRCTN86184521.",
      "fieldsOfStudy": [
        "Medicine"
      ],
      "topics": [
        "usual care",
        "discharge strategy",
        "ECG discharge",
        "MACE",
        "chest pain",
        "emergency department patients",
        "wider system factors",
        "patients",
        "safe early discharge",
        "ECG",
        "Eligible adult patients",
        "41% women",
        ">40%",
        "non-ischaemic ECG",
        "CI",
        "pooled adjusted OR",
        "hours",
        "discharge",
        "95% CI",
        "peak symptoms"
      ]
    },
    "org": {
      "title": "Finite-Horizon Throughput Region for Wireless Multi-User Interference Channels",
      "url": "https://www.semanticscholar.org/paper/12cb84e520e7c06163f0de252814f349907f26b9",
      "abstract": "This paper studies a wireless network consisting of multiple transmitter-receiver pairs where interference is treated as noise. Previously, the throughput region of such networks was characterized for either one time slot or an infinite time horizon. We aim to fill the gap by investigating the throughput region for transmissions over a finite time horizon. Unlike the infinite-horizon throughput region, which is simply the convex hull of the throughput region of one time slot, the finite-horizon throughput region is generally non-convex. Instead of directly characterizing all achievable rate-tuples in the finite-horizon throughput region, we propose a metric termed the rate margin, which not only determines whether any given rate-tuple is within the throughput region (i.e., achievable or unachievable), but also tells the amount of scaling that can be done to the given achievable (unachievable) rate-tuple such that the resulting rate-tuple is still within (brought back into) the throughput region. Furthermore, we derive an efficient algorithm to find the rate-achieving policy for any given rate-tuple in the finite-horizon throughput region.",
      "fieldsOfStudy": [
        "Computer Science",
        "Mathematics"
      ],
      "topics": [
        "throughput region",
        "finite-horizon throughput region",
        "infinite-horizon throughput region",
        "finite time horizon",
        "networks",
        "infinite time horizon",
        "noise",
        "given rate-tuple",
        "rate margin",
        "achievable rate-tuples",
        "time slot",
        "resulting rate-tuple",
        "convex hull",
        "given achievable (unachievable) rate-tuple",
        "scaling"
      ]
    }
  },
  {
    "sim": 0.5629770432435015,
    "gen": {
      "title": "Semantic Drift Compensation for Class-Incremental Learning",
      "url": "https://www.semanticscholar.org/paper/8bb62a88bc665018cec0acfdef0c708d9de30f0f",
      "abstract": "Class-incremental learning of deep networks sequentially increases the number of classes to be classified. During training, the network has only access to data of one task at a time, where each task contains several classes. In this setting, networks suffer from catastrophic forgetting which refers to the drastic drop in performance on previous tasks. The vast majority of methods have studied this scenario for classification networks, where for each new task the classification layer of the network must be augmented with additional weights to make room for the newly added classes. Embedding networks have the advantage that new classes can be naturally included into the network without adding new weights. Therefore, we study incremental learning for embedding networks. In addition, we propose a new method to estimate the drift, called semantic drift, of features and compensate for it without the need of any exemplars. We approximate the drift of previous tasks based on the drift that is experienced by current task data. We perform experiments on fine-grained datasets, CIFAR100 and ImageNet-Subset. We demonstrate that embedding networks suffer significantly less from catastrophic forgetting. We outperform existing methods which do not require exemplars and obtain competitive results compared to methods which store exemplars. Furthermore, we show that our proposed SDC when combined with existing methods to prevent forgetting consistently improves results.",
      "fieldsOfStudy": [
        "Computer Science"
      ],
      "topics": [
        "new classes",
        "classes",
        "competitive results",
        "previous tasks",
        "current task data",
        "classes",
        "exemplars",
        "classification networks",
        "deep networks",
        "embedding networks",
        "results",
        "networks",
        "new weights",
        "existing methods",
        "Embedding networks"
      ]
    },
    "org": {
      "title": "Combining Evaluation Metrics via the Unanimous Improvement Ratio and its Application to Clustering Tasks",
      "url": "https://www.semanticscholar.org/paper/69abd7b926466256923064e9a1d41d446199de12",
      "abstract": "Many Artificial Intelligence tasks cannot be evaluated with a single quality criterion and some sort of weighted combination is needed to provide system rankings. A problem of weighted combination measures is that slight changes in the relative weights may produce substantial changes in the system rankings. This paper introduces the Unanimous Improvement Ratio (UIR), a measure that complements standard metric combination criteria (such as van Rijsbergen's F-measure) and indicates how robust the measured differences are to changes in the relative weights of the individual metrics. UIR is meant to elucidate whether a perceived difference between two systems is an artifact of how individual metrics are weighted. \n \nBesides discussing the theoretical foundations of UIR, this paper presents empirical results that confirm the validity and usefulness of the metric for the Text Clustering problem, where there is a tradeoff between precision and recall based metrics and results are particularly sensitive to the weighting scheme used to combine them. Remarkably, our experiments show that UIR can be used as a predictor of how well differences between systems measured on a given test bed will also hold in a different test bed.",
      "fieldsOfStudy": [
        "Computer Science",
        "Mathematics"
      ],
      "topics": [
        "individual metrics",
        "recall based metrics",
        "different test bed",
        "system rankings",
        "substantial changes",
        "given test bed",
        "standard metric combination criteria",
        "systems",
        "slight changes",
        "weighted combination",
        "changes",
        "Text Clustering",
        "differences",
        "empirical results",
        "precision and recall based metrics",
        "results"
      ]
    }
  },
  null,
  {
    "sim": 0.39100328818498875,
    "gen": {
      "title": "Modeling and predicting the growth and death of membership-based websites",
      "url": "https://www.semanticscholar.org/paper/5122fa96ccde8f2f9031a92e3116be0ed74c2975",
      "abstract": "Driven by outstanding success stories of Internet startups such as Facebook and The Huffington Post, recent studies have thoroughly described their growth. These highly visible online success stories, however, overshadow an untold number of similar ventures that fail. The study of website popularity is ultimately incomplete without general mechanisms that can describe both successes and failures. In this work we present six years of the daily number of users (DAU) of twenty-two membership-based websites - encompassing online social networks, grassroots movements, online forums, and membership-only Internet stores - well balanced between successes and failures. We then propose a combination of reaction-diffusion-decay processes whose resulting equations seem not only to describe well the observed DAU time series but also provide means to roughly predict their evolution. This model allows an approximate automatic DAU-based classification of websites into self-sustainable v.s. unsustainable and whether the startup growth is mostly driven by marketing & media campaigns or word-of-mouth adoptions.",
      "fieldsOfStudy": [
        "Computer Science",
        "Physics",
        "Business"
      ],
      "topics": [
        "outstanding success stories",
        "Internet startups",
        "successes",
        "online social networks",
        "online forums",
        "failures",
        "DAU",
        "recent studies",
        "website popularity",
        "similar ventures",
        "websites",
        "grassroots movements",
        "general mechanisms",
        "mouth",
        "observed DAU time series",
        "means"
      ]
    },
    "org": {
      "title": "A Brain Emotional Learning-based Prediction Model for the prediction of geomagnetic storms",
      "url": "https://www.semanticscholar.org/paper/669267b670d7ada3556edc8fc8ace0e6602e1de4",
      "abstract": "This paper introduces a new type of brain emotional learning inspired models (BELIMs). The suggested model is utilized as a suitable model for predicting geomagnetic storms. The model is known as BELPM which is an acronym for Brain Emotional Learning-based Prediction Model. The structure of the suggested model consists of four main parts and mimics the corresponding regions of the neural structure underlying fear conditioning. The functions of these parts are implemented by assigning adaptive networks to the different parts. The learning algorithm of BELPM is based on the steepest descent (SD) and the least square estimator (LSE). In this paper, BELPM is employed to predict geomagnetic storms using the Disturbance Storm Time (Dst) index. To evaluate the performance of BELPM, the obtained results have been compared with the results of the adaptive neuro-fuzzy inference system (ANFIS).",
      "fieldsOfStudy": [
        "Computer Science",
        "Engineering"
      ],
      "topics": [
        "fear conditioning",
        "inspired models",
        "adaptive networks",
        "Prediction Model",
        "geomagnetic storms",
        "ANFIS",
        "LSE",
        "Brain Emotional Learning",
        "Dst",
        "Brain Emotional Learning-based Prediction Model",
        "BELIMs",
        "mimics",
        "Disturbance Storm Time",
        "BELPM",
        "brain emotional learning inspired models",
        "adaptive neuro-fuzzy inference system",
        "different parts",
        "SD",
        "square estimator",
        "main parts",
        "neural structure"
      ]
    }
  },
  null,
  {
    "sim": 0.6070141472927104,
    "gen": {
      "title": "A Noncooperative Game Approach to Autonomous Racing",
      "url": "https://www.semanticscholar.org/paper/cb97e2865041e13aeeefac816984374c952cc1c2",
      "abstract": "We consider autonomous racing of two cars and present an approach to formulate racing decisions as a noncooperative nonzero-sum game. We design three different games where the players aim to fulfill static track constraints as well as avoid collision with each other; the latter constraint depends on the combined actions of the two players. The difference between the games is the collision constraints and the payoff. In the first game, collision avoidance is only considered by the follower, and each player maximizes their own progress toward the finish line. We show that, thanks to the sequential structure of this game, equilibria can be computed through an efficient sequential maximization approach. Furthermore, we show that these actions, if feasible, are also a Stackelberg and Nash equilibrium in pure strategies of our second game where both players consider the collision constraints. The payoff of our third game is designed to promote blocking, by additionally rewarding the cars for staying ahead at the end of the horizon. We show that this changes the Stackelberg equilibrium, but has a minor influence on the Nash equilibria. For online implementation, we propose to play the games in a moving horizon fashion and discuss two methods for guaranteeing feasibility of the resulting coupled repeated games. Finally, we study the performance of the proposed approaches in simulation for a setup that replicates the miniature race car tested at the Automatic Control Laboratory, ETH Z\u00fcrich, Switzerland. The simulation study shows that the presented games can successfully model different racing behaviors and generate interesting racing situations.",
      "fieldsOfStudy": [
        "Computer Science",
        "Mathematics"
      ],
      "topics": [
        "static track constraints",
        "interesting racing situations",
        "different racing behaviors",
        "racing decisions",
        "collision avoidance",
        "resulting coupled repeated games",
        "collision",
        "autonomous racing",
        "second game",
        "presented games",
        "different games",
        "ETH Z\u00fcrich",
        "Switzerland",
        "game",
        "pure strategies",
        "equilibria"
      ]
    },
    "org": {
      "title": "Multi-Agent Low-Dimensional Linear Bandits",
      "url": "https://www.semanticscholar.org/paper/3147102ebe7c2310a56eecd5dbd74f107df22065",
      "abstract": "We study a multi-agent stochastic linear bandit with side information, parameterized by an unknown vector $\\theta^* \\in \\mathbb{R}^d$. The side information consists of a finite collection of low-dimensional subspaces, one of which contains $\\theta^*$. In our setting, agents can collaborate to reduce regret by sending recommendations across a communication graph connecting them. We present a novel decentralized algorithm, where agents communicate subspace indices with each other, and each agent plays a projected variant of LinUCB on the corresponding (low-dimensional) subspace. Through a combination of collaborative best subspace identification, and per-agent learning of an unknown vector in the corresponding low-dimensional subspace, we show that the per-agent regret is much smaller than the case when agents do not communicate. By collaborating to identify the subspace containing $\\theta^*$, we show that each agent effectively solves an easier instance of the linear bandit (compared to the case of no collaboration), thus leading to the reduced per-agent regret. We finally complement these results through simulations.",
      "fieldsOfStudy": [
        "Computer Science",
        "Mathematics"
      ],
      "topics": [
        "agents",
        "subspace indices",
        "collaborative best subspace identification",
        "regret",
        "low-dimensional subspaces",
        "corresponding low-dimensional subspace",
        "information",
        "agent",
        "LinUCB",
        "recommendations",
        "multi-agent stochastic linear bandit",
        "subspace",
        "\\theta^",
        "unknown vector",
        "\\theta^*$."
      ]
    }
  },
  null,
  {
    "sim": 0.3713834134064241,
    "gen": {
      "title": "Confidence Scores Make Instance-dependent Label-noise Learning Possible",
      "url": "https://www.semanticscholar.org/paper/1e32136f4c024ee001623b14d87c589699ec35ee",
      "abstract": "Learning with noisy labels has drawn a lot of attention. In this area, most of recent works only consider class-conditional noise, where the label noise is independent of its input features. This noise model may not be faithful to many real-world applications. Instead, few pioneer works have studied instance-dependent noise, but these methods are limited to strong assumptions on noise models. To alleviate this issue, we introduce confidence-scored instance-dependent noise (CSIDN), where each instance-label pair is associated with a confidence score. The confidence scores are sufficient to estimate the noise functions of each instance with minimal assumptions. Moreover, such scores can be easily and cheaply derived during the construction of the dataset through crowdsourcing or automatic annotation. To handle CSIDN, we design a benchmark algorithm termed instance-level forward correction. Empirical results on synthetic and real-world datasets demonstrate the utility of our proposed method.",
      "fieldsOfStudy": [
        "Computer Science",
        "Mathematics"
      ],
      "topics": [
        "noise models",
        "strong assumptions",
        "minimal assumptions",
        "instance-dependent noise",
        "This noise model",
        "real-world applications",
        "class-conditional noise",
        "noisy labels",
        "label noise",
        "confidence-scored instance-dependent noise",
        "instance-level forward correction",
        "noise functions",
        "attention",
        "scores",
        "proposed method",
        "automatic annotation"
      ]
    },
    "org": {
      "title": "Approximate Edge Analytics for the IoT Ecosystem",
      "url": "https://www.semanticscholar.org/paper/31960fc841e833825f280e28093ab65ccc493cfd",
      "abstract": "IoT-enabled devices continue to generate a massive amount of data. Transforming this continuously arriving raw data into timely insights is critical for many modern online services. For such settings, the traditional form of data analytics over the entire dataset would be prohibitively limiting and expensive for supporting real-time stream analytics. In this work, we make a case for approximate computing for data analytics in IoT settings. Approximate computing aims for efficient execution of workflows where an approximate output is sufficient instead of the exact output. The idea behind approximate computing is to compute over a representative sample instead of the entire input dataset. Thus, approximate computing - based on the chosen sample size - can make a systematic trade-off between the output accuracy and computation efficiency. This motivated the design of APPROXIOT - a data analytics system for approximate computing in IoT. To realize this idea, we designed an online hierarchical stratified reservoir sampling algorithm that uses edge computing resources to produce approximate output with rigorous error bounds. To showcase the effectiveness of our algorithm, we implemented APPROXIOT based on Apache Kafka and evaluated its effectiveness using a set of microbenchmarks and real-world case studies. Our results show that APPROXIOT achieves a speedup 1.3X-9.9X with varying sampling fraction of 80% to 10% compared to simple random sampling.",
      "fieldsOfStudy": [
        "Computer Science"
      ],
      "topics": [
        "data analytics",
        "approximate computing",
        "modern online services",
        "rigorous error bounds",
        "computation efficiency",
        "simple random sampling",
        "IoT settings",
        "real-time stream analytics",
        "varying sampling fraction",
        "edge computing resources",
        "raw data",
        "data",
        "settings",
        "online hierarchical stratified reservoir sampling algorithm",
        "data analytics system"
      ]
    }
  },
  null,
  {
    "sim": 0.7035490943080857,
    "gen": {
      "title": "Performance Analysis of AODV, DSDV and DSR in MANETs",
      "url": "https://www.semanticscholar.org/paper/dd52804f7885906ae3c626eeb5739b7ed8fb7046",
      "abstract": "Mobile Ad hoc Networks (MANETs) are considered as a new paradigm of infrastructure-less mobile wireless communication systems. MANETs are being widely studied and it is the technology that is attracting a large variety of applications. Routing in MANETs is considered a challenging task due to the unpredictable changes in the network topology, resulting from the random and frequent movement of the nodes and due to the absence of any centralized control [1][2]. In this paper, we evaluate the performance of reactive routing protocols, Ad hoc On demand Distance Vector (AODV) and Dynamic Source Routing (DSR) and proactive routing protocol Destination Sequenced Distance Vector (DSDV). The major goal of this study is to analyze the performance of well known MANETs routing protocol in high mobility case under low, medium and high density scenario. Unlike military applications, most of the other applications of MANETs require moderate to high mobility. Hence it becomes important to study the impact of high mobility on the performance of these routing protocols. The performance is analyzed with respect to Average End-to-End Delay, Normalized Routing Load (NRL), Packet Delivery Fraction (PDF) and Throughput. Simulation results verify that AODV gives better performance as compared to DSR and DSDV.",
      "fieldsOfStudy": [
        "Computer Science"
      ],
      "topics": [
        "high mobility",
        "DSDV",
        "Dynamic Source Routing",
        "Destination Sequenced Distance Vector",
        "Packet Delivery Fraction",
        "reactive routing protocols",
        "Distance Vector",
        "Normalized Routing Load",
        "proactive routing protocol",
        "better performance",
        "Throughput",
        "applications",
        "military applications",
        "Routing",
        "known MANETs routing protocol"
      ]
    },
    "org": {
      "title": "Optimal resource allocation in distributed broadband wireless communication systems",
      "url": "https://www.semanticscholar.org/paper/be5e50df1583cdf455e2949b30543e1385864d86",
      "abstract": "This paper is concerned with optimization of distributed broadband wireless communication (BWC) systems. BWC systems contain a distributed antenna system (DAS) connected to a base station with optical fiber. BWC systems have been proposed as a solution to the power constraint problem in traditional cellular networks. So far, the research on BWC systems have advanced on two separate tracks, design of the system to meet the quality of service requirements (QoS) and optimization of location of the DAS. In this paper, we consider a combined optimization of BWC systems. We consider uplink communications with multiple levels of priority traffic having any renewal arrival and departure processes. We develop an analysis that determines packet delay violation probability for each priority level as a function of the outage probability of the distributed antenna system through the application of results from renewal theory. Then, we determine the optimal locations of the antennas that minimize the antenna outage probability. We also study the trade off between the packet delay violation probability and packet loss probability.",
      "fieldsOfStudy": [
        "Computer Science"
      ],
      "topics": [
        "packet loss probability",
        "BWC systems",
        "optical fiber",
        "departure processes",
        "traditional cellular networks",
        "renewal theory",
        "distributed antenna system",
        "antenna outage probability",
        "priority traffic",
        "service requirements",
        "packet delay violation probability",
        "multiple levels",
        "DAS",
        "optimization",
        "outage probability"
      ]
    }
  },
  {
    "sim": 0.42441199215856296,
    "gen": {
      "title": "Stability of Stochastic Differential Equations With Respect to Semimartingales",
      "url": "https://www.semanticscholar.org/paper/7c1acc2422a2e735c9ec04f71c5a267eded13ed9",
      "abstract": "Aims to systemize the results available in literature to be found on stability of stochastic differential equations. Numerous problems in engineering, biology and economics lead to the study of stochastic differential equations with respect to semimartingales.",
      "fieldsOfStudy": [
        "Mathematics"
      ],
      "topics": [
        "stochastic differential equations",
        "semimartingales",
        "respect",
        "stability",
        "economics",
        "literature",
        "biology",
        "Numerous problems",
        "engineering",
        "study",
        "results",
        "Aims"
      ]
    },
    "org": {
      "title": "Mutual Information, Relative Entropy, and Estimation in the Poisson Channel",
      "url": "https://www.semanticscholar.org/paper/cd80259a31af3a725e96ec4f0e2107df371ecee0",
      "abstract": "Let be a nonnegative random variable and let the conditional distribution of a random variable , given , be Poisson , for a parameter . We identify a natural loss function such that: (1) the derivative of the mutual information between and with respect to is equal to the minimum mean loss in estimating based on , regardless of the distribution of ; (2) when is estimated based on by a mismatched estimator that would have minimized the expected loss had , the integral over all values of of the excess mean loss is equal to the relative entropy between and . For a continuous time setting where is a nonnegative stochastic process and the conditional law of , given , is that of a non-homogeneous Poisson process with intensity function , under the same loss function: (1) the minimum mean loss in causal filtering when is equal to the expected value of the minimum mean loss in noncausal filtering (smoothing) achieved with a channel whose parameter is uniformly distributed between 0 and . Bridging the two quantities is the mutual information between and ; (2) this relationship between the mean losses in causal and noncausal filtering holds also in the case where the filters employed are mismatched, i.e., optimized assuming a law on which is not the true one. Bridging the two quantities in this case is the sum of the mutual information and the relative entropy between the true and the mismatched distribution of . Thus, relative entropy quantifies the excess estimation loss due to mismatch in this setting. These results are parallel to those recently found for the Gaussian channel: the I-MMSE relationship of Guo , the relative entropy and mismatched estimation relationship of Verdu\u0301, and the relationship between causal and noncasual mismatched estimation of Weissman.",
      "fieldsOfStudy": [
        "Physics",
        "Computer Science",
        "Mathematics"
      ],
      "topics": [
        "estimation relationship",
        "relative entropy",
        "noncausal filtering",
        "intensity function",
        "minimum mean loss",
        "excess mean loss",
        "causal filtering",
        "excess estimation loss",
        "loss function",
        "mean losses",
        "natural loss function",
        "expected loss",
        "Poisson",
        "non-homogeneous Poisson process",
        "causal",
        "mismatched estimation relationship",
        "causal and noncasual mismatched estimation"
      ]
    }
  },
  {
    "sim": 0.4597868041384049,
    "gen": {
      "title": "Online Algorithms for Multi-shop Ski Rental with Machine Learned Advice",
      "url": "https://www.semanticscholar.org/paper/992c78af5c1648688cbc5afa0bf36fe7fd425fdd",
      "abstract": "We study the problem of augmenting online algorithms with machine learned (ML) advice. In particular, we consider the \\emph{multi-shop ski rental} (MSSR) problem, which is a generalization of the classical ski rental problem. In MSSR, each shop has different prices for buying and renting a pair of skis, and a skier has to make decisions on when and where to buy. We obtain both deterministic and randomized online algorithms with provably improved performance when either a single or multiple ML predictions are used to make decisions. These online algorithms have no knowledge about the quality or the prediction error type of the ML prediction. The performance of these online algorithms are robust to the poor performance of the predictors, but improve with better predictions. Extensive experiments using both synthetic and real world data traces verify our theoretical observations and show better performance against algorithms that purely rely on online decision making.",
      "fieldsOfStudy": [
        "Computer Science"
      ],
      "topics": [
        "online decision making",
        "better predictions",
        "decisions",
        "skis",
        "better performance",
        "ML",
        "algorithms",
        "classical ski rental problem",
        "\\emph{multi-shop ski rental",
        "ML prediction",
        "prediction error type",
        "different prices",
        "MSSR",
        "These online algorithms",
        "online algorithms",
        "single or multiple ML predictions"
      ]
    },
    "org": {
      "title": "Optimal Bounds between $f$-Divergences and Integral Probability Metrics",
      "url": "https://www.semanticscholar.org/paper/b62a02eecddbf94a1a6a837852c4dde053d02812",
      "abstract": "The families of $f$-divergences (e.g. the Kullback-Leibler divergence) and Integral Probability Metrics (e.g. total variation distance or maximum mean discrepancies) are widely used to quantify the similarity between probability distributions. In this work, we systematically study the relationship between these two families from the perspective of convex duality. Starting from a tight variational representation of the $f$-divergence, we derive a generalization of the moment-generating function, which we show exactly characterizes the best lower bound of the $f$-divergence as a function of a given IPM. Using this characterization, we obtain new bounds while also recovering in a unified manner well-known results, such as Hoeffding's lemma, Pinsker's inequality and its extension to subgaussian functions, and the Hammersley-Chapman-Robbins bound. The variational representation also allows us to prove new results on topological properties of the divergence which may be of independent interest.",
      "fieldsOfStudy": [
        "Computer Science",
        "Mathematics"
      ],
      "topics": [
        "subgaussian functions",
        "probability distributions",
        "maximum mean discrepancies",
        "independent interest",
        "new results",
        "Integral Probability Metrics",
        "convex duality",
        "new bounds",
        "topological properties",
        "subgaussian",
        "Pinsker",
        "given IPM",
        "Hoeffding",
        "e.g. total variation distance",
        "function",
        "best lower bound",
        "Hammersley-Chapman-Robbins bound",
        "unified manner"
      ]
    }
  },
  {
    "sim": 0.6109525178141167,
    "gen": {
      "title": "Point sets with many non-crossing perfect matchings",
      "url": "https://www.semanticscholar.org/paper/e2c4f463354d8cb23d8445831186f2a06897791c",
      "abstract": null,
      "fieldsOfStudy": [
        "Computer Science",
        "Mathematics"
      ]
    },
    "org": {
      "title": "Advancing Through Terrains",
      "url": "https://www.semanticscholar.org/paper/a85aa92fb85c66302d40076193d66937cf25df08",
      "abstract": "We study terrain visibility graphs, a well-known graph class closely related to polygon visibility graphs in computational geometry, for which a precise graph-theoretical characterization is still unknown. Over the last decade, terrain visibility graphs attracted attention in the context of time series analysis with various practical applications in areas such as physics, geography and medical sciences. We make progress in understanding terrain visibility graphs by providing several graph-theoretic results. For example, we show that they cannot contain antiholes of size larger than five. Moreover, we obtain two algorithmic results. We devise a fast output-sensitive shortest path algorithm on terrain-like graphs and a polynomial-time algorithm for \\textsc{Dominating Set} on special terrain visibility graphs (called funnel visibility graphs).",
      "fieldsOfStudy": [
        "Computer Science",
        "Physics"
      ],
      "topics": [
        "special terrain visibility graphs",
        "polygon visibility graphs",
        "funnel visibility graphs",
        "medical sciences",
        "time series analysis",
        "graph-theoretic results",
        "practical applications",
        "terrain-like graphs",
        "computational geometry",
        "precise graph-theoretical characterization",
        "geography",
        "physics",
        "areas",
        "polygon",
        "\\textsc{Dominating Set"
      ]
    }
  },
  null,
  null,
  {
    "sim": 0.41118865264967697,
    "gen": {
      "title": "Outlier Detection for Text Data : An Extended Version",
      "url": "https://www.semanticscholar.org/paper/1ccdc829da85743dc26f279dd5cac58a20c1d9ef",
      "abstract": "The problem of outlier detection is extremely challenging in many domains such as text, in which the attribute values are typically non-negative, and most values are zero. In such cases, it often becomes difficult to separate the outliers from the natural variations in the patterns in the underlying data. In this paper, we present a matrix factorization method, which is naturally able to distinguish the anomalies with the use of low rank approximations of the underlying data. Our iterative algorithm TONMF is based on block coordinate descent (BCD) framework. We define blocks over the term-document matrix such that the function becomes solvable. Given most recently updated values of other matrix blocks, we always update one block at a time to its optimal. Our approach has significant advantages over traditional methods for text outlier detection. Finally, we present experimental results illustrating the effectiveness of our method over competing methods.",
      "fieldsOfStudy": [
        "Computer Science",
        "Mathematics"
      ],
      "topics": [
        "values",
        "matrix blocks",
        "competing methods",
        "traditional methods",
        "outlier detection",
        "block coordinate descent",
        "low rank approximations",
        "blocks",
        "domains",
        "cases",
        "text",
        "underlying data",
        "matrix factorization method",
        "attribute values",
        "significant advantages"
      ]
    },
    "org": {
      "title": "SPEC2: SPECtral SParsE CNN Accelerator on FPGAs",
      "url": "https://www.semanticscholar.org/paper/ca041b5982690391d8c3073d4a5dc3db2cff8833",
      "abstract": "To accelerate inference of Convolutional Neural Networks (CNNs), various techniques have been proposed to reduce computation redundancy. Converting convolutional layers into frequency domain significantly reduces the computation complexity of the sliding window operations in space domain. On the other hand, weight pruning techniques address the redundancy in model parameters by converting dense convolutional kernels into sparse ones. To obtain high-throughput FPGA implementation, we propose spec - the first work to prune and accelerate spectral CNNs. First, we propose a systematic pruning algorithm based on Alternative Direction Method of Multipliers (ADMM). The offline pruning iteratively sets the majority of spectral weights to zero, without using any handcrafted heuristics. Then, we design an optimized pipeline architecture on FPGA that has efficient random access into the sparse kernels and exploits various dimensions of parallelism in convolutional layers. Overall, achieves high inference throughput with extremely low computation complexity and negligible accuracy degradation. We demonstrate by pruning and implementing LeNet and VGG16 on the Xilinx Virtex platform. After pruning 75% of the spectral weights, achieves 0% accuracy loss for LeNet, and <1% accuracy loss for VGG16. The resulting accelerators achieve up to 24\u00d7 higher throughput, compared with the state-of-the-art FPGA implementations for VGG16.",
      "fieldsOfStudy": [
        "Computer Science",
        "Engineering"
      ],
      "topics": [
        "space domain",
        "frequency domain",
        "computation redundancy",
        "dense convolutional kernels",
        "negligible accuracy degradation",
        "convolutional layers",
        "techniques",
        "sparse ones",
        "dimensions",
        "weight pruning techniques",
        "VGG16",
        "spectral weights",
        "model parameters",
        "Alternative Direction Method",
        "ADMM",
        "efficient random access",
        "extremely low computation complexity"
      ]
    }
  },
  null,
  {
    "sim": 0.7856777021171701,
    "gen": {
      "title": "The Lottery Ticket Hypothesis: Finding Sparse, Trainable Neural Networks",
      "url": "https://www.semanticscholar.org/paper/21937ecd9d66567184b83eca3d3e09eb4e6fbd60",
      "abstract": "Neural network pruning techniques can reduce the parameter counts of trained networks by over 90%, decreasing storage requirements and improving computational performance of inference without compromising accuracy. However, contemporary experience is that the sparse architectures produced by pruning are difficult to train from the start, which would similarly improve training performance. \nWe find that a standard pruning technique naturally uncovers subnetworks whose initializations made them capable of training effectively. Based on these results, we articulate the \"lottery ticket hypothesis:\" dense, randomly-initialized, feed-forward networks contain subnetworks (\"winning tickets\") that - when trained in isolation - reach test accuracy comparable to the original network in a similar number of iterations. The winning tickets we find have won the initialization lottery: their connections have initial weights that make training particularly effective. \nWe present an algorithm to identify winning tickets and a series of experiments that support the lottery ticket hypothesis and the importance of these fortuitous initializations. We consistently find winning tickets that are less than 10-20% of the size of several fully-connected and convolutional feed-forward architectures for MNIST and CIFAR10. Above this size, the winning tickets that we find learn faster than the original network and reach higher test accuracy.",
      "fieldsOfStudy": [
        "Computer Science",
        "Mathematics"
      ],
      "topics": [
        "higher test accuracy",
        "trained networks",
        "training performance",
        "accuracy",
        "Neural network pruning techniques",
        "computational performance",
        "training",
        "iterations",
        "isolation - reach test accuracy",
        "tickets",
        "original network",
        "inference",
        "storage requirements",
        "initial weights",
        "CIFAR10",
        "test accuracy",
        "winning tickets",
        "subnetworks"
      ]
    },
    "org": {
      "title": "Gradient Flow in Sparse Neural Networks and How Lottery Tickets Win",
      "url": "https://www.semanticscholar.org/paper/917b18b8dad23284c0a42f665f2ba1984fa360de",
      "abstract": "Sparse Neural Networks (NNs) can match the generalization of dense NNs using a fraction of the compute/storage for inference, and have the potential to enable efficient training. However, naively training unstructured sparse NNs from random initialization results in significantly worse generalization, with the notable exceptions of Lottery Tickets (LTs) and Dynamic Sparse Training (DST). In this work, we attempt to answer: (1) why training unstructured sparse networks from random initialization performs poorly and; (2) what makes LTs and DST the exceptions? We show that sparse NNs have poor gradient flow at initialization and propose a modified initialization for unstructured connectivity. Furthermore, we find that DST methods significantly improve gradient flow during training over traditional sparse training methods. Finally, we show that LTs do not improve gradient flow, rather their success lies in re-learning the pruning solution they are derived from \u2014 however, this comes at the cost of learning novel solutions.",
      "fieldsOfStudy": [
        "Computer Science"
      ],
      "topics": [
        "novel solutions",
        "traditional sparse training methods",
        "efficient training",
        "poor gradient flow",
        "training",
        "random initialization results",
        "DST methods",
        "initialization",
        "Dynamic Sparse Training",
        "unstructured sparse NNs",
        "Lottery Tickets",
        "LTs",
        "unstructured sparse networks",
        "sparse NNs",
        "DST"
      ]
    }
  },
  null,
  {
    "sim": 0.5823034839449798,
    "gen": {
      "title": "Model Reduction Framework with a New Take on Active Subspaces for Optimization Problems with Linearized Fluid\u2010Structure Interaction Constraints",
      "url": "https://www.semanticscholar.org/paper/e265d150a3c836d9609b99d9c64a466d240c8ed5",
      "abstract": "In this paper, a new take on the concept of an active subspace for reducing the dimension of the design parameter space in a multidisciplinary analysis and optimization (MDAO) problem is proposed. The new approach is intertwined with the concepts of adaptive parameter sampling, projection\u2010based model order reduction, and a database of linear, projection\u2010based reduced\u2010order models equipped with interpolation on matrix manifolds, in order to construct an efficient computational framework for MDAO. The framework is fully developed for MDAO problems with linearized fluid\u2010structure interaction constraints. It is applied to the aeroelastic tailoring, under flutter constraints, of two different flight systems: a flexible configuration of NASA's Common Research Model; and NASA's Aeroelastic Research Wing #2 (ARW\u20102). The obtained results illustrate the feasibility of the computational framework for realistic MDAO problems and highlight the benefits of the new approach for constructing an active subspace in both terms of solution optimality and wall\u2010clock time reduction.",
      "fieldsOfStudy": [
        "Computer Science",
        "Mathematics"
      ],
      "topics": [
        "projection\u2010based reduced\u2010order models",
        "MDAO problems",
        "linearized fluid\u2010structure interaction constraints",
        "wall\u2010clock time reduction",
        "realistic MDAO problems",
        "NASA",
        "flutter constraints",
        "matrix manifolds",
        "order",
        "NASAs Aeroelastic Research Wing",
        "NASAs Common Research Model",
        "MDAO",
        "adaptive parameter sampling",
        "ARW\u20102",
        "linear, projection\u2010based reduced\u2010order models"
      ]
    },
    "org": {
      "title": "A least squares radial basis function finite difference method with improved stability properties",
      "url": "https://www.semanticscholar.org/paper/9aa862401d2fee4b79dec6fc4a302f8737392004",
      "abstract": "Localized collocation methods based on radial basis functions (RBFs) for elliptic problems appear to be non-robust in the presence of Neumann boundary conditions. In this paper we overcome this issue by formulating the RBF-generated finite difference method in a discrete least-squares setting instead. This allows us to prove high-order convergence under node refinement and to numerically verify that the least-squares formulation is more accurate and robust than the collocation formulation. The implementation effort for the modified algorithm is comparable to that for the collocation method.",
      "fieldsOfStudy": [
        "Computer Science",
        "Mathematics"
      ],
      "topics": [
        "Neumann boundary conditions",
        "Localized collocation methods",
        "radial basis functions",
        "elliptic problems",
        "Neumann",
        "RBFs",
        "collocation formulation",
        "node refinement",
        "collocation method",
        "least-squares formulation",
        "RBF-generated finite difference method",
        "discrete least-squares",
        "high-order convergence",
        "presence",
        "modified algorithm",
        "discrete least-squares setting"
      ]
    }
  },
  {
    "sim": 0.4535516821400092,
    "gen": {
      "title": "A behavioural approach to obstacle avoidance for mobile manipulators based on distributed sensing",
      "url": "https://www.semanticscholar.org/paper/7b75522b5769df17af90b70344f7be703b10f653",
      "abstract": "A reactive obstacle avoidance method for mobile manipulators is presented. The objectives of the developed algorithm are twofold. The first one is to find a trajectory in the configuration space of a mobile manipulator so as to follow a given trajectory in the task space. The second objective consists in locally adjusting the trajectory in the configuration space in order to avoid collisions with potentially moving obstacles and self-collisions in unstructured and dynamic environments. The perception is exclusively based on a set of proximity sensors distributed on the robot mechanical structure and visual information are not required. Thanks to the adoption of this kind of proximity distributed perception, the approach does not require a 3D model of the robot and allows the real-time collision avoidance without the need of a sensorized environment. To achieve the features cited above, a behaviour-based technique known as Null-Space-Based (NSB) approach has been adopted with some modifications.On one hand, the concept of a total pseudo-energy based on the information from the distributed sensors has been introduced. On the other hand, a method to combine different tasks has been proposed to guarantee the smoothness of the realtime trajectory adjustments. Another significant feature of the method is the strict coordination between the base and the arm exploiting the redundant degrees of freedom, that is a relevant topic in mobile manipulation.",
      "fieldsOfStudy": [
        "Computer Science",
        "Engineering"
      ],
      "topics": [
        "collisions",
        "visual information",
        "proximity distributed perception",
        "proximity sensors",
        "different tasks",
        "unstructured and dynamic environments",
        "realtime trajectory adjustments",
        "sensorized environment",
        "obstacles",
        "given trajectory",
        "mobile manipulator",
        "robot mechanical structure",
        "order",
        "mobile manipulators",
        "real-time collision avoidance",
        "A reactive obstacle avoidance method",
        "self-collisions"
      ]
    },
    "org": {
      "title": "Unsupervised and Generic Short-Term Anticipation of Human Body Motions",
      "url": "https://www.semanticscholar.org/paper/67c39caadc97a355b1823ae828f5f5f621cf0a7a",
      "abstract": "Various neural network based methods are capable of anticipating human body motions from data for a short period of time. What these methods lack are the interpretability and explainability of the network and its results. We propose to use Dynamic Mode Decomposition with delays to represent and anticipate human body motions. Exploring the influence of the number of delays on the reconstruction and prediction of various motion classes, we show that the anticipation errors in our results are comparable to or even better for very short anticipation times (<0.4 s) than a recurrent neural network based method. We perceive our method as a first step towards the interpretability of the results by representing human body motions as linear combinations of previous states and delays. In addition, compared to the neural network based methods large training times are not needed. Actually, our methods do not even regress to any other motions than the one to be anticipated and hence it is of a generic nature.",
      "fieldsOfStudy": [
        "Computer Science",
        "Mathematics",
        "Medicine"
      ],
      "topics": [
        "human body motions",
        "motion classes",
        "large training times",
        "time",
        "delays",
        "neural network based methods",
        "previous states",
        "short anticipation times",
        "linear combinations",
        "Dynamic Mode Decomposition",
        "data",
        "linear",
        "motions",
        "prediction"
      ]
    }
  },
  {
    "sim": 0.44659025176319467,
    "gen": {
      "title": "A New Approach to Linear/Nonlinear Distributed Fusion Estimation Problem",
      "url": "https://www.semanticscholar.org/paper/0f7b1c2dfdc36a64d7e654aa65560877797fae96",
      "abstract": "In this paper, we study the distributed fusion estimation problem for linear time-varying systems and nonlinear systems with bounded noises, where the addressed noises do not provide any statistical information, and are unknown but bounded. When considering linear time-varying fusion systems with bounded noises, a new local Kalman-like estimator is designed such that the square error of the estimator is bounded as time goes to $\\infty$. A novel constructive method is proposed to find an upper bound of fusion estimation error, then a convex optimization problem on the design of an optimal weighting fusion criterion is established in terms of linear matrix inequalities, which can be solved by standard software packages. Furthermore, according to the design method of linear time-varying fusion systems, each local nonlinear estimator is derived for nonlinear systems with bounded noises by using Taylor series expansion, and a corresponding distributed fusion criterion is obtained by solving a convex optimization problem. Finally, target tracking system and localization of a mobile robot are given to show the advantages and effectiveness of the proposed methods.",
      "fieldsOfStudy": [
        "Computer Science"
      ],
      "topics": [
        "bounded noises",
        "fusion estimation error",
        "nonlinear systems",
        "linear matrix inequalities",
        "standard software packages",
        "target tracking system",
        "linear time-varying systems",
        "time",
        "distributed fusion estimation problem",
        "linear",
        "Taylor series expansion",
        "corresponding distributed fusion criterion",
        "optimal weighting fusion criterion",
        "convex optimization problem",
        "local nonlinear estimator"
      ]
    },
    "org": {
      "title": "Nonlinear Dynamics of Binocular Rivalry: A Comparative Study",
      "url": "https://www.semanticscholar.org/paper/19633357b813583da97e5a1b3aa2e57d6a1ed3ac",
      "abstract": "When our eyes are presented with the same image, the brain processes it to view it as a single coherent one. The lateral shift in the position of our eyes, causes the two images to possess certain differences, which our brain exploits for the purpose of depth perception and to gauge the size of objects at different distances, a process commonly known as stereopsis. However, when presented with two different visual stimuli, the visual awareness alternates. This phenomenon of binocular rivalry is a result of competition between the corresponding neuronal populations of the two eyes. The article presents a comparative study of various dynamical models proposed to capture this process. It goes on to study the effect of a certain parameter on the rate of perceptual alternations and proceeds to disprove the initial propositions laid down to characterise this phenomenon. It concludes with a discussion on the possible future work that can be conducted to obtain a better picture of the neuronal functioning behind this rivalry.",
      "fieldsOfStudy": [
        "Computer Science",
        "Biology"
      ],
      "topics": [
        "binocular rivalry",
        "stereopsis",
        "certain differences",
        "different distances",
        "depth perception",
        "competition",
        "corresponding neuronal populations",
        "perceptual alternations",
        "dynamical models",
        "objects",
        "neuronal functioning",
        "proceeds",
        "visual awareness alternates",
        "different visual stimuli",
        "eyes",
        "image",
        "rivalry"
      ]
    }
  },
  {
    "sim": 0.40549642276054354,
    "gen": {
      "title": "Mechanics-based solution verification for porous media models",
      "url": "https://www.semanticscholar.org/paper/68cbdc360034ccdcac1c3516b0490fb08396283b",
      "abstract": "This paper presents a new approach to verify accuracy of computational simulations. We develop mathematical theorems which can serve as robust a posteriori error estimation techniques to identify numerical pollution, check the performance of adaptive meshes, and verify numerical solutions. We demonstrate performance of this methodology on problems from flow thorough porous media. However, one can extend it to other models. We construct mathematical properties such that the solutions to Darcy and Darcy-Brinkman equations satisfy them. The mathematical properties include the total minimum mechanical power, minimum dissipation theorem, reciprocal relation, and maximum principle for the vorticity. All the developed theorems have firm mechanical bases and are independent of numerical methods. So, these can be utilized for solution verification of finite element, finite volume, finite difference, lattice Boltzmann methods and so forth. In particular, we show that, for a given set of boundary conditions, Darcy velocity has the minimum total mechanical power of all the kinematically admissible vector fields. We also show that a similar result holds for Darcy-Brinkman velocity. We then show for a conservative body force, the Darcy and Darcy-Brinkman velocities have the minimum total dissipation among their respective kinematically admissible vector fields. Using numerical examples, we show that the minimum dissipation and total mechanical power theorems can be utilized to identify pollution errors in numerical solutions. The solutions to Darcy and Darcy-Brinkman equations are shown to satisfy a reciprocal relation, which has the potential to identify errors in the numerical implementation of boundary conditions.",
      "fieldsOfStudy": [
        "Mathematics"
      ],
      "topics": [
        "numerical methods",
        "Darcy velocity",
        "numerical pollution",
        "Darcy",
        "numerical examples",
        "Boltzmann methods",
        "boundary conditions",
        "pollution errors",
        "reciprocal relation",
        "firm mechanical bases",
        "thorough porous media",
        "total minimum mechanical power",
        "minimum dissipation theorem",
        "lattice Boltzmann methods",
        "flow thorough porous media",
        "finite difference",
        "finite volume"
      ]
    },
    "org": {
      "title": "High-Resolution Angle Tracking for Mobile Wideband Millimeter-Wave Systems With Antenna Array Calibration",
      "url": "https://www.semanticscholar.org/paper/acf844c913b01b8820fd814fd8595b7f96bd947b",
      "abstract": "Millimeter-wave (mmWave) systems use directional beams to support high-rate data communications. Small misalignment between the transmit and receive beams (e.g., due to the mobility) can result in a significant drop of the received signal quality, especially in line-of-sight communication channels. In this paper, we propose and evaluate high-resolution angle tracking strategies for wideband mmWave systems with mobility. We custom design pairs of auxiliary beams as the tracking beams, and use them to capture the angle variations, toward which the steering directions of the data beams are adjusted. Different from conventional beam tracking designs, the proposed framework neither depends on the angle variation model nor requires an on-grid assumption. For practical implementation of the proposed methods, we examine the impact of the array calibration errors on the auxiliary beam pair design. Numerical results reveal that by employing the proposed methods, good angle tracking performance can be achieved under various antenna array configurations, channel models, and mobility conditions.",
      "fieldsOfStudy": [
        "Computer Science",
        "Engineering",
        "Mathematics"
      ],
      "topics": [
        "channel models",
        "mobility conditions",
        "conventional beam tracking designs",
        "auxiliary beams",
        "directional beams",
        "good angle tracking performance",
        "mobility",
        "antenna array configurations",
        "auxiliary beam pair design",
        "wideband mmWave systems",
        "high-resolution angle tracking strategies",
        "tracking beams",
        "angle variation model",
        "high-rate data communications",
        "sight"
      ]
    }
  },
  {
    "sim": 0.36218648289644473,
    "gen": {
      "title": "Efficient Structured Surrogate Loss and Regularization in Structured Prediction",
      "url": "https://www.semanticscholar.org/paper/eaa922ad594312899341cac8effc9b3c36d3cbf5",
      "abstract": "In this dissertation, we focus on several important problems in structured prediction. In structured prediction, the label has a rich intrinsic substructure, and the loss varies with respect to the predicted label and the true label pair. Structured SVM is an extension of binary SVM to adapt to such structured tasks. \nIn the first part of the dissertation, we study the surrogate losses and its efficient methods. To minimize the empirical risk, a surrogate loss which upper bounds the loss, is used as a proxy to minimize the actual loss. Since the objective function is written in terms of the surrogate loss, the choice of the surrogate loss is important, and the performance depends on it. Another issue regarding the surrogate loss is the efficiency of the argmax label inference for the surrogate loss. Efficient inference is necessary for the optimization since it is often the most time-consuming step. We present a new class of surrogate losses named bi-criteria surrogate loss, which is a generalization of the popular surrogate losses. We first investigate an efficient method for a slack rescaling formulation as a starting point utilizing decomposability of the model. Then, we extend the algorithm to the bi-criteria surrogate loss, which is very efficient and also shows performance improvements. \nIn the second part of the dissertation, another important issue of regularization is studied. Specifically, we investigate a problem of regularization in hierarchical classification when a structural imbalance exists in the label structure. We present a method to normalize the structure, as well as a new norm, namely shared Frobenius norm. It is suitable for hierarchical classification that adapts to the data in addition to the label structure.",
      "fieldsOfStudy": [
        "Computer Science",
        "Mathematics"
      ],
      "topics": [
        "Frobenius norm",
        "Frobenius",
        "surrogate losses",
        "performance improvements",
        "new norm",
        "Efficient inference",
        "actual loss",
        "structured tasks",
        "true label pair",
        "shared Frobenius norm",
        "bi-criteria surrogate loss",
        "structured prediction"
      ]
    },
    "org": {
      "title": "On the average performance of caching and coded multicasting with random demands",
      "url": "https://www.semanticscholar.org/paper/7e04ea25db43fc4513157e73d6f369145e8005d7",
      "abstract": "For a network with one sender, n receivers (users) and m possible messages (files), caching side information at the users allows to satisfy arbitrary simultaneous demands by sending a common (multicast) coded message. In the worst-case demand setting, explicit deterministic and random caching strategies and explicit linear coding schemes have been shown to be order optimal. In this work, we consider the same scenario where the user demands are random i.i.d., according to a Zipf popularity distribution. In this case, we pose the problem in terms of the minimum average number of equivalent message transmissions. We present a novel decentralized random caching placement and a coded delivery scheme which are shown to achieve order-optimal performance. As a matter of fact, this is the first order-optimal result for the caching and coded multicasting problem in the case of random demands.",
      "fieldsOfStudy": [
        "Computer Science",
        "Mathematics"
      ],
      "topics": [
        "explicit linear coding schemes",
        "caching side information",
        "random demands",
        "equivalent message transmissions",
        "arbitrary simultaneous demands",
        "possible messages",
        "random i.i.d",
        "message",
        "explicit deterministic and random caching strategies",
        "multicasting problem",
        "novel decentralized random caching placement",
        "users",
        "coded delivery scheme",
        "order-optimal performance",
        "files",
        "m possible messages",
        "information",
        "caching and coded multicasting problem",
        "user demands",
        "Zipf popularity distribution"
      ]
    }
  },
  null,
  {
    "sim": 0.5840282247283404,
    "gen": {
      "title": "Online Algorithms for Multi-shop Ski Rental with Machine Learned Advice",
      "url": "https://www.semanticscholar.org/paper/992c78af5c1648688cbc5afa0bf36fe7fd425fdd",
      "abstract": "We study the problem of augmenting online algorithms with machine learned (ML) advice. In particular, we consider the \\emph{multi-shop ski rental} (MSSR) problem, which is a generalization of the classical ski rental problem. In MSSR, each shop has different prices for buying and renting a pair of skis, and a skier has to make decisions on when and where to buy. We obtain both deterministic and randomized online algorithms with provably improved performance when either a single or multiple ML predictions are used to make decisions. These online algorithms have no knowledge about the quality or the prediction error type of the ML prediction. The performance of these online algorithms are robust to the poor performance of the predictors, but improve with better predictions. Extensive experiments using both synthetic and real world data traces verify our theoretical observations and show better performance against algorithms that purely rely on online decision making.",
      "fieldsOfStudy": [
        "Computer Science"
      ],
      "topics": [
        "online decision making",
        "better predictions",
        "decisions",
        "skis",
        "better performance",
        "ML",
        "algorithms",
        "classical ski rental problem",
        "\\emph{multi-shop ski rental",
        "ML prediction",
        "prediction error type",
        "different prices",
        "MSSR",
        "These online algorithms",
        "online algorithms",
        "single or multiple ML predictions"
      ]
    },
    "org": {
      "title": "Dispersion on Trees",
      "url": "https://www.semanticscholar.org/paper/20bef259a11f4a67f134a1ced201034e4fa75235",
      "abstract": "In the $k$-dispersion problem, we need to select $k$ nodes of a given graph so as to maximize the minimum distance between any two chosen nodes. This can be seen as a generalization of the independent set problem, where the goal is to select nodes so that the minimum distance is larger than 1. We design an optimal $O(n)$ time algorithm for the dispersion problem on trees consisting of $n$ nodes, thus improving the previous $O(n\\log n)$ time solution from 1997. \nWe also consider the weighted case, where the goal is to choose a set of nodes of total weight at least $W$. We present an $O(n\\log^2n)$ algorithm improving the previous $O(n\\log^4 n)$ solution. Our solution builds on the search version (where we know the minimum distance $\\lambda$ between the chosen nodes) for which we present tight $\\Theta(n\\log n)$ upper and lower bounds.",
      "fieldsOfStudy": [
        "Computer Science",
        "Mathematics"
      ],
      "topics": [
        "n)$",
        "chosen nodes",
        "total weight",
        "minimum distance",
        "$k$ nodes",
        "previous $O(n\\log^4 n)$ solution",
        "\\Theta(n\\log",
        "O(n\\log^2n)$",
        "chosen nodes",
        "trees",
        "independent set problem",
        "search version",
        "W$.",
        "dispersion problem",
        "$n$ nodes",
        "given graph",
        "Our solution"
      ]
    }
  },
  {
    "sim": 0.42580596648371427,
    "gen": {
      "title": "A Comprehensive View of Fitness Landscapes with Neutrality and Fitness Clouds",
      "url": "https://www.semanticscholar.org/paper/f20247ff81a8d9f996ab33456f79b5ae944d9bc0",
      "abstract": null,
      "fieldsOfStudy": [
        "Computer Science"
      ]
    },
    "org": {
      "title": "How Convolutional Neural Network Architecture Biases Learned Opponency and Colour Tuning",
      "url": "https://www.semanticscholar.org/paper/1d7dab70e7ff753a188cd0b7f65dcd9c7cc38c8c",
      "abstract": "Recent work suggests that changing Convolutional Neural Network (CNN) architecture by introducing a bottleneck in the second layer can yield changes in learned function. To understand this relationship fully requires a way of quantitatively comparing trained networks. The fields of electrophysiology and psychophysics have developed a wealth of methods for characterising visual systems which permit such comparisons. Inspired by these methods, we propose an approach to obtaining spatial and colour tuning curves for convolutional neurons, which can be used to classify cells in terms of their spatial and colour opponency. We perform these classifications for a range of CNNs with different depths and bottleneck widths. Our key finding is that networks with a bottleneck show a strong functional organisation: almost all cells in the bottleneck layer become both spatially and colour opponent, cells in the layer following the bottleneck become non-opponent. The colour tuning data can further be used to form a rich understanding of how colour is encoded by a network. As a concrete demonstration, we show that shallower networks without a bottleneck learn a complex non-linear colour system, whereas deeper networks with tight bottlenecks learn a simple channel opponent code in the bottleneck layer. We further develop a method of obtaining a hue sensitivity curve for a trained CNN which enables high level insights that complement the low level findings from the colour tuning data. We go on to train a series of networks under different conditions to ascertain the robustness of the discussed results. Ultimately, our methods and findings coalesce with prior art, strengthening our ability to interpret trained CNNs and furthering our understanding of the connection between architecture and learned representation. Trained models and code for all experiments are available at https://github.com/ecs-vlc/opponency.",
      "fieldsOfStudy": [
        "Computer Science",
        "Biology"
      ],
      "topics": [
        "tight bottlenecks",
        "bottleneck widths",
        "colour opponency",
        "trained networks",
        "colour",
        "learned function",
        "deeper networks",
        "learned representation",
        "shallower networks",
        "networks",
        "complex non-linear colour system",
        "cells",
        "high level insights",
        "spatial and colour tuning curves",
        "trained CNNs",
        "bottleneck layer"
      ]
    }
  },
  {
    "sim": 0.437463388015767,
    "gen": {
      "title": "Unsupervised Deep Homography: A Fast and Robust Homography Estimation Model",
      "url": "https://www.semanticscholar.org/paper/5409587af979a088d6d77282c1554a6f68034f11",
      "abstract": "Homography estimation between multiple aerial images can provide relative pose estimation for collaborative autonomous exploration and monitoring. The usage on a robotic system requires a fast and robust homography estimation algorithm. In this letter, we propose an unsupervised learning algorithm that trains a deep convolutional neural network to estimate planar homographies. We compare the proposed algorithm to traditional-feature-based and direct methods, as well as a corresponding supervised learning algorithm. Our empirical results demonstrate that compared to traditional approaches, the unsupervised algorithm achieves faster inference speed, while maintaining comparable or better accuracy and robustness to illumination variation. In addition, our unsupervised method has superior adaptability and performance compared to the corresponding supervised deep learning method. Our image dataset and a Tensorflow implementation of our work are available at  https://github.com/tynguyen/unsupervisedDeepHomographyRAL2018.",
      "fieldsOfStudy": [
        "Computer Science",
        "Engineering"
      ],
      "topics": [
        "planar homographies",
        "illumination variation",
        "relative pose estimation",
        "faster inference speed",
        "collaborative autonomous exploration",
        "corresponding supervised deep learning method",
        "monitoring",
        "corresponding supervised learning algorithm",
        "Homography estimation",
        "superior adaptability",
        "multiple aerial images",
        "traditional approaches",
        "https://github.com/tynguyen/unsupervisedDeepHomographyRAL2018",
        "unsupervised learning algorithm",
        "deep convolutional neural network",
        "robustness"
      ]
    },
    "org": {
      "title": "Improving Neural Relation Extraction with Positive and Unlabeled Learning",
      "url": "https://www.semanticscholar.org/paper/ee383b6891513e156a95543f2bf89aea509e3e5a",
      "abstract": "We present a novel approach to improve the performance of distant supervision relation extraction with Positive and Unlabeled (PU) Learning. This approach first applies reinforcement learning to decide whether a sentence is positive to a given relation, and then positive and unlabeled bags are constructed. In contrast to most previous studies, which mainly use selected positive instances only, we make full use of unlabeled instances and propose two new representations for positive and unlabeled bags. These two representations are then combined in an appropriate way to make bag-level prediction. Experimental results on a widely used real-world dataset demonstrate that this new approach indeed achieves significant and consistent improvements as compared to several competitive baselines.",
      "fieldsOfStudy": [
        "Computer Science"
      ],
      "topics": [
        "selected positive instances",
        "competitive baselines",
        "unlabeled instances",
        "distant supervision relation extraction",
        "use",
        "bag-level prediction",
        "positive and unlabeled bags",
        "reinforcement learning",
        "new approach",
        "significant and consistent improvements",
        "new representations",
        "given relation",
        "Experimental results",
        "Positive and Unlabeled (PU) Learning",
        "previous studies"
      ]
    }
  },
  {
    "sim": 0.39456750682263564,
    "gen": {
      "title": "A Cloud-edge Cooperative Dispatching Method for Distribution Networks Considering Photovoltaic Generation Uncertainty",
      "url": "https://www.semanticscholar.org/paper/f780d4f17fc0c23eb625d55882e70f068bb0c408",
      "abstract": "With the increasing penetration of renewable energy generation, uncertainty and randomness pose great challenges for optimal dispatching in distribution networks. We proposea cloud-edge cooperative dispatching (CECD) method to exploitthe new opportunities offered by Internet of Things (IoT) technology. To alleviate the huge pressure on the modeling and computing of large-scale distribution system, the method deploysedge nodes in small-scale transformer areas in which robust optimization subproblem models are introduced to address thephotovoltaic (PV) uncertainty. Considering the limited communication and computing capabilities of the edge nodes, the cloudcenter in the distribution automation system (DAS) establishes a utility grid master problem model that enforces the consistency between the solution at each edge node with the utility gridbased on the alternating direction method of multipliers (AD-MM). Furthermore, the voltage constraint derived from the linear power flow equations is adopted for enhancing the operation security of the distribution network. We perform a cloud-edge system simulation of the proposed CECD method anddemonstrate a dispatching application. The case study is carried out on a modified 33-node system to verify the remarkable performance of the proposed model and method.",
      "fieldsOfStudy": [
        "Computer Science"
      ],
      "topics": [
        "optimal dispatching",
        "method",
        "robust optimization subproblem models",
        "great challenges",
        "uncertainty",
        "exploitthe new opportunities",
        "nodes",
        "large-scale distribution system",
        "utility grid master problem model",
        "distribution automation system",
        "proposed CECD method",
        "alternating direction method",
        "distribution network",
        "PV",
        "new opportunities",
        "randomness"
      ]
    },
    "org": {
      "title": "On the number of cliques in graphs with a forbidden minor",
      "url": "https://www.semanticscholar.org/paper/f03930de4826481a3442f89e26fcad117ff32ab5",
      "abstract": null,
      "fieldsOfStudy": [
        "Mathematics",
        "Computer Science"
      ]
    }
  },
  {
    "sim": 0.4734922969702098,
    "gen": {
      "title": "Advances in Multimedia Information Processing - PCM 2006, 7th Pacific Rim Conference on Multimedia, Hangzhou, China, November 2-4, 2006, Proceedings",
      "url": "https://www.semanticscholar.org/paper/4d8e3d9a441fa1fa67975b0e3593ee31f8deb463",
      "abstract": null,
      "fieldsOfStudy": [
        "Computer Science",
        "Geography"
      ]
    },
    "org": {
      "title": "TextCaps: a Dataset for Image Captioning with Reading Comprehension",
      "url": "https://www.semanticscholar.org/paper/33eadd4e666a894306a22ba0839c5e0cef77280e",
      "abstract": null,
      "fieldsOfStudy": [
        "Computer Science"
      ]
    }
  },
  {
    "sim": 0.43988936299650905,
    "gen": {
      "title": "A Practical Tutorial on Graph Neural Networks",
      "url": "https://www.semanticscholar.org/paper/28a22bea3d98b326715bcede9b0216d3f788e1b8",
      "abstract": "Graph neural networks (GNNs) have recently grown in popularity in the field of artificial intelligence (AI) due to their unique ability to ingest relatively unstructured data types as input data. Although some elements of the GNN architecture are conceptually similar in operation to traditional neural networks (and neural network variants), other elements represent a departure from traditional deep learning techniques. This tutorial exposes the power and novelty of GNNs to AI practitioners by collating and presenting details regarding the motivations, concepts, mathematics, and applications of the most common and performant variants of GNNs. Importantly, we present this tutorial concisely, alongside practical examples, thus providing a practical and accessible tutorial on the topic of GNNs.",
      "fieldsOfStudy": [
        "Computer Science"
      ],
      "topics": [
        "input data",
        "relatively unstructured data types",
        "traditional deep learning techniques",
        "neural network variants",
        "GNNs",
        "traditional neural networks",
        "AI practitioners",
        "Graph neural networks",
        "artificial intelligence",
        "AI",
        "elements",
        "practical examples",
        "applications",
        "mathematics",
        "concepts",
        "details",
        "graph neural"
      ]
    },
    "org": {
      "title": "Improved Time Warp Edit Distance - A Parallel Dynamic Program in Linear Memory",
      "url": "https://www.semanticscholar.org/paper/92330d250d2081b479877520ead17b42f10b6ee4",
      "abstract": "Edit Distance is a classic family of dynamic programming problems, among which Time Warp Edit Distance refines the problem with the notion of a metric and temporal elasticity. A novel Improved Time Warp Edit Distance algorithm that is both massively parallelizable and requiring only linear storage is presented. This method uses the procession of a three diagonal band to cover the original dynamic program space. Every element of the diagonal update can be computed in parallel. The core method is a feature of the TWED Longest Common Subsequence data dependence and is applicable to dynamic programs that share similar band subproblem structure. The algorithm has been implemented as a CUDA C library with Python bindings. Speedups for challenging problems are phenomenal.",
      "fieldsOfStudy": [
        "Computer Science",
        "Mathematics"
      ],
      "topics": [
        "similar band subproblem structure",
        "dynamic programming problems",
        "dynamic programs",
        "Improved Time Warp Edit Distance",
        "Edit Distance",
        "Python bindings",
        "linear storage",
        "problems",
        "original dynamic program space",
        "A novel Improved Time Warp Edit Distance algorithm",
        "parallel",
        "TWED Longest Common Subsequence data dependence",
        "CUDA C",
        "Python",
        "challenging problems",
        "metric and temporal elasticity",
        "CUDA",
        "CUDA C library"
      ]
    }
  },
  null,
  null,
  null,
  {
    "sim": 0.35368595434020667,
    "gen": {
      "title": "Distributed formation control of nonholonomic mobile robots by bounded feedback in the presence of obstacles",
      "url": "https://www.semanticscholar.org/paper/5e11f7c0b26a8c8e03234cf9ce0cdf6499cbbfb8",
      "abstract": "The problem of distributed formation control of nonholonomic mobile robots is addressed in this paper, in which the robots are designed to track a formation. Collision avoidance among agents is guaranteed using a control law based on a repulsive force. In an uncertain environment where obstacles exist, the construction of a repulsive force and rotational direction enables agents to avoid and pass the obstacles. The control inputs of each robot are designed to be bounded. Numerical simulations with different formations are implemented to demonstrate the efficacy of the proposed scheme.",
      "fieldsOfStudy": [
        "Computer Science",
        "Engineering"
      ],
      "topics": [
        "rotational direction",
        "distributed formation control",
        "different formations",
        "agents",
        "nonholonomic mobile",
        "repulsive force",
        "proposed scheme",
        "obstacles",
        "control law",
        "Collision avoidance",
        "Numerical simulations",
        "formation",
        "The control inputs",
        "nonholonomic mobile robots",
        "robot"
      ]
    },
    "org": {
      "title": "On the inefficiency of ride-sourcing services towards urban congestion",
      "url": "https://www.semanticscholar.org/paper/e68877166985b56c58481af7dda6327c05a82ba4",
      "abstract": null,
      "fieldsOfStudy": [
        "Physics",
        "Computer Science",
        "Engineering",
        "Mathematics",
        "Business"
      ]
    }
  },
  {
    "sim": 0.17811817373341599,
    "gen": {
      "title": "Learning Robust State Abstractions for Hidden-Parameter Block MDPs",
      "url": "https://www.semanticscholar.org/paper/9835d0d85faa36d2c27bea806487b988935e92a2",
      "abstract": null,
      "fieldsOfStudy": [
        "Computer Science"
      ]
    },
    "org": {
      "title": "A Data Quality Metric (DQM): How to Estimate the Number of Undetected Errors in Data Sets",
      "url": "https://www.semanticscholar.org/paper/11b9ac1beb9f661e111bfa72be04eb4147f5ef75",
      "abstract": "Data cleaning, whether manual or algorithmic, is rarely perfect leaving a dataset with an unknown number of false positives and false negatives after cleaning. In many scenarios, quantifying the number of remaining errors is challenging because our data integrity rules themselves may be incomplete, or the available gold-standard datasets may be too small to extrapolate. As the use of inherently fallible crowds becomes more prevalent in data cleaning problems, it is important to have estimators to quantify the extent of such errors. We propose novel species estimators to estimate the number of distinct remaining errors in a dataset after it has been cleaned by a set of crowd workers -- essentially, quantifying the utility of hiring additional workers to clean the dataset. This problem requires new estimators that are robust to false positives and false negatives, and we empirically show on three real-world datasets that existing species estimators are unstable for this problem, while our proposed techniques quickly converge.",
      "fieldsOfStudy": [
        "Computer Science"
      ],
      "topics": [
        "existing species estimators",
        "crowd workers",
        "false negatives",
        "false positives",
        "distinct remaining errors",
        "remaining errors",
        "novel species estimators",
        "errors",
        "additional workers",
        "data cleaning problems",
        "estimators",
        "available gold-standard datasets",
        "dataset",
        "dataset",
        "Data cleaning",
        "cleaning"
      ]
    }
  },
  null,
  null,
  {
    "sim": 0.4575966990366134,
    "gen": {
      "title": "Underwater Localization in Sparse 3D Acoustic Sensor Networks",
      "url": "https://www.semanticscholar.org/paper/8b07278a6bdc3b8eba217a12fd614a992924d595",
      "abstract": "We study the localization problem in sparse 3D underwater sensor networks. Considering the fact that depth information is typically available for underwater sensors, we transform the 3D underwater positioning problem into its two- dimensional counterpart via a projection technique and prove that a non-degenerative projection preserves network localizability. We further prove that given a network and a constant k, all of the geometric k-lateration localization methods are equivalent. Based on these results, we design a purely distributed localization framework termed USP. This framework can be applied with any ranging method proposed for 2D terrestrial sensor networks. Through theoretical analysis and extensive simulation, we show that USP preserves the localizability of the original 3D network via a simple projection and improves localization capabilities when bilateration is employed. USP has low storage and computation requirements, and predictable and balanced communication overhead.",
      "fieldsOfStudy": [
        "Computer Science"
      ],
      "topics": [
        "sparse 3D underwater sensor networks",
        "network localizability",
        "2D terrestrial sensor networks",
        "localization capabilities",
        "underwater sensors",
        "original 3D network",
        "non-degenerative projection",
        "USP",
        "problem",
        "bilateration",
        "geometric k-lateration localization methods",
        "simple projection",
        "projection technique",
        "localization problem",
        "3D underwater",
        "3D underwater positioning problem",
        "predictable and balanced communication overhead"
      ]
    },
    "org": {
      "title": "Local communities obstruct global consensus: Naming game on multi-local-world networks",
      "url": "https://www.semanticscholar.org/paper/65f2ea659f159ddbb673cb41150a86cdd0117ebd",
      "abstract": null,
      "fieldsOfStudy": [
        "Computer Science",
        "Physics"
      ]
    }
  },
  {
    "sim": 0.7396865585128755,
    "gen": {
      "title": "Predicting Short-Term Traffic Flow by Long Short-Term Memory Recurrent Neural Network",
      "url": "https://www.semanticscholar.org/paper/d6271639f66074d7d40626c759513b3e77c34334",
      "abstract": "Intelligent Transportation System (ITS) is a significant part of smart city, and short-term traffic flow prediction plays an important role in intelligent transportation management and route guidance. A number of models and algorithms based on time series prediction and machine learning were applied to short-term traffic flow prediction and achieved good results. However, most of the models require the length of the input historical data to be predefined and static, which cannot automatically determine the optimal time lags. To overcome this shortage, a model called Long Short-Term Memory Recurrent Neural Network (LSTM RNN) is proposed in this paper, which takes advantages of the three multiplicative units in the memory block to determine the optimal time lags dynamically. The dataset from Caltrans Performance Measurement System (PeMS) is used for building the model and comparing LSTM RNN with several well-known models, such as random walk(RW), support vector machine(SVM), single layer feed forward neural network(FFNN) and stacked autoencoder(SAE). The results show that the proposed prediction model achieves higher accuracy and generalizes well.",
      "fieldsOfStudy": [
        "Computer Science"
      ],
      "topics": [
        "time series prediction",
        "route guidance",
        "LSTM RNN",
        "short-term traffic flow prediction",
        "intelligent transportation management",
        "models",
        "good results",
        "autoencoder(SAE",
        "machine learning",
        "proposed prediction model",
        "single layer",
        "vector machine(SVM",
        "higher accuracy",
        "random walk(RW",
        "optimal time lags",
        "single layer feed forward neural network(FFNN",
        "stacked autoencoder(SAE",
        "support vector machine(SVM"
      ]
    },
    "org": {
      "title": "Multi-step-ahead Prediction from Short-term Data by Delay-embedding-based Forecast Machine",
      "url": "https://www.semanticscholar.org/paper/fae085d311ccddf2f226f7cd01825c57a5efc43c",
      "abstract": "Making accurate multi-step-ahead prediction for a complex system is a challenge for many practical applications, especially when only short-term time-series data are available. In this work, we proposed a novel framework, Delay-Embedding-based Forecast Machine (DEFM), to predict the future values of a target variable in an accurate and multi-step-ahead manner based on the high-dimensional short-term measurements. With a three-module spatiotemporal architecture, DEFM leverages deep learning to effectively extract both the spatially and sequentially associated information from the short-term dynamics even with time-varying parameters or additive noise. Being trained through a self-supervised scheme, DEFM well fits a nonlinear transformation that maps from the observed high-dimensional information to the delay embeddings of a target variable, thus predicting the future information. The effectiveness and accuracy of DEFM is demonstrated by applications on both representative models and six real-world datasets. The comparison with four traditional prediction methods exhibits the superiority and robustness of DEFM.",
      "fieldsOfStudy": [
        "Computer Science",
        "Engineering"
      ],
      "topics": [
        "additive noise",
        "DEFM leverages",
        "DEFM",
        "practical applications",
        "accurate multi-step-ahead prediction",
        "applications",
        "Forecast Machine",
        "deep learning",
        "future information",
        "target variable",
        "time-varying parameters",
        "observed high-dimensional information",
        "short-term time-series data",
        "high-dimensional short-term measurements",
        "short-term dynamics",
        "robustness",
        "future values"
      ]
    }
  },
  {
    "sim": 0.33775402520771436,
    "gen": {
      "title": "A reliability assessment method of crack-containing oil-gas pipelines and its sensitivity analysis",
      "url": "https://www.semanticscholar.org/paper/207166efaf939a14c23cc327e59b3c03bdb8cabf",
      "abstract": "A probabilistic safety assessment methodology based on the dual-criteria approach was employed to make up for the shortcomings of the structural reliability assessment and sensitivity analysis in the structural integrity assessment of crack-containing oil-gas pipelines.The present paper proposed a three-dimensional reliability assessment method combined by the direct Monte Carlo simulation and two-dimensional kernel density estimation,by which the failure probability and probabilistic density distribution of random assessment points could be obtained based on the known probabilistic distribution characteristics of assessment parameters.The paper made a comparison between assessment results obtained from the BS 7910 standard and those from the API 579 standard,and completed the sensitivity analysis of individual random assessment parameters by using an interval analytical method.The analytical results indicated that the three-dimensional reliability assessment technique could evaluate the structural integrity of oil-gas pipelines more intuitively,while the interval analytical method was applicable to the sensitivity analysis of parameters.Compared with the BS 7910 standard,the API 579 standard provided less conservative evaluations and less scattered results at the same level of analyses.Therefore,the proposed reliability assessment method and sensitivity analysis were available to the applicability assessment of oil-gas pipelines.",
      "fieldsOfStudy": [
        "Geology"
      ],
      "topics": [
        "assessment parameters",
        "individual random assessment parameters",
        "assessment results",
        "random assessment points",
        "sensitivity analysis",
        "analyses",
        "probabilistic density distribution",
        "proposed reliability assessment method",
        "interval analytical method",
        "structural integrity assessment",
        "parameters",
        "A probabilistic safety assessment methodology",
        "oil-gas pipelines"
      ]
    },
    "org": {
      "title": "Modeling Radicalization Phenomena in Heterogeneous Populations",
      "url": "https://www.semanticscholar.org/paper/d2992d2dc2c89e72308ea9256214f3bc29825a8f",
      "abstract": "The phenomenon of radicalization is investigated within a mixed population composed of core and sensitive subpopulations. The latest includes first to third generation immigrants. Respective ways of life may be partially incompatible. In case of a conflict core agents behave as inflexible about the issue. In contrast, sensitive agents can decide either to live peacefully adjusting their way of life to the core one, or to oppose it with eventually joining violent activities. The interplay dynamics between peaceful and opponent sensitive agents is driven by pairwise interactions. These interactions occur both within the sensitive population and by mixing with core agents. The update process is monitored using a Lotka-Volterra-like Ordinary Differential Equation. Given an initial tiny minority of opponents that coexist with both inflexible and peaceful agents, we investigate implications on the emergence of radicalization. Opponents try to turn peaceful agents to opponents driving radicalization. However, inflexible core agents may step in to bring back opponents to a peaceful choice thus weakening the phenomenon. The required minimum individual core involvement to actually curb radicalization is calculated. It is found to be a function of both the majority or minority status of the sensitive subpopulation with respect to the core subpopulation and the degree of activeness of opponents. The results highlight the instrumental role core agents can have to hinder radicalization within the sensitive subpopulation. Some hints are outlined to favor novel public policies towards social integration.",
      "fieldsOfStudy": [
        "Political Science",
        "Physics",
        "Computer Science",
        "Medicine"
      ],
      "topics": [
        "core agents",
        "inflexible core agents",
        "opponents driving radicalization",
        "sensitive agents",
        "peaceful agents",
        "radicalization",
        "opponents",
        "core and sensitive subpopulations",
        "peaceful and opponent sensitive agents",
        "conflict core agents",
        "violent activities",
        "pairwise interactions",
        "Ordinary Differential Equation",
        "core subpopulation",
        "social integration",
        "sensitive subpopulation",
        "The required minimum individual core involvement"
      ]
    }
  },
  {
    "sim": 0.42887638858917465,
    "gen": {
      "title": "A Bayesian Approach for Predicting Food and Beverage Sales in Staff Canteens and Restaurants",
      "url": "https://www.semanticscholar.org/paper/80b236e42e0a354d885d8bf4c4e889db2d261d97",
      "abstract": null,
      "fieldsOfStudy": [
        "Mathematics",
        "Computer Science"
      ]
    },
    "org": {
      "title": "High academic performance is associated with shorter sleep and later bedtimes for young adults",
      "url": "https://www.semanticscholar.org/paper/243534c7709d169322a936d3c0334883de6325d3",
      "abstract": "Shorter sleep is known to be negatively associated with academic performance. However, this result has mostly been found in homogenous samples (e.g., students from one university) or when using relative measures of academic performance, such as grade point average. Consequently, the relationship between academic performance and sleep patterns at the population level is not well understood. In this paper, we examined the relationship between academic performance as measured by a standardized test and sleep patterns using data from a Russian panel study (N = 4,400) that was nationally representative for one age cohort (20-21 years old). In addition to self-reported sleep patterns, the data set contained information about participants' online activities over a period of up to 10 years, which allowed us to track the evolution of this relationship over time. We found that high academic performance was associated with shorter sleep, later bedtime, and increased online activity at night. The relationship between high academic performance and online activity at night was stable over a period of 5 years. Our findings suggest that the relationship between academic performance and sleep patterns can be more complex than previously believed and that high performance may be achieved at the expense of individual well-being.",
      "fieldsOfStudy": [
        "Psychology",
        "Computer Science"
      ],
      "topics": [
        "high academic performance",
        "sleep patterns",
        "high performance",
        "online activity",
        "shorter sleep",
        "grade point average",
        "time",
        "night",
        "relative measures",
        "self-reported sleep patterns",
        "participants online activities",
        "later bedtime",
        "data",
        "increased online activity"
      ]
    }
  },
  null,
  {
    "sim": 0.3370791250791594,
    "gen": {
      "title": "Pancreas Segmentation in CT and MRI Images via Domain Specific Network Designing and Recurrent Neural Contextual Learning",
      "url": "https://www.semanticscholar.org/paper/dc74948c1fe79fea2c5d8b22dde3d2aaa0de3a46",
      "abstract": "Automatic pancreas segmentation in radiology images, eg., computed tomography (CT) and magnetic resonance imaging (MRI), is frequently required by computer-aided screening, diagnosis, and quantitative assessment. Yet pancreas is a challenging abdominal organ to segment due to the high inter-patient anatomical variability in both shape and volume metrics. Recently, convolutional neural networks (CNNs) have demonstrated promising performance on accurate segmentation of pancreas. However, the CNN-based method often suffers from segmentation discontinuity for reasons such as noisy image quality and blurry pancreatic boundary. From this point, we propose to introduce recurrent neural networks (RNNs) to address the problem of spatial non-smoothness of inter-slice pancreas segmentation across adjacent image slices. To inference initial segmentation, we first train a 2D CNN sub-network, where we modify its network architecture with deep-supervision and multi-scale feature map aggregation so that it can be trained from scratch with small-sized training data and presents superior performance than transferred models. Thereafter, the successive CNN outputs are processed by another RNN sub-network, which refines the consistency of segmented shapes. More specifically, the RNN sub-network consists convolutional long short-term memory (CLSTM) units in both top-down and bottom-up directions, which regularizes the segmentation of an image by integrating predictions of its neighboring slices. We train the stacked CNN-RNN model end-to-end and perform quantitative evaluations on both CT and MRI images.",
      "fieldsOfStudy": [
        "Computer Science"
      ],
      "topics": [
        "adjacent image slices",
        "noisy image quality",
        "inter-slice pancreas segmentation",
        "quantitative assessment",
        "radiology images",
        "quantitative evaluations",
        "accurate segmentation",
        "blurry pancreatic boundary",
        "segmentation discontinuity",
        "Automatic pancreas segmentation",
        "magnetic resonance imaging",
        "initial segmentation",
        "MRI",
        "transferred models",
        "CT"
      ]
    },
    "org": {
      "title": "Threshold saturation of spatially coupled sparse superposition codes for all memoryless channels",
      "url": "https://www.semanticscholar.org/paper/735c9fe6d86fc5f777ba067740f1b35da520ec66",
      "abstract": "We recently proved threshold saturation for spatially coupled sparse superposition codes on the additive white Gaussian noise channel [1]. Here we generalize our analysis to a much broader setting. We show for any memoryless channel that spatial coupling allows generalized approximate message-passing (GAMP) decoding to reach the potential (or Bayes optimal) threshold of the code ensemble. Moreover in the large input alphabet size limit: i) the GAMP algorithmic threshold of the underlying (or uncoupled) code ensemble is simply expressed as a Fisher information; ii) the potential threshold tends to Shannon's capacity. Although we focus on coding for sake of coherence with our previous results, the framework and methods are very general and hold for a wide class of generalized estimation problems with random linear mixing.",
      "fieldsOfStudy": [
        "Mathematics",
        "Computer Science",
        "Physics"
      ],
      "topics": [
        "threshold saturation",
        "random linear mixing",
        "generalized estimation problems",
        "spatial coupling",
        "GAMP",
        "additive white Gaussian noise channel",
        "GAMP algorithmic threshold",
        "Fisher",
        "potential threshold",
        "Shannon",
        "spatially coupled sparse superposition codes",
        "Bayes",
        "code ensemble",
        "Gaussian",
        "Shannons capacity",
        "Fisher information",
        "methods",
        "memoryless channel",
        "potential (or Bayes optimal) threshold"
      ]
    }
  },
  null,
  null,
  {
    "sim": 0.320979535622705,
    "gen": {
      "title": "Coordinated Control of Dual-Motor Using the Interval Type-2 Fuzzy Logic in Autonomous Steering System of AGV",
      "url": "https://www.semanticscholar.org/paper/1ece1f5c8a5266fedb486e98163a6fa8fd8000a4",
      "abstract": null,
      "fieldsOfStudy": [
        "Computer Science"
      ]
    },
    "org": {
      "title": "Improved Bounds on Restricted Isometry Constants for Gaussian Matrices",
      "url": "https://www.semanticscholar.org/paper/42c488475434d8e4980c09f4e1f698cef76e4870",
      "abstract": "The restricted isometry constant (RIC) of a matrix $A$ measures how close to an isometry is the action of $A$ on vectors with few nonzero entries, measured in the $\\ell^2$ norm. Specifically, the upper and lower RICs of a matrix $A$ of size $n\\times N$ are the maximum and the minimum deviation from unity (one) of the largest and smallest, respectively, square of singular values of all ${N\\choose k}$ matrices formed by taking $k$ columns from $A$. Calculation of the RIC is intractable for most matrices due to its combinatorial nature; however, many random matrices typically have bounded RIC in some range of problem sizes $(k,n,N)$. We provide the best known bound on the RIC for Gaussian matrices, which is also the smallest known bound on the RIC for any large rectangular matrix. Our results are built on the prior bounds of Blanchard, Cartis, and Tanner [SIAM Rev., to appear], with improvements achieved by grouping submatrices that share a substantial number of columns.",
      "fieldsOfStudy": [
        "Computer Science",
        "Mathematics"
      ],
      "topics": [
        "k}$ matrices",
        "Gaussian matrices",
        "random matrices",
        "matrices",
        "RIC",
        "nonzero entries",
        "large rectangular matrix",
        "singular values",
        "$k$ columns",
        "matrix",
        "submatrices",
        "unity",
        "problem",
        "A$",
        "columns",
        "problem sizes",
        "size",
        "matrix $A$",
        "vectors",
        "N$"
      ]
    }
  },
  null,
  null,
  null
]